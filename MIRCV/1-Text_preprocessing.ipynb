{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "In this exercise, you'll need to implement basic text preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "import ipytest\n",
    "import string\n",
    "import re\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tokenization\n",
    "\n",
    "Split an input text into tokens based on whitespaces, punctuation, hyphens, and HTML markup. Additionally, lowercase all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str)-> List[str]:    \n",
    "    \"\"\"Returns a sequence of terms given an input text.\"\"\"\n",
    "    # Your solution here\n",
    "    text = text.lower()\n",
    "    simbols = \"\"\n",
    "    for i in string.punctuation:\n",
    "        simbols = simbols + \"\\\\\" + i + \"+|\" \n",
    "    PATTERN = r\"\\s+|\" + simbols + \"<...>\" + \"<....>\" + \"<.....>\"\n",
    "    print(PATTERN)\n",
    "    res = re.split(PATTERN, text)\n",
    "    while(\"\" in res):\n",
    "        res.remove(\"\")\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31m                                                                                         [100%]\u001B[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001B[31m\u001B[1m____________________________________________ test_html _____________________________________________\u001B[0m\n",
      "\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_html\u001B[39;49;00m():\n",
      ">       \u001B[94massert\u001B[39;49;00m tokenize(\u001B[33m\"\u001B[39;49;00m\u001B[33maaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m) == [\u001B[33m\"\u001B[39;49;00m\u001B[33maaa\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mccc\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33meee\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mfff\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\n",
      "\u001B[1m\u001B[31mE       AssertionError: assert ['aaa', 'bbb'...', 'ddd', ...] == ['aaa', 'ccc', 'eee', 'fff']\u001B[0m\n",
      "\u001B[1m\u001B[31mE         At index 1 diff: 'bbb' != 'ccc'\u001B[0m\n",
      "\u001B[1m\u001B[31mE         Left contains 5 more items, first extra item: 'eee'\u001B[0m\n",
      "\u001B[1m\u001B[31mE         Use -v to get more diff\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_1401/2160663131.py\u001B[0m:11: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "\\s+|\\!+|\\\"+|\\#+|\\$+|\\%+|\\&+|\\'+|\\(+|\\)+|\\*+|\\++|\\,+|\\-+|\\.+|\\/+|\\:+|\\;+|\\<+|\\=+|\\>+|\\?+|\\@+|\\[+|\\\\+|\\]+|\\^+|\\_+|\\`+|\\{+|\\|+|\\}+|\\~+|<...><....><.....>\n",
      "['aaa', 'bbb', 'ccc', 'ddd', 'eee', 'ddd', 'bbb', 'fff', 'ggg']\n",
      "===================================== short test summary info ======================================\n",
      "FAILED tmpb2_zi8cd.py::test_html - AssertionError: assert ['aaa', 'bbb'...', 'ddd', ...] == ['aaa...\n",
      "\u001B[31m\u001B[31m\u001B[1m1 failed\u001B[0m, \u001B[32m3 passed\u001B[0m\u001B[31m in 0.07s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_whitespace():\n",
    "    assert tokenize(\"aaa bbb ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_punctuation():\n",
    "    assert tokenize(\"aaa! bbb.ccc,ddd:eee ff\\\"f\") == [\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"ff\", \"f\"]\n",
    "    \n",
    "def test_hyphens():\n",
    "    assert tokenize(\"aaa bbb-Ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_html():\n",
    "    assert tokenize(\"aaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\") == [\"aaa\", \"ccc\", \"eee\", \"fff\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Stopwords removal\n",
    "\n",
    "Remove stopwords from a sequence of tokens, given a set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens: List[str], stopwords: Set[str]) -> List[str]:\n",
    "    \"\"\"Removes stopwords from a sequence of tokens.\"\"\"\n",
    "    # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_no_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {}) == [\"this\", \"is\", \"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {\"is\", \"this\"}) == [\"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords2():\n",
    "    assert remove_stopwords([\"this\", \"isolate\", \"otto\"], {\"is\", \"this\", \"to\"}) == [\"isolate\", \"otto\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Suffix-s stemming\n",
    "\n",
    "Remove the s-suffix from all terms in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_s_stemmer(terms: List[str]) -> List[str]:\n",
    "    \"\"\"Removes the s-suffix from all terms in a sequence.\"\"\"\n",
    "    # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_stemming():\n",
    "    assert suffix_s_stemmer([\"dogs\", \"better\", \"cats\"]) == [\"dog\", \"better\", \"cat\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5e4a7eaa80b8b0cab0ebfd023b62b4fb81ced96acf97d50ef5e35c1384dc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}