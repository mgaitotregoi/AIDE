{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "In this exercise, you'll need to implement basic text preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "import ipytest\n",
    "import string\n",
    "import re\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tokenization\n",
    "\n",
    "Split an input text into tokens based on whitespaces, punctuation, hyphens, and HTML markup. Additionally, lowercase all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str)-> List[str]:    \n",
    "    \"\"\"Returns a sequence of terms given an input text.\"\"\"\n",
    "    # Remove HTML markup using a regular expression.\n",
    "    re_html = re.compile(\"<[^>]+>\")\n",
    "    text = re_html.sub(\" \", text)\n",
    "    # Replace punctuation marks (including hyphens) with spaces.\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, \" \")\n",
    "    # Lowercase and split on whitespaces.\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                         [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_________________________________________ test_whitespace __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_whitespace\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa bbb ccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbbb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'aaa bbb ccc' == ['aaa', 'bbb', 'ccc']\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 'aaa bbb ccc' = tokenize('aaa bbb ccc')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_3646/2160663131.py\u001b[0m:2: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "aaa bbb ccc\n",
      "\u001b[31m\u001b[1m_________________________________________ test_punctuation _________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_punctuation\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa! bbb.ccc,ddd:eee ff\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbbb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mddd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33meee\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       assert 'aaa  bbb ccc ddd eee ff f' == ['aaa', 'bbb', 'ccc', 'ddd', 'eee', 'ff', ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 'aaa  bbb ccc ddd eee ff f' = tokenize('aaa! bbb.ccc,ddd:eee ff\"f')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_3646/2160663131.py\u001b[0m:5: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "aaa! bbb.ccc,ddd:eee ff\"f\n",
      "\u001b[31m\u001b[1m___________________________________________ test_hyphens ___________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_hyphens\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa bbb-Ccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbbb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'aaa bbb Ccc' == ['aaa', 'bbb', 'ccc']\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 'aaa bbb Ccc' = tokenize('aaa bbb-Ccc')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_3646/2160663131.py\u001b[0m:8: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "aaa bbb-Ccc\n",
      "\u001b[31m\u001b[1m____________________________________________ test_html _____________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_html\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33meee\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'aaa  ccc  eee  fff  ' == ['aaa', 'ccc', 'eee', 'fff']\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 'aaa  ccc  eee  fff  ' = tokenize('aaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_3646/2160663131.py\u001b[0m:11: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "aaa  ccc  eee  fff  \n",
      "===================================== short test summary info ======================================\n",
      "FAILED tmpp1t8x85d.py::test_whitespace - AssertionError: assert 'aaa bbb ccc' == ['aaa', 'bbb', '...\n",
      "FAILED tmpp1t8x85d.py::test_punctuation - assert 'aaa  bbb ccc ddd eee ff f' == ['aaa', 'bbb', 'c...\n",
      "FAILED tmpp1t8x85d.py::test_hyphens - AssertionError: assert 'aaa bbb Ccc' == ['aaa', 'bbb', 'ccc']\n",
      "FAILED tmpp1t8x85d.py::test_html - AssertionError: assert 'aaa  ccc  eee  fff  ' == ['aaa', 'ccc'...\n",
      "\u001b[31m\u001b[31m\u001b[1m4 failed\u001b[0m\u001b[31m in 0.31s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_whitespace():\n",
    "    assert tokenize(\"aaa bbb ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_punctuation():\n",
    "    assert tokenize(\"aaa! bbb.ccc,ddd:eee ff\\\"f\") == [\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"ff\", \"f\"]\n",
    "    \n",
    "def test_hyphens():\n",
    "    assert tokenize(\"aaa bbb-Ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_html():\n",
    "    assert tokenize(\"aaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\") == [\"aaa\", \"ccc\", \"eee\", \"fff\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Stopwords removal\n",
    "\n",
    "Remove stopwords from a sequence of tokens, given a set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens: List[str], stopwords: Set[str]) -> List[str]:\n",
    "    \"\"\"Removes stopwords from a sequence of tokens.\"\"\"\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_no_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {}) == [\"this\", \"is\", \"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {\"is\", \"this\"}) == [\"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords2():\n",
    "    assert remove_stopwords([\"this\", \"isolate\", \"otto\"], {\"is\", \"this\", \"to\"}) == [\"isolate\", \"otto\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Suffix-s stemming\n",
    "\n",
    "Remove the s-suffix from all terms in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_s_stemmer(terms: List[str]) -> List[str]:\n",
    "    \"\"\"Removes the s-suffix from all terms in a sequence.\"\"\"\n",
    "    stemmed_terms = []\n",
    "    for term in terms:        \n",
    "        stemmed_term = term[:-1] if term[-1] == \"s\" else term\n",
    "        stemmed_terms.append(stemmed_term)\n",
    "    return stemmed_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_stemming():\n",
    "    assert suffix_s_stemmer([\"dogs\", \"better\", \"cats\"]) == [\"dog\", \"better\", \"cat\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5e4a7eaa80b8b0cab0ebfd023b62b4fb81ced96acf97d50ef5e35c1384dc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
