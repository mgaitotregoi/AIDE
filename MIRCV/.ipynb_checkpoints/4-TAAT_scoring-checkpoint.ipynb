{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-at-a-time Query Processing\n",
    "\n",
    "Implement term-at-a-time query processing using a simple scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index\n",
    "\n",
    "For simplicity, the inverted index for the document collection is given as a dictionary, with a terms as keys and posting lists as values. Each posting is a (document ID, term frequency) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "index = {\n",
    "    \"beijing\": [        (1, 1),                 (4, 1)],\n",
    "    \"dish\":    [        (1, 1),                 (4, 1)],\n",
    "    \"duck\":    [(0, 3), (1, 2), (2, 2),         (4, 1)],\n",
    "    \"rabbit\":  [                (2, 1), (3, 1)        ],\n",
    "    \"recipe\":  [                (2, 1), (3, 1), (4, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document lengths\n",
    "\n",
    "The length of each document is provided in a list (Normally, this information would be present in a document index).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = [3, 4, 4, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time scoring\n",
    "\n",
    "We utilize the fact that the posting lists are ordered by document ID. Then, it's enough to iterate through each query term's posting list only once. We keep a score accumulator for each document.\n",
    "\n",
    "The retrieval function we use is the following:\n",
    "\n",
    "$$score(q,d) = \\sum_{t \\in q} w_{t,d} \\times w_{t,q}$$\n",
    "\n",
    "where $w_{t,d}$ and $w_{t,q}$ are length-normalized term frequencies, i.e., $w_{t,d} = \\frac{c_{t,d}}{|d|}$, where $c_{t,d}$ is the number of occurrences of term $t$ in document $d$ and $|d|$ is the document length, i.e., the total number of terms. Similarly for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_collection(index: Dict[str, List[Tuple[int, int]]], \n",
    "                     doc_len: List[int], \n",
    "                     query: str) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Scores all documents in the collection.\n",
    "    \n",
    "    Args:\n",
    "        index: Dict holding the inverted index.\n",
    "        doc_len: List with document lengths.\n",
    "        query: Search query.\n",
    "    \n",
    "    Returns:\n",
    "        List with (document_id, score) tuples, ordered by score desc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turns the query string into a \"term: freq\" dictionary.\n",
    "    query_freqs = dict(Counter(query.split()))\n",
    "    # Computes query length (i.e., sum of all query term frequencies).\n",
    "    query_len = sum(query_freqs.values())\n",
    "\n",
    "    doc_scores = {docid: 0 for docid in range(len(doc_len))}  # Holds the final document scores (accumulator).\n",
    "    \n",
    "    # Iterate through each posting list.\n",
    "    for term, tf_q in query_freqs.items():\n",
    "        # TODO: TAAT scoring algorithm        \n",
    "\n",
    "    # TODO: return doc_scores sorted\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_scoring():\n",
    "    scores = score_collection(index, doc_len, \"beijing duck recipe\")    \n",
    "    assert scores[0][0] == 0\n",
    "    assert scores[0][1] == pytest.approx(1/3, rel=1e-2)\n",
    "    assert scores[2][0] == 2\n",
    "    assert scores[2][1] == pytest.approx(1/4, rel=1e-2)\n",
    "    assert scores[4][0] == 3\n",
    "    assert scores[4][1] == pytest.approx(1/6, rel=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5e4a7eaa80b8b0cab0ebfd023b62b4fb81ced96acf97d50ef5e35c1384dc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
