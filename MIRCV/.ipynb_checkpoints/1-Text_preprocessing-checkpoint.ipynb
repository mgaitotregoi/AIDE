{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "In this exercise, you'll need to implement basic text preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "import ipytest\n",
    "import string\n",
    "import re\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tokenization\n",
    "\n",
    "Split an input text into tokens based on whitespaces, punctuation, hyphens, and HTML markup. Additionally, lowercase all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str)-> List[str]:    \n",
    "    \"\"\"Returns a sequence of terms given an input text.\"\"\"\n",
    "    # Your solution here\n",
    "    text = text.lower()\n",
    "    punctuation = list(string.punctuation)\n",
    "    PATTERN = r\"\\s+ | \"\n",
    "    res = re.split(PATTERN, text)\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                         [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_________________________________________ test_punctuation _________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_punctuation\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa! bbb.ccc,ddd:eee ff\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbbb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mddd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33meee\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       assert ['aaa!', 'bbb...:eee', 'ff\"f'] == ['aaa', 'bbb'...e', 'ff', ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 0 diff: 'aaa!' != 'aaa'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains 4 more items, first extra item: 'ddd'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_2754/2160663131.py\u001b[0m:5: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "['aaa!', 'bbb.ccc,ddd:eee', 'ff\"f']\n",
      "\u001b[31m\u001b[1m___________________________________________ test_hyphens ___________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_hyphens\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa bbb-Ccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbbb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert ['aaa', 'bbb-ccc'] == ['aaa', 'bbb', 'ccc']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 1 diff: 'bbb-ccc' != 'bbb'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains one more item: 'ccc'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_2754/2160663131.py\u001b[0m:8: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "['aaa', 'bbb-ccc']\n",
      "\u001b[31m\u001b[1m____________________________________________ test_html _____________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_html\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(\u001b[33m\"\u001b[39;49;00m\u001b[33maaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == [\u001b[33m\"\u001b[39;49;00m\u001b[33maaa\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mccc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33meee\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert ['aaa', '<bbb... '<ggg', '/>'] == ['aaa', 'ccc', 'eee', 'fff']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 1 diff: '<bbb>ccc' != 'ccc'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains one more item: '/>'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_2754/2160663131.py\u001b[0m:11: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "['aaa', '<bbb>ccc', '<ddd>eee</ddd></bbb>fff', '<ggg', '/>']\n",
      "===================================== short test summary info ======================================\n",
      "FAILED tmpijliemnv.py::test_punctuation - assert ['aaa!', 'bbb...:eee', 'ff\"f'] == ['aaa', 'bbb'....\n",
      "FAILED tmpijliemnv.py::test_hyphens - AssertionError: assert ['aaa', 'bbb-ccc'] == ['aaa', 'bbb',...\n",
      "FAILED tmpijliemnv.py::test_html - AssertionError: assert ['aaa', '<bbb... '<ggg', '/>'] == ['aaa...\n",
      "\u001b[31m\u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_whitespace():\n",
    "    assert tokenize(\"aaa bbb ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_punctuation():\n",
    "    assert tokenize(\"aaa! bbb.ccc,ddd:eee ff\\\"f\") == [\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"ff\", \"f\"]\n",
    "    \n",
    "def test_hyphens():\n",
    "    assert tokenize(\"aaa bbb-Ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_html():\n",
    "    assert tokenize(\"aaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\") == [\"aaa\", \"ccc\", \"eee\", \"fff\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Stopwords removal\n",
    "\n",
    "Remove stopwords from a sequence of tokens, given a set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens: List[str], stopwords: Set[str]) -> List[str]:\n",
    "    \"\"\"Removes stopwords from a sequence of tokens.\"\"\"\n",
    "    # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_no_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {}) == [\"this\", \"is\", \"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {\"is\", \"this\"}) == [\"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords2():\n",
    "    assert remove_stopwords([\"this\", \"isolate\", \"otto\"], {\"is\", \"this\", \"to\"}) == [\"isolate\", \"otto\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Suffix-s stemming\n",
    "\n",
    "Remove the s-suffix from all terms in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_s_stemmer(terms: List[str]) -> List[str]:\n",
    "    \"\"\"Removes the s-suffix from all terms in a sequence.\"\"\"\n",
    "    # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_stemming():\n",
    "    assert suffix_s_stemmer([\"dogs\", \"better\", \"cats\"]) == [\"dog\", \"better\", \"cat\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5e4a7eaa80b8b0cab0ebfd023b62b4fb81ced96acf97d50ef5e35c1384dc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
