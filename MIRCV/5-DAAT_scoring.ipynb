{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-at-a-time Query Processing\n",
    "\n",
    "Implement document-at-a-time query processing using a simple scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index\n",
    "\n",
    "For simplicity, the inverted index for the document collection is given as a dictionary, with a terms as keys and posting lists as values. Each posting is a (document ID, term frequency) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {\n",
    "    \"beijing\": [        (1, 1),                 (4, 1)],\n",
    "    \"dish\":    [        (1, 1),                 (4, 1)],\n",
    "    \"duck\":    [(0, 3), (1, 2), (2, 2),         (4, 1)],\n",
    "    \"rabbit\":  [                (2, 1), (3, 1)        ],\n",
    "    \"recipe\":  [                (2, 1), (3, 1), (4, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document lengths\n",
    "\n",
    "The length of each document is provided in a list (Normally, this information would be present in a document index).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = [3, 4, 4, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time scoring\n",
    "\n",
    "The retrieval function we use is the following:\n",
    "\n",
    "$$score(q,d) = \\sum_{t \\in q} w_{t,d} \\times w_{t,q}$$\n",
    "\n",
    "where $w_{t,d}$ and $w_{t,q}$ are length-normalized term frequencies, i.e., $w_{t,d} = \\frac{c_{t,d}}{|d|}$, where $c_{t,d}$ is the number of occurrences of term $t$ in document $d$ and $|d|$ is the document length, i.e., the total number of terms. Similarly for the query.\n",
    "\n",
    "We utilize the fact that the posting lists are ordered by document ID. Then, it's enough to iterate in parallel through term query term's posting list, and score the minimum docid at each iteration. We keep a pointer for each query term and we move it forward every time a docid is scored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an helper function that, given a dict of posting list pointers `pos`, returns the minimum docid. If all pointers go beyond the end of the corresponding posting list, we return `num_docs`, a special docid that it is guaranteed to not appear in any posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_docid(index: Dict[str, List[Tuple[int, int]]], \n",
    "              pos: Dict[str, int],\n",
    "              num_docs: int) -> int:\n",
    "    \"\"\"Returns the minimum docid across posting lists.\n",
    "    \n",
    "    Args:\n",
    "        index: Dict holding the inverted index.\n",
    "        pos: the current positions in the query posting lists.\n",
    "        num_docs: the number of indexed documents.\n",
    "    \n",
    "    Returns:\n",
    "        the minimum docid across the posting lists, or num_docs if all \n",
    "        postings in all posting lists have been processed.\n",
    "    \"\"\"\n",
    "    min_docid = num_docs\n",
    "\n",
    "    # For each query term posting list.\n",
    "    for term, pointer in pos.items():\n",
    "         # TODO: check if the posting list contains a valid docid; if so, update min_docid\n",
    "        docid = index[term][pointer][0]\n",
    "        if min_docid > docid:\n",
    "            min_docid = docid\n",
    "\n",
    "    for term, pointer in pos.items():\n",
    "        print(term)\n",
    "        print(pointer)\n",
    "        if index[term][pointer][0] == min_docid and pointer < len(index[term]):\n",
    "            pos[term] = pointer + 1\n",
    "            \n",
    "\n",
    "    # Return the min docid computed.\n",
    "    return min_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_collection(index: Dict[str, List[Tuple[int, int]]], \n",
    "                      doc_len: List[int], \n",
    "                      query: str) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Scores all documents in the collection.\n",
    "    \n",
    "    Args:\n",
    "        index: Dict holding the inverted index.\n",
    "        doc_len: List with document lengths.\n",
    "        query: Search query.\n",
    "    \n",
    "    Returns:\n",
    "        List with (document_id, score) tuples, ordered by score desc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turns the query string into a \"term: freq\" dictionary.\n",
    "    query_freqs = dict(Counter(query.split()))\n",
    "\n",
    "    # Computes query length (i.e., sum of all query term frequencies).\n",
    "    query_len = sum(query_freqs.values())\n",
    "\n",
    "    doc_scores = {}  # Holds the final document scores (this should be a priority list, but for simplicity we use a dictionary here).\n",
    "    \n",
    "    pos = {term: 0 for term in query_freqs}  # Holds a pointer for each query term's posting list.\n",
    "    \n",
    "    # The starting docid.\n",
    "    docid = min_docid(index, pos, len(doc_len))\n",
    "    # While the posting lists have not been completely traversed.\n",
    "    while docid != len(doc_len):\n",
    "        score = 0.0\n",
    "        # TODO: DAAT scoring algorithm\n",
    "        for term, pointer in pos.items():\n",
    "            back_pointer = pointer - 1\n",
    "            if pointer == 0 or index[term][back_pointer][0] != docid:\n",
    "                continue\n",
    "            print(\"processo\")\n",
    "            weight = index[term][back_pointer][1] / doc_len[docid]\n",
    "            partial_score = weight * (query_freqs[term] / query_len)\n",
    "            score += partial_score\n",
    "        doc_scores[docid] = score\n",
    "        print(doc_scores)\n",
    "        # Update the docid.\n",
    "        break\n",
    "        docid = min_docid(index, pos, len(doc_len))\n",
    "    # TODO: return doc_scores sorted\n",
    "    return sorted(doc_scores.items(), key=lambda item: item[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mF\u001B[0m\u001B[31m                                                                                            [100%]\u001B[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001B[31m\u001B[1m___________________________________________ test_scoring ___________________________________________\u001B[0m\n",
      "\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_scoring\u001B[39;49;00m():\n",
      "        scores = score_collection(index, doc_len, \u001B[33m\"\u001B[39;49;00m\u001B[33mbeijing duck recipe\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\n",
      "        \u001B[94massert\u001B[39;49;00m scores[\u001B[94m0\u001B[39;49;00m][\u001B[94m0\u001B[39;49;00m] == \u001B[94m0\u001B[39;49;00m\n",
      "        \u001B[94massert\u001B[39;49;00m scores[\u001B[94m0\u001B[39;49;00m][\u001B[94m1\u001B[39;49;00m] == pytest.approx(\u001B[94m1\u001B[39;49;00m/\u001B[94m3\u001B[39;49;00m, rel=\u001B[94m1e-2\u001B[39;49;00m)\n",
      ">       \u001B[94massert\u001B[39;49;00m scores[\u001B[94m2\u001B[39;49;00m][\u001B[94m0\u001B[39;49;00m] == \u001B[94m2\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       IndexError: list index out of range\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31m/var/folders/jq/sxfhvkbn11s0jzgvkdhwp6v00000gn/T/ipykernel_3440/224187975.py\u001B[0m:5: IndexError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "beijing\n",
      "0\n",
      "duck\n",
      "0\n",
      "recipe\n",
      "0\n",
      "processo\n",
      "{0: 0.3333333333333333}\n",
      "===================================== short test summary info ======================================\n",
      "FAILED tmppguo9ixa.py::test_scoring - IndexError: list index out of range\n",
      "\u001B[31m\u001B[31m\u001B[1m1 failed\u001B[0m\u001B[31m in 0.02s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_scoring():\n",
    "    scores = score_collection(index, doc_len, \"beijing duck recipe\")    \n",
    "    assert scores[0][0] == 0\n",
    "    assert scores[0][1] == pytest.approx(1/3, rel=1e-2)\n",
    "    assert scores[2][0] == 2\n",
    "    assert scores[2][1] == pytest.approx(1/4, rel=1e-2)\n",
    "    assert scores[4][0] == 3\n",
    "    assert scores[4][1] == pytest.approx(1/6, rel=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5e4a7eaa80b8b0cab0ebfd023b62b4fb81ced96acf97d50ef5e35c1384dc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}