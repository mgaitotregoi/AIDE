{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgiorgi13/AIDE/blob/main/pix2pixHD_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "N-fvIhrKELDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pF_YMYXAtxV",
        "outputId": "f5ac1503-2a07-49ca-9297-d2be8fceda7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dominate\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dominate gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import gdown"
      ],
      "metadata": {
        "id": "DW2OEAxLB1ni"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qMC8BzYnB-Mq",
        "outputId": "8b8bc37c-7b3d-4077-d1da-4e7a9c34d80b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"drive/MyDrive/Haze-Fog-suppression\"\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "0vFtFOdwCHWs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download library for no reference metrics"
      ],
      "metadata": {
        "id": "f2F05uey2amY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/buyizhiyou/NRVQA.git"
      ],
      "metadata": {
        "id": "xgW4xriY17oA",
        "outputId": "424ff3eb-4e2f-4ac7-cc6d-238ae2cfa21e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NRVQA'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 166 (delta 43), reused 131 (delta 22), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (166/166), 9.97 MiB | 11.47 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Apex for Automatic Mixed Precision to speed up training (NOT WORKING)"
      ],
      "metadata": {
        "id": "8GO4-qJQlpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "cLaTD67wlxW_",
        "outputId": "7321fd9d-5eb9-400f-bb79-a4edf163e797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Processing /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info\n",
            "  writing /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-k24uqvr5/apex-0.1.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/dist-packages (from apex==0.1) (23.1)\n",
            "Building wheels for collected packages: apex\n",
            "  Running command Building wheel for apex (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "\n",
            "  Compiling cuda extensions with\n",
            "  nvcc: NVIDIA (R) Cuda compiler driver\n",
            "  Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "  Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "  Cuda compilation tools, release 11.8, V11.8.89\n",
            "  Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "  from /usr/local/cuda/bin\n",
            "\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-cpython-310\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/__init__.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/_ucc_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  copying apex/contrib/test/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/_transducer_ref.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/test_fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'apex_C' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'amp_C' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'syncbn' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-cpython-310/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/welford.cu -o build/temp.linux-x86_64-cpython-310/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/syncbn.o build/temp.linux-x86_64-cpython-310/csrc/welford.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_layer_norm_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp.cpp -o build/temp.linux-x86_64-cpython-310/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "  csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "     57 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:64:76: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "        |                                                              ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:65:85: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "        |                                                                       ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:67:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "        |                                            ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:69:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "  csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "    115 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
            "    120 |   for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                   ~~^~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:121:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "        |                                                    ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:124:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/mlp.o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_dense_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "  csrc/fused_dense.cpp:30:62: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     30 |   auto out = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                    ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:33:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "  csrc/fused_dense.cpp:64:68: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "        |                                                          ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:68:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     68 |   auto d_bias = at::empty({out_features}, input.type());\n",
            "        |                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:70:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:73:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "  csrc/fused_dense.cpp:106:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:107:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:108:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                        ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:111:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "  csrc/fused_dense.cpp:149:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "        |                                                              ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:150:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "        |                                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:151:57: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "        |                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:152:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    152 |   auto d_bias2 = at::empty({out_features}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:153:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:154:71: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                             ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:157:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc/megatron\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'generic_scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_weight_gradient_mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing requirements to apex.egg-info/requires.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-d3ttvz1h/.tmp-eld9dset/apex-0.1-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'amp_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_dense_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'syncbn.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/group_norm/group_norm.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/test/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
            "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
            "  adding 'apex/contrib/test/fmha/__init__.py'\n",
            "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
            "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
            "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
            "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
            "  adding 'apex/contrib/test/transducer/__init__.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
            "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/_ucc_util.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp310-cp310-linux_x86_64.whl size=32236627 sha256=8d0f5d57c52dd93eda408da02959a0dc706e869e8b8c56c93333c8ac6452e593\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ep_u209/wheels/af/00/73/0e6dbf1a46334a3c4c0360bcdaf1085990eaf534b2bb7efefb\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility"
      ],
      "metadata": {
        "id": "CcWqZ45pecYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import niqe\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "\n",
        "\n",
        "# Definisci i percorsi delle cartelle contenenti le immagini\n",
        "indoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/indoor/synth/nebbia/test_100/images\"  # Cartella con le immagini generate\n",
        "indoor_original_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/indoor/test_A\"        # Cartella con le immagini originali\n",
        "outdoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/outdoor/synth/nebbia/test_100/images\"  # Cartella con le immagini generate\n",
        "outdoor_original_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/outdoor/test_A\"       # Cartella con le immagini originali\n",
        "\n",
        "# salva in pix2pixHD/results le metriche\n",
        "def calculate_metrics(synth_folder, gt_folder,type):\n",
        "    # Ottieni la lista dei nomi di file dalle cartelle\n",
        "\n",
        "    image_files = os.listdir(synth_folder)\n",
        "\n",
        "    # Filtra i nomi dei file per rimuovere quelli che contengono \"input_label\"\n",
        "    filtered_image_files = [file for file in image_files if \"input_label\" not in file]\n",
        "\n",
        "    # Inizializza liste per memorizzare i risultati\n",
        "    mse_scores = []\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    piqe_scores = []\n",
        "    niqe_scores = []\n",
        "\n",
        "    # Itera attraverso le immagini e calcola le metriche\n",
        "    for image_file in filtered_image_files:\n",
        "        # Carica le immagini\n",
        "        synth_image = cv2.imread(os.path.join(synth_folder, image_file))\n",
        "        gt_image = cv2.imread(os.path.join(gt_folder, image_file[:4]+\".png\"))\n",
        "\n",
        "        # Ridimensiona l'immagine sintetizzata alla stessa dimensione dell'immagine originale\n",
        "        synth_image = cv2.resize(synth_image, (gt_image.shape[1], gt_image.shape[0]))\n",
        "\n",
        "        # Calcola le metriche con skimage\n",
        "        mse = np.mean((gt_image - synth_image) ** 2)\n",
        "        ssim_score = ssim(gt_image, synth_image, channel_axis=2)\n",
        "        psnr_score = psnr(gt_image, synth_image)\n",
        "\n",
        "        # Calcola le metriche con piqe e niqe (high score means low quality)\n",
        "        synth_array = synth_image / 255.0\n",
        "\n",
        "        # Esegui il comando e cattura l'output\n",
        "        completed_process = subprocess.run(['python', '/content/drive/MyDrive/Haze-Fog-suppression/NRVQA/test.py', '--mode', 'piqe', f'--path={os.path.join(synth_folder, image_file)}'], stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "        # Ottieni l'output dall'oggetto CompletedProcess\n",
        "        output = completed_process.stdout\n",
        "\n",
        "        # Estrai lo score dalla stringa di output\n",
        "        score_line = [line for line in output.splitlines() if 'score:' in line][0]\n",
        "        piqe_score = float(score_line.split(':')[-1])\n",
        "\n",
        "        # Esegui il comando e cattura l'output\n",
        "        completed_process = subprocess.run(['python', '/content/drive/MyDrive/Haze-Fog-suppression/NRVQA/test.py', '--mode', 'niqe', f'--path={os.path.join(synth_folder, image_file)}'], stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "        # Ottieni l'output dall'oggetto CompletedProcess\n",
        "        output = completed_process.stdout\n",
        "\n",
        "        # Estrai lo score dalla stringa di output\n",
        "        score_line = [line for line in output.splitlines() if 'score:' in line][0]\n",
        "        niqe_score = float(score_line.split(':')[-1])\n",
        "\n",
        "        # Aggiungi i risultati alle liste\n",
        "        mse_scores.append(mse)\n",
        "        ssim_scores.append(ssim_score)\n",
        "        psnr_scores.append(psnr_score)\n",
        "        piqe_scores.append(piqe_score)\n",
        "        niqe_scores.append(niqe_score)\n",
        "\n",
        "\n",
        "    # Creazione del dataframe\n",
        "    data = {\n",
        "        'Image': filtered_image_files,  # Nomi dei file delle immagini generate\n",
        "        'MSE': mse_scores,\n",
        "        'SSIM': ssim_scores,\n",
        "        'PSNR': psnr_scores,\n",
        "        'PIQE': piqe_scores,\n",
        "        'NIQE': niqe_scores\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Calcolo dei valori medi\n",
        "    average_scores = df[['MSE', 'SSIM', 'PSNR', 'PIQE', 'NIQE']].mean()\n",
        "\n",
        "    # Aggiunta della riga con i valori medi al dataframe\n",
        "    average_scores_df = pd.DataFrame([average_scores], columns=['MSE', 'SSIM', 'PSNR', 'PIQE', 'NIQE'])\n",
        "    average_scores_df['Image'] = 'AVG'\n",
        "    # Aggiungi una riga vuota tra i risultati delle immagini e i valori medi\n",
        "    df = pd.concat([df, pd.DataFrame(columns=df.columns)], ignore_index=True)\n",
        "    df = pd.concat([df, average_scores_df], ignore_index=True)\n",
        "\n",
        "    # Salva il dataframe completo in un file CSV\n",
        "    df.to_csv(type + '_result.csv', index=False)\n",
        "\n",
        "def test_indoor_outdoor(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder):\n",
        "  calculate_metrics(indoor_synth_folder, indoor_original_folder,\"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results/indoor\")\n",
        "  calculate_metrics(outdoor_synth_folder, outdoor_original_folder,\"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results/outdoor\")"
      ],
      "metadata": {
        "id": "5OG31gwjegsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD download\n"
      ],
      "metadata": {
        "id": "6eqPZHOXEAfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YoX7TkTS9MG",
        "outputId": "09120a1d-5280-4fed-f4ba-e3f727363778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pix2pixHD'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 55.68 MiB | 20.18 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n",
            "Updating files: 100% (115/115), done.\n",
            "/content/drive/MyDrive/Haze-suppression/pix2pixHD\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/pix2pixHD.git\n",
        "os.chdir(\"pix2pixHD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKuREuwyAtxd"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"./checkpoints/label2city_1024p/\")\n",
        "os.chdir(\"./checkpoints/label2city_1024p/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download'\n",
        "output = 'latest_net_G.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "5VRByC-oFDWv",
        "outputId": "76edc983-ec8a-4868-c3e7-80a9d0229ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download\n",
            "To: /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/checkpoints/label2city_1024p/latest_net_G.pth\n",
            "100%|██████████| 732M/732M [00:11<00:00, 62.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'latest_net_G.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"../..\")"
      ],
      "metadata": {
        "id": "zpELF8NXHBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD Train\n",
        "train the model with image with different level of fog\n"
      ],
      "metadata": {
        "id": "CFdeWg-Sqj6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pix2pixHD/"
      ],
      "metadata": {
        "id": "RxD4lD7grQjI",
        "outputId": "bb73f69e-9ecf-4c53-ab5a-d88477a4c637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "cBS7uIxAqn6W",
        "outputId": "98797bcc-129e-4794-b804-86e71a2df6eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 97 at iteration 1800\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:07<00:00, 81.8MB/s]\n",
            "create web directory ./checkpoints/nebbia/web...\n",
            "(epoch: 97, iters: 1900, time: 1.948) G_GAN: 2.089 G_GAN_Feat: 2.718 G_VGG: 1.824 D_real: 0.113 D_fake: 0.053 \n",
            "(epoch: 97, iters: 2000, time: 0.559) G_GAN: 1.776 G_GAN_Feat: 2.576 G_VGG: 1.368 D_real: 0.017 D_fake: 0.116 \n",
            "(epoch: 97, iters: 2100, time: 0.551) G_GAN: 1.953 G_GAN_Feat: 2.653 G_VGG: 1.674 D_real: 0.014 D_fake: 0.010 \n",
            "(epoch: 97, iters: 2200, time: 0.553) G_GAN: 1.932 G_GAN_Feat: 2.998 G_VGG: 2.237 D_real: 0.007 D_fake: 0.007 \n",
            "(epoch: 97, iters: 2300, time: 0.554) G_GAN: 1.616 G_GAN_Feat: 2.476 G_VGG: 1.229 D_real: 0.017 D_fake: 0.143 \n",
            "(epoch: 97, iters: 2400, time: 0.552) G_GAN: 1.637 G_GAN_Feat: 2.797 G_VGG: 1.949 D_real: 0.007 D_fake: 0.139 \n",
            "(epoch: 97, iters: 2500, time: 0.554) G_GAN: 1.909 G_GAN_Feat: 2.727 G_VGG: 1.499 D_real: 0.069 D_fake: 0.079 \n",
            "(epoch: 97, iters: 2600, time: 0.552) G_GAN: 2.069 G_GAN_Feat: 2.842 G_VGG: 1.586 D_real: 0.011 D_fake: 0.013 \n",
            "(epoch: 97, iters: 2700, time: 0.552) G_GAN: 1.790 G_GAN_Feat: 2.722 G_VGG: 1.862 D_real: 0.006 D_fake: 0.036 \n",
            "(epoch: 97, iters: 2800, time: 0.553) G_GAN: 1.904 G_GAN_Feat: 2.943 G_VGG: 1.584 D_real: 0.011 D_fake: 0.012 \n",
            "saving the latest model (epoch 97, total_steps 406000)\n",
            "(epoch: 97, iters: 2900, time: 0.556) G_GAN: 2.038 G_GAN_Feat: 2.717 G_VGG: 1.657 D_real: 0.266 D_fake: 0.025 \n",
            "(epoch: 97, iters: 3000, time: 0.553) G_GAN: 1.662 G_GAN_Feat: 2.671 G_VGG: 1.727 D_real: 0.015 D_fake: 0.081 \n",
            "(epoch: 97, iters: 3100, time: 0.553) G_GAN: 1.356 G_GAN_Feat: 2.756 G_VGG: 1.185 D_real: 0.059 D_fake: 0.270 \n",
            "(epoch: 97, iters: 3200, time: 0.553) G_GAN: 1.977 G_GAN_Feat: 3.140 G_VGG: 1.712 D_real: 0.016 D_fake: 0.009 \n",
            "(epoch: 97, iters: 3300, time: 0.554) G_GAN: 1.853 G_GAN_Feat: 2.989 G_VGG: 1.877 D_real: 0.029 D_fake: 0.078 \n",
            "(epoch: 97, iters: 3400, time: 0.554) G_GAN: 1.914 G_GAN_Feat: 2.883 G_VGG: 1.630 D_real: 0.063 D_fake: 0.017 \n",
            "(epoch: 97, iters: 3500, time: 0.554) G_GAN: 1.857 G_GAN_Feat: 2.601 G_VGG: 1.882 D_real: 0.016 D_fake: 0.048 \n",
            "(epoch: 97, iters: 3600, time: 0.553) G_GAN: 1.812 G_GAN_Feat: 2.531 G_VGG: 1.410 D_real: 0.020 D_fake: 0.026 \n",
            "(epoch: 97, iters: 3700, time: 0.553) G_GAN: 1.556 G_GAN_Feat: 2.395 G_VGG: 1.364 D_real: 0.058 D_fake: 0.119 \n",
            "(epoch: 97, iters: 3800, time: 0.554) G_GAN: 1.709 G_GAN_Feat: 2.822 G_VGG: 1.580 D_real: 0.009 D_fake: 0.081 \n",
            "saving the latest model (epoch 97, total_steps 407000)\n",
            "(epoch: 97, iters: 3900, time: 0.556) G_GAN: 1.977 G_GAN_Feat: 3.325 G_VGG: 2.262 D_real: 0.013 D_fake: 0.006 \n",
            "(epoch: 97, iters: 4000, time: 0.553) G_GAN: 2.256 G_GAN_Feat: 3.136 G_VGG: 1.894 D_real: 0.343 D_fake: 0.028 \n",
            "(epoch: 97, iters: 4100, time: 0.554) G_GAN: 1.617 G_GAN_Feat: 2.395 G_VGG: 1.395 D_real: 0.028 D_fake: 0.071 \n",
            "(epoch: 97, iters: 4200, time: 0.554) G_GAN: 1.833 G_GAN_Feat: 3.102 G_VGG: 1.853 D_real: 0.026 D_fake: 0.019 \n",
            "End of epoch 97 / 200 \t Time Taken: 1491 sec\n",
            "(epoch: 98, iters: 100, time: 0.554) G_GAN: 1.927 G_GAN_Feat: 2.838 G_VGG: 2.003 D_real: 0.012 D_fake: 0.011 \n",
            "(epoch: 98, iters: 200, time: 0.552) G_GAN: 1.688 G_GAN_Feat: 3.145 G_VGG: 2.240 D_real: 0.020 D_fake: 0.047 \n",
            "(epoch: 98, iters: 300, time: 0.552) G_GAN: 2.031 G_GAN_Feat: 3.002 G_VGG: 1.869 D_real: 0.027 D_fake: 0.008 \n",
            "(epoch: 98, iters: 400, time: 0.551) G_GAN: 2.198 G_GAN_Feat: 2.770 G_VGG: 1.639 D_real: 0.063 D_fake: 0.022 \n",
            "(epoch: 98, iters: 500, time: 0.550) G_GAN: 2.254 G_GAN_Feat: 2.304 G_VGG: 1.164 D_real: 0.129 D_fake: 0.049 \n",
            "(epoch: 98, iters: 600, time: 0.549) G_GAN: 2.200 G_GAN_Feat: 3.070 G_VGG: 1.983 D_real: 0.030 D_fake: 0.019 \n",
            "saving the latest model (epoch 98, total_steps 408000)\n",
            "(epoch: 98, iters: 700, time: 0.546) G_GAN: 1.991 G_GAN_Feat: 2.614 G_VGG: 1.670 D_real: 0.149 D_fake: 0.010 \n",
            "(epoch: 98, iters: 800, time: 0.547) G_GAN: 1.613 G_GAN_Feat: 2.951 G_VGG: 1.669 D_real: 0.044 D_fake: 0.132 \n",
            "(epoch: 98, iters: 900, time: 0.549) G_GAN: 1.858 G_GAN_Feat: 2.699 G_VGG: 1.811 D_real: 0.011 D_fake: 0.028 \n",
            "(epoch: 98, iters: 1000, time: 0.549) G_GAN: 1.930 G_GAN_Feat: 2.959 G_VGG: 1.917 D_real: 0.062 D_fake: 0.012 \n",
            "(epoch: 98, iters: 1100, time: 0.551) G_GAN: 2.304 G_GAN_Feat: 2.793 G_VGG: 1.849 D_real: 0.288 D_fake: 0.037 \n",
            "(epoch: 98, iters: 1200, time: 0.550) G_GAN: 1.669 G_GAN_Feat: 2.524 G_VGG: 1.611 D_real: 0.012 D_fake: 0.072 \n",
            "(epoch: 98, iters: 1300, time: 0.549) G_GAN: 1.636 G_GAN_Feat: 3.138 G_VGG: 2.353 D_real: 0.021 D_fake: 0.043 \n",
            "(epoch: 98, iters: 1400, time: 0.549) G_GAN: 1.778 G_GAN_Feat: 3.325 G_VGG: 1.740 D_real: 0.015 D_fake: 0.108 \n",
            "(epoch: 98, iters: 1500, time: 0.549) G_GAN: 1.687 G_GAN_Feat: 2.926 G_VGG: 1.550 D_real: 0.091 D_fake: 0.056 \n",
            "(epoch: 98, iters: 1600, time: 0.549) G_GAN: 2.187 G_GAN_Feat: 3.097 G_VGG: 1.893 D_real: 0.008 D_fake: 0.011 \n",
            "saving the latest model (epoch 98, total_steps 409000)\n",
            "(epoch: 98, iters: 1700, time: 0.547) G_GAN: 1.690 G_GAN_Feat: 2.706 G_VGG: 1.697 D_real: 0.030 D_fake: 0.085 \n",
            "(epoch: 98, iters: 1800, time: 0.547) G_GAN: 1.616 G_GAN_Feat: 2.825 G_VGG: 1.713 D_real: 0.013 D_fake: 0.079 \n",
            "(epoch: 98, iters: 1900, time: 0.548) G_GAN: 1.762 G_GAN_Feat: 2.624 G_VGG: 1.745 D_real: 0.164 D_fake: 0.056 \n",
            "(epoch: 98, iters: 2000, time: 0.548) G_GAN: 1.504 G_GAN_Feat: 2.842 G_VGG: 1.521 D_real: 0.449 D_fake: 0.101 \n",
            "(epoch: 98, iters: 2100, time: 0.549) G_GAN: 1.847 G_GAN_Feat: 2.831 G_VGG: 1.847 D_real: 0.032 D_fake: 0.034 \n",
            "(epoch: 98, iters: 2200, time: 0.549) G_GAN: 1.800 G_GAN_Feat: 2.856 G_VGG: 1.645 D_real: 0.042 D_fake: 0.061 \n",
            "(epoch: 98, iters: 2300, time: 0.549) G_GAN: 1.746 G_GAN_Feat: 2.572 G_VGG: 1.538 D_real: 0.010 D_fake: 0.088 \n",
            "(epoch: 98, iters: 2400, time: 0.549) G_GAN: 1.868 G_GAN_Feat: 2.759 G_VGG: 1.814 D_real: 0.365 D_fake: 0.023 \n",
            "(epoch: 98, iters: 2500, time: 0.549) G_GAN: 1.944 G_GAN_Feat: 2.746 G_VGG: 1.666 D_real: 0.019 D_fake: 0.015 \n",
            "(epoch: 98, iters: 2600, time: 0.549) G_GAN: 1.981 G_GAN_Feat: 2.628 G_VGG: 1.677 D_real: 0.016 D_fake: 0.013 \n",
            "saving the latest model (epoch 98, total_steps 410000)\n",
            "(epoch: 98, iters: 2700, time: 0.547) G_GAN: 1.858 G_GAN_Feat: 2.768 G_VGG: 1.671 D_real: 0.039 D_fake: 0.054 \n",
            "(epoch: 98, iters: 2800, time: 0.547) G_GAN: 2.111 G_GAN_Feat: 3.190 G_VGG: 2.182 D_real: 0.059 D_fake: 0.012 \n",
            "(epoch: 98, iters: 2900, time: 0.548) G_GAN: 2.024 G_GAN_Feat: 3.254 G_VGG: 1.935 D_real: 0.094 D_fake: 0.010 \n",
            "(epoch: 98, iters: 3000, time: 0.549) G_GAN: 1.113 G_GAN_Feat: 2.415 G_VGG: 1.216 D_real: 0.052 D_fake: 0.547 \n",
            "(epoch: 98, iters: 3100, time: 0.549) G_GAN: 1.857 G_GAN_Feat: 2.785 G_VGG: 1.481 D_real: 0.018 D_fake: 0.016 \n",
            "(epoch: 98, iters: 3200, time: 0.549) G_GAN: 1.943 G_GAN_Feat: 2.736 G_VGG: 1.410 D_real: 0.011 D_fake: 0.009 \n",
            "(epoch: 98, iters: 3300, time: 0.549) G_GAN: 2.116 G_GAN_Feat: 2.958 G_VGG: 2.145 D_real: 0.016 D_fake: 0.016 \n",
            "(epoch: 98, iters: 3400, time: 0.549) G_GAN: 1.920 G_GAN_Feat: 3.165 G_VGG: 1.807 D_real: 0.021 D_fake: 0.017 \n",
            "(epoch: 98, iters: 3500, time: 0.549) G_GAN: 1.977 G_GAN_Feat: 2.793 G_VGG: 1.292 D_real: 0.022 D_fake: 0.007 \n",
            "(epoch: 98, iters: 3600, time: 0.549) G_GAN: 1.816 G_GAN_Feat: 2.809 G_VGG: 2.106 D_real: 0.023 D_fake: 0.041 \n",
            "saving the latest model (epoch 98, total_steps 411000)\n",
            "(epoch: 98, iters: 3700, time: 0.547) G_GAN: 1.512 G_GAN_Feat: 2.545 G_VGG: 1.556 D_real: 0.010 D_fake: 0.156 \n",
            "(epoch: 98, iters: 3800, time: 0.548) G_GAN: 1.895 G_GAN_Feat: 2.567 G_VGG: 1.677 D_real: 0.060 D_fake: 0.038 \n",
            "(epoch: 98, iters: 3900, time: 0.548) G_GAN: 2.115 G_GAN_Feat: 2.609 G_VGG: 1.530 D_real: 0.025 D_fake: 0.008 \n",
            "(epoch: 98, iters: 4000, time: 0.549) G_GAN: 1.682 G_GAN_Feat: 2.741 G_VGG: 2.013 D_real: 0.055 D_fake: 0.047 \n",
            "(epoch: 98, iters: 4100, time: 0.549) G_GAN: 1.692 G_GAN_Feat: 2.344 G_VGG: 1.172 D_real: 0.050 D_fake: 0.110 \n",
            "(epoch: 98, iters: 4200, time: 0.549) G_GAN: 1.891 G_GAN_Feat: 2.831 G_VGG: 1.953 D_real: 0.011 D_fake: 0.016 \n",
            "End of epoch 98 / 200 \t Time Taken: 2329 sec\n",
            "(epoch: 99, iters: 100, time: 0.549) G_GAN: 2.074 G_GAN_Feat: 2.720 G_VGG: 1.932 D_real: 0.035 D_fake: 0.016 \n",
            "(epoch: 99, iters: 200, time: 0.549) G_GAN: 1.184 G_GAN_Feat: 2.494 G_VGG: 1.661 D_real: 0.026 D_fake: 0.541 \n",
            "(epoch: 99, iters: 300, time: 0.549) G_GAN: 1.913 G_GAN_Feat: 2.700 G_VGG: 1.563 D_real: 0.031 D_fake: 0.025 \n",
            "(epoch: 99, iters: 400, time: 0.549) G_GAN: 1.729 G_GAN_Feat: 2.747 G_VGG: 1.441 D_real: 0.019 D_fake: 0.073 \n",
            "saving the latest model (epoch 99, total_steps 412000)\n",
            "(epoch: 99, iters: 500, time: 0.547) G_GAN: 1.938 G_GAN_Feat: 3.020 G_VGG: 2.087 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 99, iters: 600, time: 0.548) G_GAN: 1.817 G_GAN_Feat: 2.844 G_VGG: 2.002 D_real: 0.083 D_fake: 0.053 \n",
            "(epoch: 99, iters: 700, time: 0.549) G_GAN: 1.826 G_GAN_Feat: 2.927 G_VGG: 1.803 D_real: 0.014 D_fake: 0.155 \n",
            "(epoch: 99, iters: 800, time: 0.549) G_GAN: 2.048 G_GAN_Feat: 2.719 G_VGG: 1.958 D_real: 0.018 D_fake: 0.010 \n",
            "(epoch: 99, iters: 900, time: 0.549) G_GAN: 1.648 G_GAN_Feat: 2.854 G_VGG: 2.243 D_real: 0.019 D_fake: 0.070 \n",
            "(epoch: 99, iters: 1000, time: 0.549) G_GAN: 1.992 G_GAN_Feat: 2.778 G_VGG: 1.683 D_real: 0.021 D_fake: 0.005 \n",
            "(epoch: 99, iters: 1100, time: 0.549) G_GAN: 2.081 G_GAN_Feat: 2.619 G_VGG: 1.748 D_real: 0.025 D_fake: 0.008 \n",
            "(epoch: 99, iters: 1200, time: 0.549) G_GAN: 1.109 G_GAN_Feat: 2.362 G_VGG: 1.549 D_real: 0.114 D_fake: 0.336 \n",
            "(epoch: 99, iters: 1300, time: 0.549) G_GAN: 2.043 G_GAN_Feat: 3.186 G_VGG: 1.761 D_real: 0.006 D_fake: 0.003 \n",
            "(epoch: 99, iters: 1400, time: 0.549) G_GAN: 2.011 G_GAN_Feat: 2.506 G_VGG: 1.637 D_real: 0.012 D_fake: 0.007 \n",
            "saving the latest model (epoch 99, total_steps 413000)\n",
            "(epoch: 99, iters: 1500, time: 0.547) G_GAN: 1.754 G_GAN_Feat: 2.620 G_VGG: 1.468 D_real: 0.124 D_fake: 0.153 \n",
            "(epoch: 99, iters: 1600, time: 0.550) G_GAN: 2.073 G_GAN_Feat: 2.351 G_VGG: 1.345 D_real: 0.078 D_fake: 0.022 \n",
            "(epoch: 99, iters: 1700, time: 0.554) G_GAN: 1.755 G_GAN_Feat: 2.612 G_VGG: 1.582 D_real: 0.100 D_fake: 0.046 \n",
            "(epoch: 99, iters: 1800, time: 0.554) G_GAN: 1.714 G_GAN_Feat: 2.459 G_VGG: 1.403 D_real: 0.014 D_fake: 0.062 \n",
            "(epoch: 99, iters: 1900, time: 0.553) G_GAN: 1.947 G_GAN_Feat: 3.306 G_VGG: 1.894 D_real: 0.007 D_fake: 0.006 \n",
            "(epoch: 99, iters: 2000, time: 0.554) G_GAN: 1.665 G_GAN_Feat: 2.794 G_VGG: 1.520 D_real: 0.069 D_fake: 0.048 \n",
            "(epoch: 99, iters: 2100, time: 0.554) G_GAN: 1.914 G_GAN_Feat: 3.322 G_VGG: 1.894 D_real: 0.013 D_fake: 0.019 \n",
            "(epoch: 99, iters: 2200, time: 0.554) G_GAN: 1.889 G_GAN_Feat: 2.654 G_VGG: 1.821 D_real: 0.008 D_fake: 0.018 \n",
            "(epoch: 99, iters: 2300, time: 0.553) G_GAN: 1.745 G_GAN_Feat: 2.808 G_VGG: 2.120 D_real: 0.024 D_fake: 0.043 \n",
            "(epoch: 99, iters: 2400, time: 0.553) G_GAN: 1.740 G_GAN_Feat: 3.011 G_VGG: 2.121 D_real: 0.011 D_fake: 0.025 \n",
            "saving the latest model (epoch 99, total_steps 414000)\n",
            "(epoch: 99, iters: 2500, time: 0.552) G_GAN: 1.617 G_GAN_Feat: 2.687 G_VGG: 1.218 D_real: 0.008 D_fake: 0.114 \n",
            "(epoch: 99, iters: 2600, time: 0.551) G_GAN: 1.897 G_GAN_Feat: 2.901 G_VGG: 1.661 D_real: 0.010 D_fake: 0.013 \n",
            "(epoch: 99, iters: 2700, time: 0.550) G_GAN: 2.195 G_GAN_Feat: 2.783 G_VGG: 2.024 D_real: 0.052 D_fake: 0.023 \n",
            "(epoch: 99, iters: 2800, time: 0.550) G_GAN: 1.758 G_GAN_Feat: 2.996 G_VGG: 2.116 D_real: 0.019 D_fake: 0.025 \n",
            "(epoch: 99, iters: 2900, time: 0.549) G_GAN: 1.612 G_GAN_Feat: 2.586 G_VGG: 1.547 D_real: 0.058 D_fake: 0.109 \n",
            "(epoch: 99, iters: 3000, time: 0.549) G_GAN: 2.024 G_GAN_Feat: 2.702 G_VGG: 1.456 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 99, iters: 3100, time: 0.548) G_GAN: 1.887 G_GAN_Feat: 2.752 G_VGG: 1.515 D_real: 0.012 D_fake: 0.017 \n",
            "(epoch: 99, iters: 3200, time: 0.549) G_GAN: 1.722 G_GAN_Feat: 2.869 G_VGG: 1.913 D_real: 0.022 D_fake: 0.096 \n",
            "(epoch: 99, iters: 3300, time: 0.549) G_GAN: 1.989 G_GAN_Feat: 2.893 G_VGG: 1.723 D_real: 0.085 D_fake: 0.007 \n",
            "(epoch: 99, iters: 3400, time: 0.549) G_GAN: 2.044 G_GAN_Feat: 2.686 G_VGG: 1.735 D_real: 0.004 D_fake: 0.008 \n",
            "saving the latest model (epoch 99, total_steps 415000)\n",
            "(epoch: 99, iters: 3500, time: 0.547) G_GAN: 1.912 G_GAN_Feat: 2.797 G_VGG: 1.559 D_real: 0.009 D_fake: 0.025 \n",
            "(epoch: 99, iters: 3600, time: 0.548) G_GAN: 2.063 G_GAN_Feat: 2.656 G_VGG: 1.708 D_real: 0.147 D_fake: 0.012 \n",
            "(epoch: 99, iters: 3700, time: 0.548) G_GAN: 1.399 G_GAN_Feat: 2.756 G_VGG: 1.594 D_real: 0.140 D_fake: 0.114 \n",
            "(epoch: 99, iters: 3800, time: 0.548) G_GAN: 2.039 G_GAN_Feat: 2.849 G_VGG: 1.777 D_real: 0.036 D_fake: 0.007 \n",
            "(epoch: 99, iters: 3900, time: 0.549) G_GAN: 1.833 G_GAN_Feat: 2.657 G_VGG: 1.735 D_real: 0.085 D_fake: 0.053 \n",
            "(epoch: 99, iters: 4000, time: 0.549) G_GAN: 2.017 G_GAN_Feat: 2.816 G_VGG: 2.053 D_real: 0.243 D_fake: 0.014 \n",
            "(epoch: 99, iters: 4100, time: 0.549) G_GAN: 1.900 G_GAN_Feat: 2.901 G_VGG: 1.966 D_real: 0.016 D_fake: 0.009 \n",
            "(epoch: 99, iters: 4200, time: 0.549) G_GAN: 2.084 G_GAN_Feat: 2.667 G_VGG: 1.881 D_real: 0.021 D_fake: 0.015 \n",
            "End of epoch 99 / 200 \t Time Taken: 2332 sec\n",
            "(epoch: 100, iters: 100, time: 0.549) G_GAN: 1.942 G_GAN_Feat: 2.866 G_VGG: 1.855 D_real: 0.046 D_fake: 0.017 \n",
            "(epoch: 100, iters: 200, time: 0.549) G_GAN: 1.877 G_GAN_Feat: 2.907 G_VGG: 2.342 D_real: 0.014 D_fake: 0.023 \n",
            "saving the latest model (epoch 100, total_steps 416000)\n",
            "(epoch: 100, iters: 300, time: 0.547) G_GAN: 1.781 G_GAN_Feat: 2.847 G_VGG: 1.744 D_real: 0.031 D_fake: 0.035 \n",
            "(epoch: 100, iters: 400, time: 0.548) G_GAN: 1.777 G_GAN_Feat: 2.565 G_VGG: 1.514 D_real: 0.040 D_fake: 0.138 \n",
            "(epoch: 100, iters: 500, time: 0.548) G_GAN: 2.203 G_GAN_Feat: 2.456 G_VGG: 1.511 D_real: 0.100 D_fake: 0.090 \n",
            "(epoch: 100, iters: 600, time: 0.549) G_GAN: 1.670 G_GAN_Feat: 3.231 G_VGG: 2.109 D_real: 0.029 D_fake: 0.144 \n",
            "(epoch: 100, iters: 700, time: 0.549) G_GAN: 1.765 G_GAN_Feat: 2.679 G_VGG: 1.638 D_real: 0.032 D_fake: 0.053 \n",
            "(epoch: 100, iters: 800, time: 0.549) G_GAN: 2.044 G_GAN_Feat: 2.652 G_VGG: 1.256 D_real: 0.076 D_fake: 0.015 \n",
            "(epoch: 100, iters: 900, time: 0.549) G_GAN: 1.687 G_GAN_Feat: 2.701 G_VGG: 1.847 D_real: 0.343 D_fake: 0.050 \n",
            "(epoch: 100, iters: 1000, time: 0.549) G_GAN: 1.852 G_GAN_Feat: 2.450 G_VGG: 1.163 D_real: 0.043 D_fake: 0.068 \n",
            "(epoch: 100, iters: 1100, time: 0.550) G_GAN: 1.707 G_GAN_Feat: 2.770 G_VGG: 1.736 D_real: 0.026 D_fake: 0.021 \n",
            "(epoch: 100, iters: 1200, time: 0.549) G_GAN: 1.617 G_GAN_Feat: 2.579 G_VGG: 1.579 D_real: 0.147 D_fake: 0.056 \n",
            "saving the latest model (epoch 100, total_steps 417000)\n",
            "(epoch: 100, iters: 1300, time: 0.547) G_GAN: 2.101 G_GAN_Feat: 3.213 G_VGG: 1.824 D_real: 0.013 D_fake: 0.009 \n",
            "(epoch: 100, iters: 1400, time: 0.548) G_GAN: 2.122 G_GAN_Feat: 2.775 G_VGG: 2.171 D_real: 0.007 D_fake: 0.009 \n",
            "(epoch: 100, iters: 1500, time: 0.548) G_GAN: 1.092 G_GAN_Feat: 2.871 G_VGG: 1.996 D_real: 0.140 D_fake: 0.316 \n",
            "(epoch: 100, iters: 1600, time: 0.549) G_GAN: 1.635 G_GAN_Feat: 2.556 G_VGG: 1.899 D_real: 0.021 D_fake: 0.060 \n",
            "(epoch: 100, iters: 1700, time: 0.548) G_GAN: 1.618 G_GAN_Feat: 3.139 G_VGG: 2.025 D_real: 0.028 D_fake: 0.122 \n",
            "(epoch: 100, iters: 1800, time: 0.550) G_GAN: 2.097 G_GAN_Feat: 2.481 G_VGG: 1.524 D_real: 0.019 D_fake: 0.015 \n",
            "(epoch: 100, iters: 1900, time: 0.549) G_GAN: 1.889 G_GAN_Feat: 2.602 G_VGG: 1.597 D_real: 0.018 D_fake: 0.046 \n",
            "(epoch: 100, iters: 2000, time: 0.549) G_GAN: 1.420 G_GAN_Feat: 2.255 G_VGG: 1.411 D_real: 0.032 D_fake: 0.137 \n",
            "(epoch: 100, iters: 2100, time: 0.549) G_GAN: 1.851 G_GAN_Feat: 3.077 G_VGG: 1.699 D_real: 0.015 D_fake: 0.017 \n",
            "(epoch: 100, iters: 2200, time: 0.549) G_GAN: 2.041 G_GAN_Feat: 2.649 G_VGG: 1.825 D_real: 0.015 D_fake: 0.015 \n",
            "saving the latest model (epoch 100, total_steps 418000)\n",
            "(epoch: 100, iters: 2300, time: 0.548) G_GAN: 2.057 G_GAN_Feat: 2.966 G_VGG: 1.944 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 100, iters: 2400, time: 0.549) G_GAN: 1.629 G_GAN_Feat: 3.283 G_VGG: 2.600 D_real: 0.044 D_fake: 0.081 \n",
            "(epoch: 100, iters: 2500, time: 0.549) G_GAN: 1.462 G_GAN_Feat: 2.737 G_VGG: 1.641 D_real: 0.038 D_fake: 0.109 \n",
            "(epoch: 100, iters: 2600, time: 0.549) G_GAN: 1.799 G_GAN_Feat: 2.397 G_VGG: 1.305 D_real: 0.027 D_fake: 0.025 \n",
            "(epoch: 100, iters: 2700, time: 0.550) G_GAN: 2.069 G_GAN_Feat: 2.537 G_VGG: 1.461 D_real: 0.110 D_fake: 0.008 \n",
            "(epoch: 100, iters: 2800, time: 0.550) G_GAN: 2.045 G_GAN_Feat: 2.336 G_VGG: 1.625 D_real: 0.268 D_fake: 0.013 \n",
            "(epoch: 100, iters: 2900, time: 0.550) G_GAN: 1.720 G_GAN_Feat: 2.933 G_VGG: 2.184 D_real: 0.020 D_fake: 0.051 \n",
            "(epoch: 100, iters: 3000, time: 0.550) G_GAN: 1.845 G_GAN_Feat: 2.979 G_VGG: 2.273 D_real: 0.012 D_fake: 0.017 \n",
            "(epoch: 100, iters: 3100, time: 0.550) G_GAN: 1.864 G_GAN_Feat: 2.574 G_VGG: 1.765 D_real: 0.009 D_fake: 0.012 \n",
            "(epoch: 100, iters: 3200, time: 0.550) G_GAN: 1.623 G_GAN_Feat: 3.072 G_VGG: 1.968 D_real: 0.074 D_fake: 0.080 \n",
            "saving the latest model (epoch 100, total_steps 419000)\n",
            "(epoch: 100, iters: 3300, time: 0.548) G_GAN: 1.736 G_GAN_Feat: 2.945 G_VGG: 1.864 D_real: 0.027 D_fake: 0.023 \n",
            "(epoch: 100, iters: 3400, time: 0.549) G_GAN: 1.836 G_GAN_Feat: 2.391 G_VGG: 1.475 D_real: 0.039 D_fake: 0.033 \n",
            "(epoch: 100, iters: 3500, time: 0.549) G_GAN: 1.739 G_GAN_Feat: 2.657 G_VGG: 1.495 D_real: 0.197 D_fake: 0.041 \n",
            "(epoch: 100, iters: 3600, time: 0.549) G_GAN: 1.593 G_GAN_Feat: 2.596 G_VGG: 1.564 D_real: 0.039 D_fake: 0.154 \n",
            "(epoch: 100, iters: 3700, time: 0.550) G_GAN: 2.056 G_GAN_Feat: 2.805 G_VGG: 1.510 D_real: 0.038 D_fake: 0.006 \n",
            "(epoch: 100, iters: 3800, time: 0.550) G_GAN: 1.737 G_GAN_Feat: 2.571 G_VGG: 1.838 D_real: 0.223 D_fake: 0.068 \n",
            "(epoch: 100, iters: 3900, time: 0.550) G_GAN: 1.847 G_GAN_Feat: 2.914 G_VGG: 2.210 D_real: 0.010 D_fake: 0.013 \n",
            "(epoch: 100, iters: 4000, time: 0.550) G_GAN: 1.812 G_GAN_Feat: 3.255 G_VGG: 1.485 D_real: 0.011 D_fake: 0.012 \n",
            "(epoch: 100, iters: 4100, time: 0.550) G_GAN: 2.081 G_GAN_Feat: 2.711 G_VGG: 1.778 D_real: 0.254 D_fake: 0.027 \n",
            "(epoch: 100, iters: 4200, time: 0.550) G_GAN: 1.938 G_GAN_Feat: 2.901 G_VGG: 1.430 D_real: 0.065 D_fake: 0.006 \n",
            "saving the latest model (epoch 100, total_steps 420000)\n",
            "End of epoch 100 / 200 \t Time Taken: 2333 sec\n",
            "saving the model at the end of epoch 100, iters 420000\n",
            "(epoch: 101, iters: 100, time: 0.541) G_GAN: 1.836 G_GAN_Feat: 2.893 G_VGG: 2.319 D_real: 0.153 D_fake: 0.013 \n",
            "(epoch: 101, iters: 200, time: 0.545) G_GAN: 1.966 G_GAN_Feat: 2.941 G_VGG: 1.396 D_real: 0.284 D_fake: 0.012 \n",
            "(epoch: 101, iters: 300, time: 0.547) G_GAN: 1.727 G_GAN_Feat: 2.774 G_VGG: 1.855 D_real: 0.031 D_fake: 0.081 \n",
            "(epoch: 101, iters: 400, time: 0.548) G_GAN: 1.618 G_GAN_Feat: 2.941 G_VGG: 1.917 D_real: 0.013 D_fake: 0.137 \n",
            "(epoch: 101, iters: 500, time: 0.550) G_GAN: 1.926 G_GAN_Feat: 2.785 G_VGG: 1.905 D_real: 0.230 D_fake: 0.027 \n",
            "(epoch: 101, iters: 600, time: 0.550) G_GAN: 1.983 G_GAN_Feat: 2.726 G_VGG: 1.910 D_real: 0.042 D_fake: 0.007 \n",
            "(epoch: 101, iters: 700, time: 0.550) G_GAN: 1.453 G_GAN_Feat: 2.512 G_VGG: 1.446 D_real: 0.235 D_fake: 0.169 \n",
            "(epoch: 101, iters: 800, time: 0.550) G_GAN: 1.771 G_GAN_Feat: 2.562 G_VGG: 1.153 D_real: 0.009 D_fake: 0.024 \n",
            "(epoch: 101, iters: 900, time: 0.550) G_GAN: 2.028 G_GAN_Feat: 2.823 G_VGG: 1.667 D_real: 0.043 D_fake: 0.031 \n",
            "(epoch: 101, iters: 1000, time: 0.550) G_GAN: 2.037 G_GAN_Feat: 2.565 G_VGG: 1.734 D_real: 0.036 D_fake: 0.004 \n",
            "saving the latest model (epoch 101, total_steps 421000)\n",
            "(epoch: 101, iters: 1100, time: 0.548) G_GAN: 1.987 G_GAN_Feat: 2.842 G_VGG: 1.824 D_real: 0.009 D_fake: 0.013 \n",
            "(epoch: 101, iters: 1200, time: 0.549) G_GAN: 1.533 G_GAN_Feat: 2.895 G_VGG: 2.024 D_real: 0.055 D_fake: 0.060 \n",
            "(epoch: 101, iters: 1300, time: 0.550) G_GAN: 1.947 G_GAN_Feat: 2.882 G_VGG: 1.722 D_real: 0.071 D_fake: 0.055 \n",
            "(epoch: 101, iters: 1400, time: 0.550) G_GAN: 1.957 G_GAN_Feat: 2.480 G_VGG: 1.662 D_real: 0.014 D_fake: 0.009 \n",
            "(epoch: 101, iters: 1500, time: 0.550) G_GAN: 2.067 G_GAN_Feat: 2.951 G_VGG: 1.828 D_real: 0.012 D_fake: 0.005 \n",
            "(epoch: 101, iters: 1600, time: 0.550) G_GAN: 1.997 G_GAN_Feat: 2.685 G_VGG: 1.591 D_real: 0.038 D_fake: 0.028 \n",
            "(epoch: 101, iters: 1700, time: 0.550) G_GAN: 1.881 G_GAN_Feat: 2.525 G_VGG: 1.584 D_real: 0.075 D_fake: 0.051 \n",
            "(epoch: 101, iters: 1800, time: 0.550) G_GAN: 1.963 G_GAN_Feat: 2.731 G_VGG: 1.428 D_real: 0.067 D_fake: 0.023 \n",
            "(epoch: 101, iters: 1900, time: 0.550) G_GAN: 1.634 G_GAN_Feat: 2.614 G_VGG: 1.824 D_real: 0.006 D_fake: 0.078 \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/train.py\", line 95, in <module>\n",
            "    loss_D.backward()        \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on high resolution G1 and G2\n",
        "import torch\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "torch.cuda.empty_cache()\n",
        "!python train.py --netG local --continue_train --load_pretrain checkpoints/nebbia/ --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 1024 --batchSize 2"
      ],
      "metadata": {
        "id": "3XhQJ5s95ZUj",
        "outputId": "e66a7b05-5166-430b-a269-d099aa36ab74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 2\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 1024\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: checkpoints/nebbia/\n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: local\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 101 at iteration 2000\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "LocalEnhancer(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 128, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "  )\n",
            "  (model1_1): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "  )\n",
            "  (model1_2): Sequential(\n",
            "    (0): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (1): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (2): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "create web directory ./checkpoints/nebbia/web...\n",
            "(epoch: 101, iters: 2100, time: 0.383) G_GAN: 1.592 G_GAN_Feat: 5.207 G_VGG: 2.289 D_real: 0.071 D_fake: 0.065 \n",
            "(epoch: 101, iters: 2200, time: 0.290) G_GAN: 1.348 G_GAN_Feat: 3.637 G_VGG: 1.688 D_real: 0.248 D_fake: 0.256 \n",
            "(epoch: 101, iters: 2300, time: 0.304) G_GAN: 1.714 G_GAN_Feat: 4.760 G_VGG: 1.955 D_real: 0.022 D_fake: 0.057 \n",
            "(epoch: 101, iters: 2400, time: 0.294) G_GAN: 1.613 G_GAN_Feat: 4.804 G_VGG: 2.445 D_real: 0.036 D_fake: 0.061 \n",
            "(epoch: 101, iters: 2500, time: 0.292) G_GAN: 1.564 G_GAN_Feat: 3.755 G_VGG: 2.038 D_real: 0.039 D_fake: 0.076 \n",
            "(epoch: 101, iters: 2600, time: 0.312) G_GAN: 1.336 G_GAN_Feat: 5.686 G_VGG: 2.232 D_real: 0.021 D_fake: 0.215 \n",
            "(epoch: 101, iters: 2700, time: 0.304) G_GAN: 2.030 G_GAN_Feat: 5.321 G_VGG: 1.513 D_real: 0.029 D_fake: 0.010 \n",
            "(epoch: 101, iters: 2800, time: 0.308) G_GAN: 2.152 G_GAN_Feat: 3.912 G_VGG: 1.695 D_real: 0.118 D_fake: 0.034 \n",
            "(epoch: 101, iters: 2900, time: 0.269) G_GAN: 2.145 G_GAN_Feat: 4.968 G_VGG: 2.125 D_real: 0.174 D_fake: 0.025 \n",
            "(epoch: 101, iters: 3000, time: 0.311) G_GAN: 1.503 G_GAN_Feat: 4.705 G_VGG: 1.572 D_real: 0.048 D_fake: 0.063 \n",
            "saving the latest model (epoch 101, total_steps 423000)\n",
            "(epoch: 101, iters: 3100, time: 0.299) G_GAN: 2.045 G_GAN_Feat: 5.750 G_VGG: 1.668 D_real: 0.019 D_fake: 0.011 \n",
            "(epoch: 101, iters: 3200, time: 0.293) G_GAN: 0.541 G_GAN_Feat: 3.046 G_VGG: 1.551 D_real: 0.045 D_fake: 1.347 \n",
            "(epoch: 101, iters: 3300, time: 0.315) G_GAN: 1.358 G_GAN_Feat: 4.192 G_VGG: 1.528 D_real: 0.041 D_fake: 0.202 \n",
            "(epoch: 101, iters: 3400, time: 0.299) G_GAN: 1.480 G_GAN_Feat: 5.785 G_VGG: 1.794 D_real: 0.013 D_fake: 0.129 \n",
            "(epoch: 101, iters: 3500, time: 0.311) G_GAN: 2.077 G_GAN_Feat: 4.840 G_VGG: 1.624 D_real: 0.625 D_fake: 0.026 \n",
            "(epoch: 101, iters: 3600, time: 0.285) G_GAN: 1.586 G_GAN_Feat: 4.124 G_VGG: 1.671 D_real: 0.018 D_fake: 0.192 \n",
            "(epoch: 101, iters: 3700, time: 0.308) G_GAN: 1.115 G_GAN_Feat: 4.132 G_VGG: 1.491 D_real: 0.024 D_fake: 0.549 \n",
            "(epoch: 101, iters: 3800, time: 0.334) G_GAN: 1.803 G_GAN_Feat: 4.478 G_VGG: 1.400 D_real: 0.154 D_fake: 0.031 \n",
            "(epoch: 101, iters: 3900, time: 0.269) G_GAN: 1.747 G_GAN_Feat: 4.181 G_VGG: 1.496 D_real: 0.024 D_fake: 0.041 \n",
            "(epoch: 101, iters: 4000, time: 0.293) G_GAN: 1.533 G_GAN_Feat: 4.542 G_VGG: 1.911 D_real: 0.060 D_fake: 0.120 \n",
            "saving the latest model (epoch 101, total_steps 424000)\n",
            "(epoch: 101, iters: 4100, time: 0.284) G_GAN: 1.530 G_GAN_Feat: 3.843 G_VGG: 1.794 D_real: 0.097 D_fake: 0.150 \n",
            "(epoch: 101, iters: 4200, time: 0.296) G_GAN: 1.383 G_GAN_Feat: 3.406 G_VGG: 1.339 D_real: 0.080 D_fake: 0.160 \n",
            "End of epoch 101 / 200 \t Time Taken: 705 sec\n",
            "(epoch: 102, iters: 100, time: 0.193) G_GAN: 1.809 G_GAN_Feat: 5.276 G_VGG: 1.367 D_real: 0.022 D_fake: 0.026 \n",
            "(epoch: 102, iters: 200, time: 0.193) G_GAN: 1.881 G_GAN_Feat: 5.282 G_VGG: 1.957 D_real: 0.066 D_fake: 0.037 \n",
            "(epoch: 102, iters: 300, time: 0.240) G_GAN: 2.049 G_GAN_Feat: 5.034 G_VGG: 2.019 D_real: 0.153 D_fake: 0.033 \n",
            "(epoch: 102, iters: 400, time: 0.225) G_GAN: 1.551 G_GAN_Feat: 4.351 G_VGG: 1.384 D_real: 0.030 D_fake: 0.094 \n",
            "(epoch: 102, iters: 500, time: 0.194) G_GAN: 1.973 G_GAN_Feat: 3.040 G_VGG: 1.674 D_real: 0.275 D_fake: 0.032 \n",
            "(epoch: 102, iters: 600, time: 0.220) G_GAN: 1.418 G_GAN_Feat: 3.723 G_VGG: 1.072 D_real: 0.071 D_fake: 0.139 \n",
            "(epoch: 102, iters: 700, time: 0.248) G_GAN: 1.485 G_GAN_Feat: 3.348 G_VGG: 1.570 D_real: 0.163 D_fake: 0.201 \n",
            "(epoch: 102, iters: 800, time: 0.204) G_GAN: 1.731 G_GAN_Feat: 5.882 G_VGG: 1.240 D_real: 0.042 D_fake: 0.045 \n",
            "saving the latest model (epoch 102, total_steps 425000)\n",
            "(epoch: 102, iters: 900, time: 0.209) G_GAN: 1.952 G_GAN_Feat: 5.027 G_VGG: 1.580 D_real: 0.033 D_fake: 0.040 \n",
            "(epoch: 102, iters: 1000, time: 0.216) G_GAN: 1.686 G_GAN_Feat: 3.432 G_VGG: 1.308 D_real: 0.188 D_fake: 0.141 \n",
            "(epoch: 102, iters: 1100, time: 0.217) G_GAN: 1.913 G_GAN_Feat: 3.372 G_VGG: 1.887 D_real: 0.127 D_fake: 0.018 \n",
            "(epoch: 102, iters: 1200, time: 0.200) G_GAN: 1.832 G_GAN_Feat: 4.231 G_VGG: 1.639 D_real: 0.148 D_fake: 0.024 \n",
            "(epoch: 102, iters: 1300, time: 0.218) G_GAN: 1.617 G_GAN_Feat: 6.255 G_VGG: 2.043 D_real: 0.030 D_fake: 0.158 \n",
            "(epoch: 102, iters: 1400, time: 0.201) G_GAN: 2.163 G_GAN_Feat: 3.985 G_VGG: 0.980 D_real: 0.106 D_fake: 0.032 \n",
            "(epoch: 102, iters: 1500, time: 0.214) G_GAN: 1.905 G_GAN_Feat: 4.820 G_VGG: 1.758 D_real: 0.090 D_fake: 0.082 \n",
            "(epoch: 102, iters: 1600, time: 0.197) G_GAN: 2.397 G_GAN_Feat: 4.035 G_VGG: 1.659 D_real: 0.073 D_fake: 0.054 \n",
            "(epoch: 102, iters: 1700, time: 0.208) G_GAN: 2.221 G_GAN_Feat: 4.375 G_VGG: 1.831 D_real: 0.173 D_fake: 0.023 \n",
            "(epoch: 102, iters: 1800, time: 0.220) G_GAN: 1.602 G_GAN_Feat: 3.855 G_VGG: 1.703 D_real: 0.240 D_fake: 0.085 \n",
            "saving the latest model (epoch 102, total_steps 426000)\n",
            "(epoch: 102, iters: 1900, time: 0.205) G_GAN: 1.885 G_GAN_Feat: 3.782 G_VGG: 1.588 D_real: 0.132 D_fake: 0.049 \n",
            "(epoch: 102, iters: 2000, time: 0.203) G_GAN: 1.280 G_GAN_Feat: 2.738 G_VGG: 1.377 D_real: 0.099 D_fake: 0.202 \n",
            "(epoch: 102, iters: 2100, time: 0.207) G_GAN: 2.328 G_GAN_Feat: 4.256 G_VGG: 1.466 D_real: 0.079 D_fake: 0.053 \n",
            "(epoch: 102, iters: 2200, time: 0.212) G_GAN: 2.064 G_GAN_Feat: 3.140 G_VGG: 1.668 D_real: 0.092 D_fake: 0.097 \n",
            "(epoch: 102, iters: 2300, time: 0.208) G_GAN: 1.909 G_GAN_Feat: 4.211 G_VGG: 1.639 D_real: 0.036 D_fake: 0.018 \n",
            "(epoch: 102, iters: 2400, time: 0.218) G_GAN: 1.719 G_GAN_Feat: 4.589 G_VGG: 1.199 D_real: 0.028 D_fake: 0.041 \n",
            "(epoch: 102, iters: 2500, time: 0.216) G_GAN: 1.245 G_GAN_Feat: 3.168 G_VGG: 1.250 D_real: 0.062 D_fake: 0.309 \n",
            "(epoch: 102, iters: 2600, time: 0.224) G_GAN: 2.016 G_GAN_Feat: 5.258 G_VGG: 1.573 D_real: 0.079 D_fake: 0.021 \n",
            "(epoch: 102, iters: 2700, time: 0.215) G_GAN: 1.723 G_GAN_Feat: 3.594 G_VGG: 1.930 D_real: 0.109 D_fake: 0.035 \n",
            "(epoch: 102, iters: 2800, time: 0.206) G_GAN: 1.905 G_GAN_Feat: 4.851 G_VGG: 1.430 D_real: 0.045 D_fake: 0.019 \n",
            "saving the latest model (epoch 102, total_steps 427000)\n",
            "(epoch: 102, iters: 2900, time: 0.210) G_GAN: 1.590 G_GAN_Feat: 4.270 G_VGG: 1.720 D_real: 0.099 D_fake: 0.097 \n",
            "(epoch: 102, iters: 3000, time: 0.229) G_GAN: 1.855 G_GAN_Feat: 3.692 G_VGG: 1.359 D_real: 0.127 D_fake: 0.072 \n",
            "(epoch: 102, iters: 3100, time: 0.199) G_GAN: 1.405 G_GAN_Feat: 2.432 G_VGG: 1.289 D_real: 0.078 D_fake: 0.202 \n",
            "(epoch: 102, iters: 3200, time: 0.203) G_GAN: 1.751 G_GAN_Feat: 3.857 G_VGG: 1.655 D_real: 0.357 D_fake: 0.052 \n",
            "(epoch: 102, iters: 3300, time: 0.214) G_GAN: 1.776 G_GAN_Feat: 3.613 G_VGG: 1.359 D_real: 0.078 D_fake: 0.056 \n",
            "(epoch: 102, iters: 3400, time: 0.207) G_GAN: 1.515 G_GAN_Feat: 3.666 G_VGG: 1.364 D_real: 0.018 D_fake: 0.106 \n",
            "(epoch: 102, iters: 3500, time: 0.209) G_GAN: 1.637 G_GAN_Feat: 3.128 G_VGG: 1.298 D_real: 0.089 D_fake: 0.128 \n",
            "(epoch: 102, iters: 3600, time: 0.205) G_GAN: 1.823 G_GAN_Feat: 4.308 G_VGG: 1.379 D_real: 0.031 D_fake: 0.042 \n",
            "(epoch: 102, iters: 3700, time: 0.204) G_GAN: 1.257 G_GAN_Feat: 4.143 G_VGG: 1.703 D_real: 0.032 D_fake: 0.359 \n",
            "(epoch: 102, iters: 3800, time: 0.204) G_GAN: 1.123 G_GAN_Feat: 2.697 G_VGG: 1.408 D_real: 0.040 D_fake: 0.466 \n",
            "saving the latest model (epoch 102, total_steps 428000)\n",
            "(epoch: 102, iters: 3900, time: 0.202) G_GAN: 1.609 G_GAN_Feat: 3.775 G_VGG: 1.498 D_real: 0.099 D_fake: 0.068 \n",
            "(epoch: 102, iters: 4000, time: 0.217) G_GAN: 1.587 G_GAN_Feat: 4.152 G_VGG: 1.512 D_real: 0.045 D_fake: 0.204 \n",
            "(epoch: 102, iters: 4100, time: 0.196) G_GAN: 1.695 G_GAN_Feat: 4.618 G_VGG: 1.564 D_real: 0.070 D_fake: 0.122 \n",
            "(epoch: 102, iters: 4200, time: 0.205) G_GAN: 1.692 G_GAN_Feat: 4.393 G_VGG: 1.560 D_real: 0.037 D_fake: 0.089 \n",
            "End of epoch 102 / 200 \t Time Taken: 948 sec\n",
            "(epoch: 103, iters: 100, time: 0.193) G_GAN: 1.639 G_GAN_Feat: 2.899 G_VGG: 1.368 D_real: 0.101 D_fake: 0.079 \n",
            "(epoch: 103, iters: 200, time: 0.193) G_GAN: 1.572 G_GAN_Feat: 4.093 G_VGG: 1.450 D_real: 0.113 D_fake: 0.099 \n",
            "(epoch: 103, iters: 300, time: 0.193) G_GAN: 1.177 G_GAN_Feat: 3.475 G_VGG: 1.643 D_real: 0.159 D_fake: 0.247 \n",
            "(epoch: 103, iters: 400, time: 0.193) G_GAN: 1.978 G_GAN_Feat: 3.779 G_VGG: 1.755 D_real: 0.130 D_fake: 0.014 \n",
            "(epoch: 103, iters: 500, time: 0.193) G_GAN: 1.702 G_GAN_Feat: 3.636 G_VGG: 1.725 D_real: 0.706 D_fake: 0.125 \n",
            "(epoch: 103, iters: 600, time: 0.193) G_GAN: 1.192 G_GAN_Feat: 3.279 G_VGG: 1.642 D_real: 0.147 D_fake: 0.260 \n",
            "saving the latest model (epoch 103, total_steps 429000)\n",
            "(epoch: 103, iters: 700, time: 0.193) G_GAN: 1.490 G_GAN_Feat: 4.806 G_VGG: 1.482 D_real: 0.192 D_fake: 0.216 \n",
            "(epoch: 103, iters: 800, time: 0.193) G_GAN: 0.935 G_GAN_Feat: 2.812 G_VGG: 1.286 D_real: 0.193 D_fake: 0.353 \n",
            "(epoch: 103, iters: 900, time: 0.193) G_GAN: 1.075 G_GAN_Feat: 4.440 G_VGG: 1.064 D_real: 0.025 D_fake: 0.864 \n",
            "(epoch: 103, iters: 1000, time: 0.193) G_GAN: 0.997 G_GAN_Feat: 3.581 G_VGG: 1.111 D_real: 0.018 D_fake: 0.596 \n",
            "(epoch: 103, iters: 1100, time: 0.193) G_GAN: 1.793 G_GAN_Feat: 3.373 G_VGG: 1.621 D_real: 0.101 D_fake: 0.029 \n",
            "(epoch: 103, iters: 1200, time: 0.193) G_GAN: 1.906 G_GAN_Feat: 5.966 G_VGG: 1.758 D_real: 0.026 D_fake: 0.010 \n",
            "(epoch: 103, iters: 1300, time: 0.193) G_GAN: 1.307 G_GAN_Feat: 3.808 G_VGG: 1.271 D_real: 0.100 D_fake: 0.282 \n",
            "(epoch: 103, iters: 1400, time: 0.193) G_GAN: 1.053 G_GAN_Feat: 2.815 G_VGG: 1.240 D_real: 0.050 D_fake: 0.463 \n",
            "(epoch: 103, iters: 1500, time: 0.193) G_GAN: 2.492 G_GAN_Feat: 5.499 G_VGG: 1.970 D_real: 0.082 D_fake: 0.056 \n",
            "(epoch: 103, iters: 1600, time: 0.193) G_GAN: 1.839 G_GAN_Feat: 5.348 G_VGG: 1.963 D_real: 0.218 D_fake: 0.053 \n",
            "saving the latest model (epoch 103, total_steps 430000)\n",
            "(epoch: 103, iters: 1700, time: 0.193) G_GAN: 1.607 G_GAN_Feat: 3.982 G_VGG: 0.968 D_real: 0.036 D_fake: 0.182 \n",
            "(epoch: 103, iters: 1800, time: 0.193) G_GAN: 1.570 G_GAN_Feat: 3.996 G_VGG: 1.583 D_real: 0.036 D_fake: 0.258 \n",
            "(epoch: 103, iters: 1900, time: 0.193) G_GAN: 1.280 G_GAN_Feat: 3.106 G_VGG: 1.073 D_real: 0.070 D_fake: 0.196 \n",
            "(epoch: 103, iters: 2000, time: 0.193) G_GAN: 0.892 G_GAN_Feat: 2.748 G_VGG: 1.073 D_real: 0.033 D_fake: 0.552 \n",
            "(epoch: 103, iters: 2100, time: 0.193) G_GAN: 1.183 G_GAN_Feat: 3.004 G_VGG: 1.435 D_real: 0.029 D_fake: 0.391 \n",
            "(epoch: 103, iters: 2200, time: 0.193) G_GAN: 2.265 G_GAN_Feat: 4.246 G_VGG: 1.565 D_real: 0.358 D_fake: 0.059 \n",
            "(epoch: 103, iters: 2300, time: 0.193) G_GAN: 1.504 G_GAN_Feat: 5.130 G_VGG: 1.624 D_real: 0.058 D_fake: 0.205 \n",
            "(epoch: 103, iters: 2400, time: 0.193) G_GAN: 1.271 G_GAN_Feat: 4.015 G_VGG: 1.289 D_real: 0.148 D_fake: 0.347 \n",
            "(epoch: 103, iters: 2500, time: 0.193) G_GAN: 1.305 G_GAN_Feat: 2.469 G_VGG: 1.146 D_real: 0.173 D_fake: 0.231 \n",
            "(epoch: 103, iters: 2600, time: 0.193) G_GAN: 2.151 G_GAN_Feat: 4.570 G_VGG: 1.286 D_real: 0.064 D_fake: 0.020 \n",
            "saving the latest model (epoch 103, total_steps 431000)\n",
            "(epoch: 103, iters: 2700, time: 0.193) G_GAN: 2.003 G_GAN_Feat: 4.484 G_VGG: 1.547 D_real: 0.048 D_fake: 0.016 \n",
            "(epoch: 103, iters: 2800, time: 0.193) G_GAN: 1.660 G_GAN_Feat: 4.073 G_VGG: 1.493 D_real: 0.336 D_fake: 0.094 \n",
            "(epoch: 103, iters: 2900, time: 0.193) G_GAN: 0.939 G_GAN_Feat: 3.046 G_VGG: 1.062 D_real: 0.049 D_fake: 0.403 \n",
            "(epoch: 103, iters: 3000, time: 0.193) G_GAN: 1.898 G_GAN_Feat: 4.998 G_VGG: 1.690 D_real: 0.152 D_fake: 0.033 \n",
            "(epoch: 103, iters: 3100, time: 0.193) G_GAN: 1.735 G_GAN_Feat: 3.648 G_VGG: 1.303 D_real: 0.022 D_fake: 0.031 \n",
            "(epoch: 103, iters: 3200, time: 0.193) G_GAN: 1.335 G_GAN_Feat: 3.491 G_VGG: 1.289 D_real: 0.099 D_fake: 0.328 \n",
            "(epoch: 103, iters: 3300, time: 0.193) G_GAN: 1.188 G_GAN_Feat: 3.237 G_VGG: 1.051 D_real: 0.204 D_fake: 0.311 \n",
            "(epoch: 103, iters: 3400, time: 0.193) G_GAN: 1.272 G_GAN_Feat: 3.194 G_VGG: 1.408 D_real: 0.027 D_fake: 0.563 \n",
            "(epoch: 103, iters: 3500, time: 0.193) G_GAN: 1.507 G_GAN_Feat: 4.371 G_VGG: 1.547 D_real: 0.015 D_fake: 0.403 \n",
            "(epoch: 103, iters: 3600, time: 0.193) G_GAN: 1.141 G_GAN_Feat: 3.243 G_VGG: 1.286 D_real: 0.026 D_fake: 0.497 \n",
            "saving the latest model (epoch 103, total_steps 432000)\n",
            "(epoch: 103, iters: 3700, time: 0.193) G_GAN: 1.429 G_GAN_Feat: 3.782 G_VGG: 1.178 D_real: 0.100 D_fake: 0.282 \n",
            "(epoch: 103, iters: 3800, time: 0.193) G_GAN: 1.625 G_GAN_Feat: 3.691 G_VGG: 1.574 D_real: 0.125 D_fake: 0.144 \n",
            "(epoch: 103, iters: 3900, time: 0.193) G_GAN: 1.923 G_GAN_Feat: 3.171 G_VGG: 1.225 D_real: 0.189 D_fake: 0.031 \n",
            "(epoch: 103, iters: 4000, time: 0.193) G_GAN: 1.019 G_GAN_Feat: 3.948 G_VGG: 1.221 D_real: 0.026 D_fake: 0.539 \n",
            "(epoch: 103, iters: 4100, time: 0.193) G_GAN: 1.277 G_GAN_Feat: 2.966 G_VGG: 1.350 D_real: 0.255 D_fake: 0.293 \n",
            "(epoch: 103, iters: 4200, time: 0.193) G_GAN: 0.961 G_GAN_Feat: 2.682 G_VGG: 1.225 D_real: 0.079 D_fake: 0.282 \n",
            "End of epoch 103 / 200 \t Time Taken: 873 sec\n",
            "(epoch: 104, iters: 100, time: 0.193) G_GAN: 2.021 G_GAN_Feat: 4.588 G_VGG: 1.365 D_real: 0.051 D_fake: 0.022 \n",
            "(epoch: 104, iters: 200, time: 0.193) G_GAN: 1.538 G_GAN_Feat: 2.938 G_VGG: 1.150 D_real: 0.321 D_fake: 0.191 \n",
            "(epoch: 104, iters: 300, time: 0.193) G_GAN: 1.854 G_GAN_Feat: 4.202 G_VGG: 1.512 D_real: 0.131 D_fake: 0.015 \n",
            "(epoch: 104, iters: 400, time: 0.193) G_GAN: 1.487 G_GAN_Feat: 3.293 G_VGG: 1.266 D_real: 0.217 D_fake: 0.091 \n",
            "saving the latest model (epoch 104, total_steps 433000)\n",
            "(epoch: 104, iters: 500, time: 0.193) G_GAN: 1.103 G_GAN_Feat: 3.011 G_VGG: 1.255 D_real: 0.045 D_fake: 0.520 \n",
            "(epoch: 104, iters: 600, time: 0.193) G_GAN: 1.804 G_GAN_Feat: 4.250 G_VGG: 1.730 D_real: 0.090 D_fake: 0.070 \n",
            "(epoch: 104, iters: 700, time: 0.193) G_GAN: 1.290 G_GAN_Feat: 3.006 G_VGG: 1.305 D_real: 0.085 D_fake: 0.224 \n",
            "(epoch: 104, iters: 800, time: 0.193) G_GAN: 1.406 G_GAN_Feat: 4.003 G_VGG: 1.571 D_real: 0.016 D_fake: 0.136 \n",
            "(epoch: 104, iters: 900, time: 0.193) G_GAN: 0.816 G_GAN_Feat: 2.802 G_VGG: 1.370 D_real: 0.150 D_fake: 0.409 \n",
            "(epoch: 104, iters: 1000, time: 0.193) G_GAN: 2.004 G_GAN_Feat: 4.587 G_VGG: 1.726 D_real: 0.502 D_fake: 0.028 \n",
            "(epoch: 104, iters: 1100, time: 0.193) G_GAN: 1.653 G_GAN_Feat: 3.056 G_VGG: 1.209 D_real: 0.066 D_fake: 0.065 \n",
            "(epoch: 104, iters: 1200, time: 0.193) G_GAN: 1.788 G_GAN_Feat: 3.851 G_VGG: 1.346 D_real: 0.131 D_fake: 0.027 \n",
            "(epoch: 104, iters: 1300, time: 0.193) G_GAN: 1.541 G_GAN_Feat: 3.870 G_VGG: 1.333 D_real: 0.013 D_fake: 0.219 \n",
            "(epoch: 104, iters: 1400, time: 0.193) G_GAN: 1.069 G_GAN_Feat: 2.478 G_VGG: 1.414 D_real: 0.053 D_fake: 0.469 \n",
            "saving the latest model (epoch 104, total_steps 434000)\n",
            "(epoch: 104, iters: 1500, time: 0.193) G_GAN: 2.312 G_GAN_Feat: 3.936 G_VGG: 1.301 D_real: 0.275 D_fake: 0.035 \n",
            "(epoch: 104, iters: 1600, time: 0.193) G_GAN: 1.855 G_GAN_Feat: 3.645 G_VGG: 1.205 D_real: 0.056 D_fake: 0.017 \n",
            "(epoch: 104, iters: 1700, time: 0.193) G_GAN: 1.367 G_GAN_Feat: 3.026 G_VGG: 1.122 D_real: 0.220 D_fake: 0.120 \n",
            "(epoch: 104, iters: 1800, time: 0.193) G_GAN: 1.691 G_GAN_Feat: 3.406 G_VGG: 1.164 D_real: 0.107 D_fake: 0.064 \n",
            "(epoch: 104, iters: 1900, time: 0.193) G_GAN: 1.689 G_GAN_Feat: 3.112 G_VGG: 1.459 D_real: 0.395 D_fake: 0.087 \n",
            "(epoch: 104, iters: 2000, time: 0.193) G_GAN: 2.033 G_GAN_Feat: 3.560 G_VGG: 1.493 D_real: 0.094 D_fake: 0.012 \n",
            "(epoch: 104, iters: 2100, time: 0.193) G_GAN: 1.295 G_GAN_Feat: 2.696 G_VGG: 1.008 D_real: 0.098 D_fake: 0.254 \n",
            "(epoch: 104, iters: 2200, time: 0.193) G_GAN: 1.331 G_GAN_Feat: 3.685 G_VGG: 1.244 D_real: 0.120 D_fake: 0.200 \n",
            "(epoch: 104, iters: 2300, time: 0.193) G_GAN: 1.686 G_GAN_Feat: 3.929 G_VGG: 1.675 D_real: 0.347 D_fake: 0.105 \n",
            "(epoch: 104, iters: 2400, time: 0.193) G_GAN: 1.394 G_GAN_Feat: 4.158 G_VGG: 1.527 D_real: 0.088 D_fake: 0.212 \n",
            "saving the latest model (epoch 104, total_steps 435000)\n",
            "(epoch: 104, iters: 2500, time: 0.193) G_GAN: 2.126 G_GAN_Feat: 4.004 G_VGG: 1.752 D_real: 0.338 D_fake: 0.029 \n",
            "(epoch: 104, iters: 2600, time: 0.193) G_GAN: 1.480 G_GAN_Feat: 3.402 G_VGG: 1.343 D_real: 0.016 D_fake: 0.212 \n",
            "(epoch: 104, iters: 2700, time: 0.193) G_GAN: 1.725 G_GAN_Feat: 3.622 G_VGG: 1.192 D_real: 0.353 D_fake: 0.037 \n",
            "(epoch: 104, iters: 2800, time: 0.193) G_GAN: 1.299 G_GAN_Feat: 2.807 G_VGG: 1.517 D_real: 0.084 D_fake: 0.255 \n",
            "(epoch: 104, iters: 2900, time: 0.193) G_GAN: 0.745 G_GAN_Feat: 2.747 G_VGG: 1.019 D_real: 0.405 D_fake: 0.375 \n",
            "(epoch: 104, iters: 3000, time: 0.193) G_GAN: 2.042 G_GAN_Feat: 5.011 G_VGG: 1.871 D_real: 0.018 D_fake: 0.009 \n",
            "(epoch: 104, iters: 3100, time: 0.193) G_GAN: 1.956 G_GAN_Feat: 3.758 G_VGG: 1.839 D_real: 0.109 D_fake: 0.022 \n",
            "(epoch: 104, iters: 3200, time: 0.193) G_GAN: 1.831 G_GAN_Feat: 3.394 G_VGG: 1.532 D_real: 0.072 D_fake: 0.030 \n",
            "(epoch: 104, iters: 3300, time: 0.193) G_GAN: 1.477 G_GAN_Feat: 4.431 G_VGG: 1.558 D_real: 0.114 D_fake: 0.216 \n",
            "(epoch: 104, iters: 3400, time: 0.193) G_GAN: 1.535 G_GAN_Feat: 4.432 G_VGG: 1.544 D_real: 0.054 D_fake: 0.131 \n",
            "saving the latest model (epoch 104, total_steps 436000)\n",
            "(epoch: 104, iters: 3500, time: 0.193) G_GAN: 1.841 G_GAN_Feat: 4.086 G_VGG: 1.235 D_real: 0.098 D_fake: 0.031 \n",
            "(epoch: 104, iters: 3600, time: 0.193) G_GAN: 1.695 G_GAN_Feat: 3.649 G_VGG: 1.370 D_real: 0.156 D_fake: 0.168 \n",
            "(epoch: 104, iters: 3700, time: 0.193) G_GAN: 2.365 G_GAN_Feat: 4.636 G_VGG: 1.513 D_real: 0.327 D_fake: 0.052 \n",
            "(epoch: 104, iters: 3800, time: 0.193) G_GAN: 1.867 G_GAN_Feat: 3.595 G_VGG: 1.292 D_real: 0.062 D_fake: 0.046 \n",
            "(epoch: 104, iters: 3900, time: 0.193) G_GAN: 1.515 G_GAN_Feat: 3.402 G_VGG: 1.116 D_real: 0.064 D_fake: 0.183 \n",
            "(epoch: 104, iters: 4000, time: 0.193) G_GAN: 1.651 G_GAN_Feat: 3.295 G_VGG: 1.293 D_real: 0.092 D_fake: 0.044 \n",
            "(epoch: 104, iters: 4100, time: 0.193) G_GAN: 1.865 G_GAN_Feat: 3.394 G_VGG: 1.678 D_real: 0.091 D_fake: 0.037 \n",
            "(epoch: 104, iters: 4200, time: 0.193) G_GAN: 1.281 G_GAN_Feat: 2.590 G_VGG: 1.138 D_real: 0.147 D_fake: 0.222 \n",
            "End of epoch 104 / 200 \t Time Taken: 871 sec\n",
            "(epoch: 105, iters: 100, time: 0.193) G_GAN: 1.343 G_GAN_Feat: 3.312 G_VGG: 1.409 D_real: 0.035 D_fake: 0.235 \n",
            "(epoch: 105, iters: 200, time: 0.193) G_GAN: 1.568 G_GAN_Feat: 5.548 G_VGG: 1.539 D_real: 0.026 D_fake: 0.088 \n",
            "saving the latest model (epoch 105, total_steps 437000)\n",
            "(epoch: 105, iters: 300, time: 0.194) G_GAN: 1.992 G_GAN_Feat: 5.161 G_VGG: 1.542 D_real: 0.045 D_fake: 0.042 \n",
            "(epoch: 105, iters: 400, time: 0.193) G_GAN: 1.115 G_GAN_Feat: 3.877 G_VGG: 1.485 D_real: 0.037 D_fake: 0.338 \n",
            "(epoch: 105, iters: 500, time: 0.193) G_GAN: 1.998 G_GAN_Feat: 4.202 G_VGG: 1.587 D_real: 0.026 D_fake: 0.012 \n",
            "(epoch: 105, iters: 600, time: 0.193) G_GAN: 1.992 G_GAN_Feat: 2.760 G_VGG: 0.965 D_real: 0.474 D_fake: 0.095 \n",
            "(epoch: 105, iters: 700, time: 0.193) G_GAN: 1.591 G_GAN_Feat: 2.756 G_VGG: 1.200 D_real: 0.201 D_fake: 0.133 \n",
            "(epoch: 105, iters: 800, time: 0.193) G_GAN: 2.284 G_GAN_Feat: 5.785 G_VGG: 1.255 D_real: 0.030 D_fake: 0.030 \n",
            "(epoch: 105, iters: 900, time: 0.193) G_GAN: 1.619 G_GAN_Feat: 3.435 G_VGG: 1.431 D_real: 0.092 D_fake: 0.071 \n",
            "(epoch: 105, iters: 1000, time: 0.193) G_GAN: 1.397 G_GAN_Feat: 3.097 G_VGG: 1.206 D_real: 0.283 D_fake: 0.141 \n",
            "(epoch: 105, iters: 1100, time: 0.193) G_GAN: 1.024 G_GAN_Feat: 5.497 G_VGG: 1.580 D_real: 0.205 D_fake: 0.220 \n",
            "(epoch: 105, iters: 1200, time: 0.193) G_GAN: 1.522 G_GAN_Feat: 3.032 G_VGG: 0.957 D_real: 0.432 D_fake: 0.097 \n",
            "saving the latest model (epoch 105, total_steps 438000)\n",
            "(epoch: 105, iters: 1300, time: 0.193) G_GAN: 1.424 G_GAN_Feat: 2.569 G_VGG: 1.107 D_real: 0.158 D_fake: 0.149 \n",
            "(epoch: 105, iters: 1400, time: 0.193) G_GAN: 1.822 G_GAN_Feat: 2.519 G_VGG: 1.010 D_real: 0.387 D_fake: 0.050 \n",
            "(epoch: 105, iters: 1500, time: 0.193) G_GAN: 1.025 G_GAN_Feat: 3.111 G_VGG: 1.229 D_real: 0.046 D_fake: 0.348 \n",
            "(epoch: 105, iters: 1600, time: 0.193) G_GAN: 1.199 G_GAN_Feat: 3.523 G_VGG: 1.280 D_real: 0.025 D_fake: 0.265 \n",
            "(epoch: 105, iters: 1700, time: 0.193) G_GAN: 1.711 G_GAN_Feat: 3.762 G_VGG: 1.356 D_real: 0.095 D_fake: 0.074 \n",
            "(epoch: 105, iters: 1800, time: 0.193) G_GAN: 1.418 G_GAN_Feat: 3.514 G_VGG: 1.283 D_real: 0.039 D_fake: 0.138 \n",
            "(epoch: 105, iters: 1900, time: 0.193) G_GAN: 1.169 G_GAN_Feat: 3.025 G_VGG: 1.184 D_real: 0.154 D_fake: 0.357 \n",
            "(epoch: 105, iters: 2000, time: 0.193) G_GAN: 1.619 G_GAN_Feat: 3.062 G_VGG: 1.346 D_real: 0.224 D_fake: 0.112 \n",
            "(epoch: 105, iters: 2100, time: 0.193) G_GAN: 1.440 G_GAN_Feat: 3.658 G_VGG: 1.741 D_real: 0.149 D_fake: 0.161 \n",
            "(epoch: 105, iters: 2200, time: 0.193) G_GAN: 1.589 G_GAN_Feat: 3.099 G_VGG: 1.147 D_real: 0.046 D_fake: 0.113 \n",
            "saving the latest model (epoch 105, total_steps 439000)\n",
            "(epoch: 105, iters: 2300, time: 0.193) G_GAN: 1.147 G_GAN_Feat: 3.566 G_VGG: 1.432 D_real: 0.076 D_fake: 0.415 \n",
            "(epoch: 105, iters: 2400, time: 0.193) G_GAN: 1.085 G_GAN_Feat: 2.980 G_VGG: 1.279 D_real: 0.097 D_fake: 0.258 \n",
            "(epoch: 105, iters: 2500, time: 0.193) G_GAN: 1.349 G_GAN_Feat: 2.489 G_VGG: 1.054 D_real: 0.038 D_fake: 0.188 \n",
            "(epoch: 105, iters: 2600, time: 0.193) G_GAN: 1.363 G_GAN_Feat: 2.707 G_VGG: 1.131 D_real: 0.105 D_fake: 0.199 \n",
            "(epoch: 105, iters: 2700, time: 0.193) G_GAN: 1.916 G_GAN_Feat: 3.011 G_VGG: 1.129 D_real: 0.073 D_fake: 0.038 \n",
            "(epoch: 105, iters: 2800, time: 0.193) G_GAN: 1.716 G_GAN_Feat: 4.444 G_VGG: 1.477 D_real: 0.030 D_fake: 0.175 \n",
            "(epoch: 105, iters: 2900, time: 0.193) G_GAN: 1.790 G_GAN_Feat: 4.489 G_VGG: 1.367 D_real: 0.021 D_fake: 0.041 \n",
            "(epoch: 105, iters: 3000, time: 0.193) G_GAN: 0.622 G_GAN_Feat: 2.739 G_VGG: 1.194 D_real: 0.020 D_fake: 0.996 \n",
            "(epoch: 105, iters: 3100, time: 0.193) G_GAN: 1.354 G_GAN_Feat: 3.227 G_VGG: 1.391 D_real: 0.094 D_fake: 0.277 \n",
            "(epoch: 105, iters: 3200, time: 0.193) G_GAN: 1.543 G_GAN_Feat: 2.480 G_VGG: 1.414 D_real: 0.029 D_fake: 0.219 \n",
            "saving the latest model (epoch 105, total_steps 440000)\n",
            "(epoch: 105, iters: 3300, time: 0.194) G_GAN: 1.405 G_GAN_Feat: 3.056 G_VGG: 1.073 D_real: 0.195 D_fake: 0.170 \n",
            "(epoch: 105, iters: 3400, time: 0.193) G_GAN: 1.435 G_GAN_Feat: 3.208 G_VGG: 1.414 D_real: 0.075 D_fake: 0.458 \n",
            "(epoch: 105, iters: 3500, time: 0.193) G_GAN: 1.433 G_GAN_Feat: 3.564 G_VGG: 1.366 D_real: 0.096 D_fake: 0.120 \n",
            "(epoch: 105, iters: 3600, time: 0.193) G_GAN: 1.975 G_GAN_Feat: 4.197 G_VGG: 1.548 D_real: 0.227 D_fake: 0.068 \n",
            "(epoch: 105, iters: 3700, time: 0.193) G_GAN: 1.578 G_GAN_Feat: 3.085 G_VGG: 1.397 D_real: 0.048 D_fake: 0.249 \n",
            "(epoch: 105, iters: 3800, time: 0.193) G_GAN: 1.109 G_GAN_Feat: 3.083 G_VGG: 1.247 D_real: 0.026 D_fake: 0.422 \n",
            "(epoch: 105, iters: 3900, time: 0.193) G_GAN: 1.998 G_GAN_Feat: 4.049 G_VGG: 1.419 D_real: 0.151 D_fake: 0.143 \n",
            "(epoch: 105, iters: 4000, time: 0.193) G_GAN: 2.532 G_GAN_Feat: 3.664 G_VGG: 1.720 D_real: 0.333 D_fake: 0.061 \n",
            "(epoch: 105, iters: 4100, time: 0.193) G_GAN: 1.940 G_GAN_Feat: 4.629 G_VGG: 1.280 D_real: 0.160 D_fake: 0.037 \n",
            "(epoch: 105, iters: 4200, time: 0.193) G_GAN: 1.351 G_GAN_Feat: 4.840 G_VGG: 1.855 D_real: 0.085 D_fake: 0.348 \n",
            "saving the latest model (epoch 105, total_steps 441000)\n",
            "End of epoch 105 / 200 \t Time Taken: 885 sec\n",
            "(epoch: 106, iters: 100, time: 0.193) G_GAN: 1.861 G_GAN_Feat: 4.108 G_VGG: 1.210 D_real: 0.063 D_fake: 0.033 \n",
            "(epoch: 106, iters: 200, time: 0.193) G_GAN: 1.682 G_GAN_Feat: 2.811 G_VGG: 1.093 D_real: 0.403 D_fake: 0.041 \n",
            "(epoch: 106, iters: 300, time: 0.193) G_GAN: 1.264 G_GAN_Feat: 2.748 G_VGG: 1.238 D_real: 0.113 D_fake: 0.229 \n",
            "(epoch: 106, iters: 400, time: 0.193) G_GAN: 2.183 G_GAN_Feat: 3.256 G_VGG: 1.324 D_real: 0.215 D_fake: 0.039 \n",
            "(epoch: 106, iters: 500, time: 0.193) G_GAN: 1.037 G_GAN_Feat: 3.334 G_VGG: 1.073 D_real: 0.170 D_fake: 0.257 \n",
            "(epoch: 106, iters: 600, time: 0.193) G_GAN: 1.703 G_GAN_Feat: 4.369 G_VGG: 1.284 D_real: 0.205 D_fake: 0.091 \n",
            "(epoch: 106, iters: 700, time: 0.193) G_GAN: 1.792 G_GAN_Feat: 4.377 G_VGG: 1.844 D_real: 0.374 D_fake: 0.095 \n",
            "(epoch: 106, iters: 800, time: 0.193) G_GAN: 1.462 G_GAN_Feat: 2.988 G_VGG: 1.320 D_real: 0.123 D_fake: 0.093 \n",
            "(epoch: 106, iters: 900, time: 0.193) G_GAN: 1.767 G_GAN_Feat: 3.434 G_VGG: 1.379 D_real: 0.045 D_fake: 0.155 \n",
            "(epoch: 106, iters: 1000, time: 0.193) G_GAN: 1.618 G_GAN_Feat: 3.257 G_VGG: 0.983 D_real: 0.022 D_fake: 0.073 \n",
            "saving the latest model (epoch 106, total_steps 442000)\n",
            "(epoch: 106, iters: 1100, time: 0.193) G_GAN: 1.559 G_GAN_Feat: 3.202 G_VGG: 1.135 D_real: 0.081 D_fake: 0.173 \n",
            "(epoch: 106, iters: 1200, time: 0.193) G_GAN: 2.071 G_GAN_Feat: 3.444 G_VGG: 1.274 D_real: 0.335 D_fake: 0.073 \n",
            "(epoch: 106, iters: 1300, time: 0.193) G_GAN: 1.802 G_GAN_Feat: 2.314 G_VGG: 1.070 D_real: 0.268 D_fake: 0.061 \n",
            "(epoch: 106, iters: 1400, time: 0.193) G_GAN: 1.715 G_GAN_Feat: 4.480 G_VGG: 1.521 D_real: 0.070 D_fake: 0.065 \n",
            "(epoch: 106, iters: 1500, time: 0.193) G_GAN: 2.024 G_GAN_Feat: 4.105 G_VGG: 1.482 D_real: 0.016 D_fake: 0.014 \n",
            "(epoch: 106, iters: 1600, time: 0.193) G_GAN: 1.944 G_GAN_Feat: 4.118 G_VGG: 1.295 D_real: 0.187 D_fake: 0.023 \n",
            "(epoch: 106, iters: 1700, time: 0.193) G_GAN: 1.595 G_GAN_Feat: 3.281 G_VGG: 1.428 D_real: 0.052 D_fake: 0.123 \n",
            "(epoch: 106, iters: 1800, time: 0.193) G_GAN: 1.806 G_GAN_Feat: 3.112 G_VGG: 1.274 D_real: 0.218 D_fake: 0.051 \n",
            "(epoch: 106, iters: 1900, time: 0.193) G_GAN: 1.941 G_GAN_Feat: 4.498 G_VGG: 1.565 D_real: 0.052 D_fake: 0.014 \n",
            "(epoch: 106, iters: 2000, time: 0.193) G_GAN: 1.794 G_GAN_Feat: 3.172 G_VGG: 1.117 D_real: 0.283 D_fake: 0.114 \n",
            "saving the latest model (epoch 106, total_steps 443000)\n",
            "(epoch: 106, iters: 2100, time: 0.194) G_GAN: 1.423 G_GAN_Feat: 3.069 G_VGG: 1.021 D_real: 0.030 D_fake: 0.149 \n",
            "(epoch: 106, iters: 2200, time: 0.193) G_GAN: 1.102 G_GAN_Feat: 3.324 G_VGG: 1.309 D_real: 0.251 D_fake: 0.179 \n",
            "(epoch: 106, iters: 2300, time: 0.193) G_GAN: 1.947 G_GAN_Feat: 3.706 G_VGG: 1.375 D_real: 0.064 D_fake: 0.040 \n",
            "(epoch: 106, iters: 2400, time: 0.193) G_GAN: 1.770 G_GAN_Feat: 4.021 G_VGG: 1.332 D_real: 0.119 D_fake: 0.046 \n",
            "(epoch: 106, iters: 2500, time: 0.193) G_GAN: 2.529 G_GAN_Feat: 3.784 G_VGG: 1.103 D_real: 0.510 D_fake: 0.046 \n",
            "(epoch: 106, iters: 2600, time: 0.193) G_GAN: 1.575 G_GAN_Feat: 3.184 G_VGG: 1.226 D_real: 0.124 D_fake: 0.059 \n",
            "(epoch: 106, iters: 2700, time: 0.193) G_GAN: 1.620 G_GAN_Feat: 3.280 G_VGG: 1.508 D_real: 0.065 D_fake: 0.079 \n",
            "(epoch: 106, iters: 2800, time: 0.193) G_GAN: 1.716 G_GAN_Feat: 2.798 G_VGG: 1.508 D_real: 0.101 D_fake: 0.082 \n",
            "(epoch: 106, iters: 2900, time: 0.193) G_GAN: 1.324 G_GAN_Feat: 3.654 G_VGG: 1.305 D_real: 0.139 D_fake: 0.119 \n",
            "(epoch: 106, iters: 3000, time: 0.193) G_GAN: 2.040 G_GAN_Feat: 3.120 G_VGG: 1.058 D_real: 0.153 D_fake: 0.025 \n",
            "saving the latest model (epoch 106, total_steps 444000)\n",
            "(epoch: 106, iters: 3100, time: 0.193) G_GAN: 1.854 G_GAN_Feat: 3.257 G_VGG: 1.436 D_real: 0.230 D_fake: 0.018 \n",
            "(epoch: 106, iters: 3200, time: 0.193) G_GAN: 1.352 G_GAN_Feat: 3.222 G_VGG: 1.322 D_real: 0.029 D_fake: 0.313 \n",
            "(epoch: 106, iters: 3300, time: 0.193) G_GAN: 1.780 G_GAN_Feat: 2.622 G_VGG: 0.923 D_real: 0.068 D_fake: 0.074 \n",
            "(epoch: 106, iters: 3400, time: 0.193) G_GAN: 1.766 G_GAN_Feat: 3.542 G_VGG: 1.357 D_real: 0.075 D_fake: 0.058 \n",
            "(epoch: 106, iters: 3500, time: 0.193) G_GAN: 2.236 G_GAN_Feat: 3.443 G_VGG: 1.330 D_real: 0.099 D_fake: 0.038 \n",
            "(epoch: 106, iters: 3600, time: 0.193) G_GAN: 1.656 G_GAN_Feat: 3.720 G_VGG: 1.370 D_real: 0.018 D_fake: 0.042 \n",
            "(epoch: 106, iters: 3700, time: 0.193) G_GAN: 1.721 G_GAN_Feat: 3.830 G_VGG: 1.504 D_real: 0.085 D_fake: 0.090 \n",
            "(epoch: 106, iters: 3800, time: 0.193) G_GAN: 1.065 G_GAN_Feat: 2.774 G_VGG: 1.276 D_real: 0.037 D_fake: 0.626 \n",
            "(epoch: 106, iters: 3900, time: 0.193) G_GAN: 1.541 G_GAN_Feat: 4.091 G_VGG: 1.414 D_real: 0.015 D_fake: 0.107 \n",
            "(epoch: 106, iters: 4000, time: 0.193) G_GAN: 1.155 G_GAN_Feat: 3.575 G_VGG: 1.200 D_real: 0.100 D_fake: 0.540 \n",
            "saving the latest model (epoch 106, total_steps 445000)\n",
            "(epoch: 106, iters: 4100, time: 0.194) G_GAN: 1.556 G_GAN_Feat: 3.092 G_VGG: 1.333 D_real: 0.011 D_fake: 0.159 \n",
            "(epoch: 106, iters: 4200, time: 0.193) G_GAN: 1.526 G_GAN_Feat: 2.934 G_VGG: 0.909 D_real: 0.019 D_fake: 0.219 \n",
            "End of epoch 106 / 200 \t Time Taken: 875 sec\n",
            "(epoch: 107, iters: 100, time: 0.193) G_GAN: 2.170 G_GAN_Feat: 4.194 G_VGG: 1.422 D_real: 0.498 D_fake: 0.022 \n",
            "(epoch: 107, iters: 200, time: 0.193) G_GAN: 2.352 G_GAN_Feat: 2.980 G_VGG: 1.313 D_real: 0.122 D_fake: 0.042 \n",
            "(epoch: 107, iters: 300, time: 0.193) G_GAN: 2.083 G_GAN_Feat: 4.039 G_VGG: 1.317 D_real: 0.335 D_fake: 0.033 \n",
            "(epoch: 107, iters: 400, time: 0.193) G_GAN: 2.061 G_GAN_Feat: 3.260 G_VGG: 0.976 D_real: 0.225 D_fake: 0.048 \n",
            "(epoch: 107, iters: 500, time: 0.193) G_GAN: 1.487 G_GAN_Feat: 3.131 G_VGG: 1.329 D_real: 0.248 D_fake: 0.125 \n",
            "(epoch: 107, iters: 600, time: 0.193) G_GAN: 1.363 G_GAN_Feat: 2.421 G_VGG: 0.932 D_real: 0.024 D_fake: 0.234 \n",
            "(epoch: 107, iters: 700, time: 0.193) G_GAN: 1.872 G_GAN_Feat: 4.298 G_VGG: 1.311 D_real: 0.043 D_fake: 0.024 \n",
            "(epoch: 107, iters: 800, time: 0.193) G_GAN: 1.420 G_GAN_Feat: 3.234 G_VGG: 1.083 D_real: 0.040 D_fake: 0.160 \n",
            "saving the latest model (epoch 107, total_steps 446000)\n",
            "(epoch: 107, iters: 900, time: 0.193) G_GAN: 1.192 G_GAN_Feat: 2.550 G_VGG: 1.157 D_real: 0.119 D_fake: 0.285 \n",
            "(epoch: 107, iters: 1000, time: 0.193) G_GAN: 1.175 G_GAN_Feat: 3.595 G_VGG: 1.533 D_real: 0.030 D_fake: 0.570 \n",
            "(epoch: 107, iters: 1100, time: 0.193) G_GAN: 1.963 G_GAN_Feat: 3.487 G_VGG: 1.297 D_real: 0.016 D_fake: 0.013 \n",
            "(epoch: 107, iters: 1200, time: 0.193) G_GAN: 1.935 G_GAN_Feat: 3.066 G_VGG: 1.529 D_real: 0.180 D_fake: 0.031 \n",
            "(epoch: 107, iters: 1300, time: 0.193) G_GAN: 1.598 G_GAN_Feat: 2.300 G_VGG: 1.233 D_real: 0.160 D_fake: 0.089 \n",
            "(epoch: 107, iters: 1400, time: 0.193) G_GAN: 1.364 G_GAN_Feat: 3.260 G_VGG: 1.460 D_real: 0.079 D_fake: 0.178 \n",
            "(epoch: 107, iters: 1500, time: 0.193) G_GAN: 1.809 G_GAN_Feat: 2.507 G_VGG: 1.063 D_real: 0.220 D_fake: 0.087 \n",
            "(epoch: 107, iters: 1600, time: 0.193) G_GAN: 2.156 G_GAN_Feat: 3.001 G_VGG: 1.051 D_real: 0.115 D_fake: 0.305 \n",
            "(epoch: 107, iters: 1700, time: 0.193) G_GAN: 0.675 G_GAN_Feat: 2.748 G_VGG: 1.479 D_real: 0.172 D_fake: 0.945 \n",
            "(epoch: 107, iters: 1800, time: 0.193) G_GAN: 1.332 G_GAN_Feat: 2.794 G_VGG: 1.197 D_real: 0.039 D_fake: 0.140 \n",
            "saving the latest model (epoch 107, total_steps 447000)\n",
            "(epoch: 107, iters: 1900, time: 0.193) G_GAN: 1.673 G_GAN_Feat: 2.144 G_VGG: 1.173 D_real: 0.307 D_fake: 0.092 \n",
            "(epoch: 107, iters: 2000, time: 0.193) G_GAN: 1.395 G_GAN_Feat: 3.007 G_VGG: 0.990 D_real: 0.021 D_fake: 0.260 \n",
            "(epoch: 107, iters: 2100, time: 0.193) G_GAN: 1.569 G_GAN_Feat: 3.425 G_VGG: 1.246 D_real: 0.210 D_fake: 0.086 \n",
            "(epoch: 107, iters: 2200, time: 0.193) G_GAN: 1.460 G_GAN_Feat: 3.396 G_VGG: 1.612 D_real: 0.066 D_fake: 0.091 \n",
            "(epoch: 107, iters: 2300, time: 0.193) G_GAN: 1.608 G_GAN_Feat: 2.879 G_VGG: 1.203 D_real: 0.095 D_fake: 0.114 \n",
            "(epoch: 107, iters: 2400, time: 0.193) G_GAN: 2.141 G_GAN_Feat: 4.068 G_VGG: 0.945 D_real: 0.483 D_fake: 0.017 \n",
            "(epoch: 107, iters: 2500, time: 0.193) G_GAN: 1.641 G_GAN_Feat: 3.246 G_VGG: 1.059 D_real: 0.040 D_fake: 0.064 \n",
            "(epoch: 107, iters: 2600, time: 0.193) G_GAN: 1.264 G_GAN_Feat: 2.624 G_VGG: 1.032 D_real: 0.143 D_fake: 0.373 \n",
            "(epoch: 107, iters: 2700, time: 0.193) G_GAN: 1.076 G_GAN_Feat: 2.558 G_VGG: 0.985 D_real: 0.019 D_fake: 0.256 \n",
            "(epoch: 107, iters: 2800, time: 0.193) G_GAN: 1.765 G_GAN_Feat: 2.282 G_VGG: 0.835 D_real: 0.262 D_fake: 0.124 \n",
            "saving the latest model (epoch 107, total_steps 448000)\n",
            "(epoch: 107, iters: 2900, time: 0.193) G_GAN: 1.668 G_GAN_Feat: 3.326 G_VGG: 1.607 D_real: 0.312 D_fake: 0.052 \n",
            "(epoch: 107, iters: 3000, time: 0.193) G_GAN: 1.491 G_GAN_Feat: 2.879 G_VGG: 1.692 D_real: 0.731 D_fake: 0.090 \n",
            "(epoch: 107, iters: 3100, time: 0.193) G_GAN: 1.763 G_GAN_Feat: 3.348 G_VGG: 1.408 D_real: 0.500 D_fake: 0.034 \n",
            "(epoch: 107, iters: 3200, time: 0.193) G_GAN: 1.578 G_GAN_Feat: 3.295 G_VGG: 1.393 D_real: 0.079 D_fake: 0.243 \n",
            "(epoch: 107, iters: 3300, time: 0.193) G_GAN: 1.877 G_GAN_Feat: 3.877 G_VGG: 1.152 D_real: 0.245 D_fake: 0.040 \n",
            "(epoch: 107, iters: 3400, time: 0.193) G_GAN: 1.108 G_GAN_Feat: 3.370 G_VGG: 1.049 D_real: 0.030 D_fake: 0.521 \n",
            "(epoch: 107, iters: 3500, time: 0.193) G_GAN: 2.182 G_GAN_Feat: 3.492 G_VGG: 1.463 D_real: 0.505 D_fake: 0.025 \n",
            "(epoch: 107, iters: 3600, time: 0.193) G_GAN: 0.618 G_GAN_Feat: 2.926 G_VGG: 1.203 D_real: 0.036 D_fake: 0.624 \n",
            "(epoch: 107, iters: 3700, time: 0.193) G_GAN: 1.825 G_GAN_Feat: 4.292 G_VGG: 1.447 D_real: 0.081 D_fake: 0.322 \n",
            "(epoch: 107, iters: 3800, time: 0.193) G_GAN: 0.917 G_GAN_Feat: 2.351 G_VGG: 0.954 D_real: 0.079 D_fake: 0.316 \n",
            "saving the latest model (epoch 107, total_steps 449000)\n",
            "(epoch: 107, iters: 3900, time: 0.193) G_GAN: 1.442 G_GAN_Feat: 3.079 G_VGG: 1.016 D_real: 0.085 D_fake: 0.255 \n",
            "(epoch: 107, iters: 4000, time: 0.193) G_GAN: 1.379 G_GAN_Feat: 2.729 G_VGG: 1.139 D_real: 0.064 D_fake: 0.237 \n",
            "(epoch: 107, iters: 4100, time: 0.193) G_GAN: 1.696 G_GAN_Feat: 3.489 G_VGG: 1.578 D_real: 0.364 D_fake: 0.031 \n",
            "(epoch: 107, iters: 4200, time: 0.193) G_GAN: 1.499 G_GAN_Feat: 2.745 G_VGG: 1.216 D_real: 0.203 D_fake: 0.060 \n",
            "End of epoch 107 / 200 \t Time Taken: 874 sec\n",
            "(epoch: 108, iters: 100, time: 0.193) G_GAN: 1.036 G_GAN_Feat: 2.868 G_VGG: 1.111 D_real: 0.052 D_fake: 0.275 \n",
            "(epoch: 108, iters: 200, time: 0.193) G_GAN: 2.007 G_GAN_Feat: 2.767 G_VGG: 1.187 D_real: 0.423 D_fake: 0.018 \n",
            "(epoch: 108, iters: 300, time: 0.193) G_GAN: 0.905 G_GAN_Feat: 2.306 G_VGG: 1.155 D_real: 0.122 D_fake: 0.330 \n",
            "(epoch: 108, iters: 400, time: 0.193) G_GAN: 1.598 G_GAN_Feat: 3.456 G_VGG: 2.329 D_real: 0.704 D_fake: 0.092 \n",
            "(epoch: 108, iters: 500, time: 0.193) G_GAN: 1.178 G_GAN_Feat: 2.719 G_VGG: 1.319 D_real: 0.047 D_fake: 0.158 \n",
            "(epoch: 108, iters: 600, time: 0.193) G_GAN: 1.673 G_GAN_Feat: 3.257 G_VGG: 1.283 D_real: 0.057 D_fake: 0.090 \n",
            "saving the latest model (epoch 108, total_steps 450000)\n",
            "(epoch: 108, iters: 700, time: 0.193) G_GAN: 1.406 G_GAN_Feat: 4.442 G_VGG: 0.852 D_real: 0.015 D_fake: 0.149 \n",
            "(epoch: 108, iters: 800, time: 0.193) G_GAN: 1.523 G_GAN_Feat: 2.930 G_VGG: 0.996 D_real: 0.113 D_fake: 0.107 \n",
            "(epoch: 108, iters: 900, time: 0.193) G_GAN: 2.191 G_GAN_Feat: 3.939 G_VGG: 1.319 D_real: 0.146 D_fake: 0.015 \n",
            "(epoch: 108, iters: 1000, time: 0.193) G_GAN: 1.871 G_GAN_Feat: 3.327 G_VGG: 1.229 D_real: 0.045 D_fake: 0.069 \n",
            "(epoch: 108, iters: 1100, time: 0.193) G_GAN: 1.586 G_GAN_Feat: 3.727 G_VGG: 1.021 D_real: 0.018 D_fake: 0.076 \n",
            "(epoch: 108, iters: 1200, time: 0.193) G_GAN: 1.221 G_GAN_Feat: 2.719 G_VGG: 1.012 D_real: 0.048 D_fake: 0.251 \n",
            "(epoch: 108, iters: 1300, time: 0.193) G_GAN: 1.422 G_GAN_Feat: 2.918 G_VGG: 1.334 D_real: 0.018 D_fake: 0.226 \n",
            "(epoch: 108, iters: 1400, time: 0.193) G_GAN: 1.231 G_GAN_Feat: 2.316 G_VGG: 1.021 D_real: 0.028 D_fake: 0.485 \n",
            "(epoch: 108, iters: 1500, time: 0.193) G_GAN: 2.398 G_GAN_Feat: 3.162 G_VGG: 1.378 D_real: 0.854 D_fake: 0.076 \n",
            "(epoch: 108, iters: 1600, time: 0.193) G_GAN: 1.479 G_GAN_Feat: 3.691 G_VGG: 0.991 D_real: 0.040 D_fake: 0.228 \n",
            "saving the latest model (epoch 108, total_steps 451000)\n",
            "(epoch: 108, iters: 1700, time: 0.193) G_GAN: 1.553 G_GAN_Feat: 2.844 G_VGG: 0.947 D_real: 0.031 D_fake: 0.128 \n",
            "(epoch: 108, iters: 1800, time: 0.193) G_GAN: 1.615 G_GAN_Feat: 2.395 G_VGG: 1.104 D_real: 0.129 D_fake: 0.124 \n",
            "(epoch: 108, iters: 1900, time: 0.193) G_GAN: 1.302 G_GAN_Feat: 3.299 G_VGG: 1.235 D_real: 0.657 D_fake: 0.200 \n",
            "(epoch: 108, iters: 2000, time: 0.193) G_GAN: 2.059 G_GAN_Feat: 4.055 G_VGG: 1.116 D_real: 0.058 D_fake: 0.041 \n",
            "(epoch: 108, iters: 2100, time: 0.193) G_GAN: 1.704 G_GAN_Feat: 2.694 G_VGG: 1.249 D_real: 0.208 D_fake: 0.044 \n",
            "(epoch: 108, iters: 2200, time: 0.193) G_GAN: 1.380 G_GAN_Feat: 2.858 G_VGG: 1.246 D_real: 0.088 D_fake: 0.267 \n",
            "(epoch: 108, iters: 2300, time: 0.193) G_GAN: 1.920 G_GAN_Feat: 3.520 G_VGG: 1.236 D_real: 0.074 D_fake: 0.050 \n",
            "(epoch: 108, iters: 2400, time: 0.193) G_GAN: 1.565 G_GAN_Feat: 3.239 G_VGG: 1.368 D_real: 0.145 D_fake: 0.071 \n",
            "(epoch: 108, iters: 2500, time: 0.193) G_GAN: 2.118 G_GAN_Feat: 3.767 G_VGG: 1.510 D_real: 0.114 D_fake: 0.072 \n",
            "(epoch: 108, iters: 2600, time: 0.193) G_GAN: 2.077 G_GAN_Feat: 3.570 G_VGG: 1.149 D_real: 0.069 D_fake: 0.036 \n",
            "saving the latest model (epoch 108, total_steps 452000)\n",
            "(epoch: 108, iters: 2700, time: 0.193) G_GAN: 1.757 G_GAN_Feat: 4.152 G_VGG: 1.423 D_real: 0.051 D_fake: 0.074 \n",
            "(epoch: 108, iters: 2800, time: 0.193) G_GAN: 2.098 G_GAN_Feat: 2.577 G_VGG: 1.112 D_real: 0.293 D_fake: 0.035 \n",
            "(epoch: 108, iters: 2900, time: 0.193) G_GAN: 1.954 G_GAN_Feat: 3.622 G_VGG: 1.456 D_real: 0.071 D_fake: 0.014 \n",
            "(epoch: 108, iters: 3000, time: 0.193) G_GAN: 1.846 G_GAN_Feat: 3.308 G_VGG: 1.046 D_real: 0.224 D_fake: 0.029 \n",
            "(epoch: 108, iters: 3100, time: 0.193) G_GAN: 1.424 G_GAN_Feat: 2.739 G_VGG: 1.185 D_real: 0.058 D_fake: 0.189 \n",
            "(epoch: 108, iters: 3200, time: 0.193) G_GAN: 1.896 G_GAN_Feat: 2.743 G_VGG: 1.056 D_real: 0.476 D_fake: 0.018 \n",
            "(epoch: 108, iters: 3300, time: 0.193) G_GAN: 1.595 G_GAN_Feat: 3.785 G_VGG: 1.062 D_real: 0.097 D_fake: 0.173 \n",
            "(epoch: 108, iters: 3400, time: 0.193) G_GAN: 1.853 G_GAN_Feat: 2.436 G_VGG: 1.105 D_real: 0.158 D_fake: 0.027 \n",
            "(epoch: 108, iters: 3500, time: 0.193) G_GAN: 1.460 G_GAN_Feat: 4.029 G_VGG: 1.312 D_real: 0.018 D_fake: 0.148 \n",
            "(epoch: 108, iters: 3600, time: 0.193) G_GAN: 1.859 G_GAN_Feat: 3.107 G_VGG: 0.939 D_real: 0.300 D_fake: 0.176 \n",
            "saving the latest model (epoch 108, total_steps 453000)\n",
            "(epoch: 108, iters: 3700, time: 0.193) G_GAN: 1.572 G_GAN_Feat: 2.399 G_VGG: 1.107 D_real: 0.116 D_fake: 0.077 \n",
            "(epoch: 108, iters: 3800, time: 0.193) G_GAN: 1.217 G_GAN_Feat: 2.565 G_VGG: 1.074 D_real: 0.025 D_fake: 0.330 \n",
            "(epoch: 108, iters: 3900, time: 0.193) G_GAN: 1.788 G_GAN_Feat: 3.439 G_VGG: 1.317 D_real: 0.180 D_fake: 0.072 \n",
            "(epoch: 108, iters: 4000, time: 0.193) G_GAN: 2.177 G_GAN_Feat: 3.312 G_VGG: 1.058 D_real: 0.091 D_fake: 0.021 \n",
            "(epoch: 108, iters: 4100, time: 0.193) G_GAN: 1.523 G_GAN_Feat: 2.208 G_VGG: 1.088 D_real: 0.203 D_fake: 0.142 \n",
            "(epoch: 108, iters: 4200, time: 0.193) G_GAN: 1.929 G_GAN_Feat: 3.017 G_VGG: 1.143 D_real: 0.090 D_fake: 0.023 \n",
            "End of epoch 108 / 200 \t Time Taken: 878 sec\n",
            "(epoch: 109, iters: 100, time: 0.193) G_GAN: 1.281 G_GAN_Feat: 2.425 G_VGG: 1.092 D_real: 0.259 D_fake: 0.334 \n",
            "(epoch: 109, iters: 200, time: 0.193) G_GAN: 1.391 G_GAN_Feat: 3.120 G_VGG: 0.902 D_real: 0.038 D_fake: 0.218 \n",
            "(epoch: 109, iters: 300, time: 0.193) G_GAN: 2.020 G_GAN_Feat: 4.608 G_VGG: 1.590 D_real: 0.042 D_fake: 0.031 \n",
            "(epoch: 109, iters: 400, time: 0.193) G_GAN: 1.975 G_GAN_Feat: 2.669 G_VGG: 1.225 D_real: 0.671 D_fake: 0.052 \n",
            "saving the latest model (epoch 109, total_steps 454000)\n",
            "(epoch: 109, iters: 500, time: 0.193) G_GAN: 1.734 G_GAN_Feat: 2.129 G_VGG: 1.271 D_real: 0.368 D_fake: 0.212 \n",
            "(epoch: 109, iters: 600, time: 0.193) G_GAN: 1.000 G_GAN_Feat: 2.886 G_VGG: 1.419 D_real: 0.042 D_fake: 0.779 \n",
            "(epoch: 109, iters: 700, time: 0.193) G_GAN: 1.617 G_GAN_Feat: 2.717 G_VGG: 1.307 D_real: 0.433 D_fake: 0.091 \n",
            "(epoch: 109, iters: 800, time: 0.193) G_GAN: 2.212 G_GAN_Feat: 3.682 G_VGG: 1.582 D_real: 0.023 D_fake: 0.025 \n",
            "(epoch: 109, iters: 900, time: 0.193) G_GAN: 1.548 G_GAN_Feat: 2.547 G_VGG: 1.127 D_real: 0.076 D_fake: 0.124 \n",
            "(epoch: 109, iters: 1000, time: 0.193) G_GAN: 2.009 G_GAN_Feat: 2.697 G_VGG: 1.127 D_real: 0.464 D_fake: 0.204 \n",
            "(epoch: 109, iters: 1100, time: 0.193) G_GAN: 1.669 G_GAN_Feat: 3.396 G_VGG: 1.207 D_real: 0.046 D_fake: 0.075 \n",
            "(epoch: 109, iters: 1200, time: 0.193) G_GAN: 1.440 G_GAN_Feat: 3.189 G_VGG: 1.184 D_real: 0.020 D_fake: 0.100 \n",
            "(epoch: 109, iters: 1300, time: 0.193) G_GAN: 2.036 G_GAN_Feat: 3.736 G_VGG: 1.194 D_real: 0.058 D_fake: 0.015 \n",
            "(epoch: 109, iters: 1400, time: 0.193) G_GAN: 1.415 G_GAN_Feat: 4.489 G_VGG: 1.568 D_real: 0.067 D_fake: 0.282 \n",
            "saving the latest model (epoch 109, total_steps 455000)\n",
            "(epoch: 109, iters: 1500, time: 0.193) G_GAN: 1.302 G_GAN_Feat: 3.583 G_VGG: 1.400 D_real: 0.122 D_fake: 0.138 \n",
            "(epoch: 109, iters: 1600, time: 0.193) G_GAN: 1.690 G_GAN_Feat: 2.896 G_VGG: 1.479 D_real: 0.139 D_fake: 0.060 \n",
            "(epoch: 109, iters: 1700, time: 0.193) G_GAN: 1.445 G_GAN_Feat: 3.441 G_VGG: 1.148 D_real: 0.131 D_fake: 0.137 \n",
            "(epoch: 109, iters: 1800, time: 0.193) G_GAN: 1.415 G_GAN_Feat: 2.589 G_VGG: 1.275 D_real: 0.133 D_fake: 0.142 \n",
            "(epoch: 109, iters: 1900, time: 0.193) G_GAN: 1.309 G_GAN_Feat: 2.559 G_VGG: 0.871 D_real: 0.061 D_fake: 0.265 \n",
            "(epoch: 109, iters: 2000, time: 0.193) G_GAN: 1.715 G_GAN_Feat: 3.737 G_VGG: 1.259 D_real: 0.133 D_fake: 0.028 \n",
            "(epoch: 109, iters: 2100, time: 0.193) G_GAN: 1.446 G_GAN_Feat: 3.069 G_VGG: 1.228 D_real: 0.161 D_fake: 0.148 \n",
            "(epoch: 109, iters: 2200, time: 0.193) G_GAN: 1.693 G_GAN_Feat: 3.170 G_VGG: 1.343 D_real: 0.302 D_fake: 0.043 \n",
            "(epoch: 109, iters: 2300, time: 0.193) G_GAN: 1.472 G_GAN_Feat: 3.834 G_VGG: 1.066 D_real: 0.136 D_fake: 0.208 \n",
            "(epoch: 109, iters: 2400, time: 0.193) G_GAN: 2.040 G_GAN_Feat: 3.206 G_VGG: 1.147 D_real: 0.239 D_fake: 0.032 \n",
            "saving the latest model (epoch 109, total_steps 456000)\n",
            "(epoch: 109, iters: 2500, time: 0.193) G_GAN: 1.104 G_GAN_Feat: 2.666 G_VGG: 1.211 D_real: 0.025 D_fake: 0.787 \n",
            "(epoch: 109, iters: 2600, time: 0.193) G_GAN: 1.906 G_GAN_Feat: 3.263 G_VGG: 1.420 D_real: 0.221 D_fake: 0.029 \n",
            "(epoch: 109, iters: 2700, time: 0.193) G_GAN: 1.843 G_GAN_Feat: 2.428 G_VGG: 1.119 D_real: 0.206 D_fake: 0.116 \n",
            "(epoch: 109, iters: 2800, time: 0.193) G_GAN: 2.286 G_GAN_Feat: 3.020 G_VGG: 1.294 D_real: 0.172 D_fake: 0.034 \n",
            "(epoch: 109, iters: 2900, time: 0.193) G_GAN: 1.672 G_GAN_Feat: 3.232 G_VGG: 1.154 D_real: 0.069 D_fake: 0.069 \n",
            "(epoch: 109, iters: 3000, time: 0.193) G_GAN: 1.353 G_GAN_Feat: 2.710 G_VGG: 1.143 D_real: 0.053 D_fake: 0.182 \n",
            "(epoch: 109, iters: 3100, time: 0.193) G_GAN: 1.003 G_GAN_Feat: 3.049 G_VGG: 1.225 D_real: 0.098 D_fake: 0.324 \n",
            "(epoch: 109, iters: 3200, time: 0.193) G_GAN: 1.667 G_GAN_Feat: 2.276 G_VGG: 1.049 D_real: 0.342 D_fake: 0.043 \n",
            "(epoch: 109, iters: 3300, time: 0.193) G_GAN: 1.495 G_GAN_Feat: 2.561 G_VGG: 1.033 D_real: 0.076 D_fake: 0.189 \n",
            "(epoch: 109, iters: 3400, time: 0.193) G_GAN: 1.802 G_GAN_Feat: 4.244 G_VGG: 1.498 D_real: 0.154 D_fake: 0.032 \n",
            "saving the latest model (epoch 109, total_steps 457000)\n",
            "(epoch: 109, iters: 3500, time: 0.193) G_GAN: 2.218 G_GAN_Feat: 4.148 G_VGG: 1.279 D_real: 0.109 D_fake: 0.033 \n",
            "(epoch: 109, iters: 3600, time: 0.193) G_GAN: 2.036 G_GAN_Feat: 3.530 G_VGG: 1.216 D_real: 0.017 D_fake: 0.018 \n",
            "(epoch: 109, iters: 3700, time: 0.193) G_GAN: 1.865 G_GAN_Feat: 2.726 G_VGG: 1.050 D_real: 0.129 D_fake: 0.022 \n",
            "(epoch: 109, iters: 3800, time: 0.193) G_GAN: 2.050 G_GAN_Feat: 4.156 G_VGG: 1.283 D_real: 0.166 D_fake: 0.026 \n",
            "(epoch: 109, iters: 3900, time: 0.193) G_GAN: 1.620 G_GAN_Feat: 3.610 G_VGG: 0.977 D_real: 0.217 D_fake: 0.052 \n",
            "(epoch: 109, iters: 4000, time: 0.193) G_GAN: 1.820 G_GAN_Feat: 3.093 G_VGG: 1.189 D_real: 0.498 D_fake: 0.022 \n",
            "(epoch: 109, iters: 4100, time: 0.193) G_GAN: 1.327 G_GAN_Feat: 3.019 G_VGG: 1.294 D_real: 0.235 D_fake: 0.142 \n",
            "(epoch: 109, iters: 4200, time: 0.193) G_GAN: 1.228 G_GAN_Feat: 2.801 G_VGG: 1.392 D_real: 0.092 D_fake: 0.170 \n",
            "End of epoch 109 / 200 \t Time Taken: 874 sec\n",
            "(epoch: 110, iters: 100, time: 0.193) G_GAN: 1.725 G_GAN_Feat: 2.491 G_VGG: 0.981 D_real: 0.175 D_fake: 0.079 \n",
            "(epoch: 110, iters: 200, time: 0.193) G_GAN: 1.901 G_GAN_Feat: 3.235 G_VGG: 0.845 D_real: 0.146 D_fake: 0.021 \n",
            "saving the latest model (epoch 110, total_steps 458000)\n",
            "(epoch: 110, iters: 300, time: 0.194) G_GAN: 0.223 G_GAN_Feat: 1.855 G_VGG: 0.852 D_real: 0.110 D_fake: 1.138 \n",
            "(epoch: 110, iters: 400, time: 0.193) G_GAN: 1.377 G_GAN_Feat: 2.973 G_VGG: 1.319 D_real: 0.200 D_fake: 0.132 \n",
            "(epoch: 110, iters: 500, time: 0.193) G_GAN: 2.190 G_GAN_Feat: 3.960 G_VGG: 1.176 D_real: 0.127 D_fake: 0.022 \n",
            "(epoch: 110, iters: 600, time: 0.193) G_GAN: 1.979 G_GAN_Feat: 3.636 G_VGG: 1.211 D_real: 0.096 D_fake: 0.013 \n",
            "(epoch: 110, iters: 700, time: 0.193) G_GAN: 1.326 G_GAN_Feat: 2.444 G_VGG: 1.027 D_real: 0.377 D_fake: 0.206 \n",
            "(epoch: 110, iters: 800, time: 0.193) G_GAN: 0.942 G_GAN_Feat: 2.227 G_VGG: 0.877 D_real: 0.029 D_fake: 0.694 \n",
            "(epoch: 110, iters: 900, time: 0.193) G_GAN: 2.188 G_GAN_Feat: 2.854 G_VGG: 0.919 D_real: 0.066 D_fake: 0.018 \n",
            "(epoch: 110, iters: 1000, time: 0.193) G_GAN: 2.118 G_GAN_Feat: 2.873 G_VGG: 0.961 D_real: 0.173 D_fake: 0.020 \n",
            "(epoch: 110, iters: 1100, time: 0.193) G_GAN: 1.312 G_GAN_Feat: 3.209 G_VGG: 1.207 D_real: 0.042 D_fake: 0.199 \n",
            "(epoch: 110, iters: 1200, time: 0.193) G_GAN: 1.469 G_GAN_Feat: 3.407 G_VGG: 1.104 D_real: 0.046 D_fake: 0.176 \n",
            "saving the latest model (epoch 110, total_steps 459000)\n",
            "(epoch: 110, iters: 1300, time: 0.193) G_GAN: 1.730 G_GAN_Feat: 3.009 G_VGG: 0.955 D_real: 0.158 D_fake: 0.073 \n",
            "(epoch: 110, iters: 1400, time: 0.193) G_GAN: 1.842 G_GAN_Feat: 5.440 G_VGG: 1.412 D_real: 0.018 D_fake: 0.026 \n",
            "(epoch: 110, iters: 1500, time: 0.193) G_GAN: 2.006 G_GAN_Feat: 2.996 G_VGG: 0.878 D_real: 0.108 D_fake: 0.012 \n",
            "(epoch: 110, iters: 1600, time: 0.193) G_GAN: 0.607 G_GAN_Feat: 2.076 G_VGG: 1.060 D_real: 0.174 D_fake: 1.014 \n",
            "(epoch: 110, iters: 1700, time: 0.193) G_GAN: 1.470 G_GAN_Feat: 3.274 G_VGG: 1.282 D_real: 0.092 D_fake: 0.091 \n",
            "(epoch: 110, iters: 1800, time: 0.193) G_GAN: 1.774 G_GAN_Feat: 3.326 G_VGG: 1.345 D_real: 0.141 D_fake: 0.111 \n",
            "(epoch: 110, iters: 1900, time: 0.193) G_GAN: 1.802 G_GAN_Feat: 3.120 G_VGG: 1.106 D_real: 0.241 D_fake: 0.029 \n",
            "(epoch: 110, iters: 2000, time: 0.193) G_GAN: 1.630 G_GAN_Feat: 3.497 G_VGG: 1.264 D_real: 0.044 D_fake: 0.084 \n",
            "(epoch: 110, iters: 2100, time: 0.193) G_GAN: 1.583 G_GAN_Feat: 3.259 G_VGG: 1.076 D_real: 0.025 D_fake: 0.173 \n",
            "(epoch: 110, iters: 2200, time: 0.193) G_GAN: 1.544 G_GAN_Feat: 2.991 G_VGG: 1.080 D_real: 0.467 D_fake: 0.083 \n",
            "saving the latest model (epoch 110, total_steps 460000)\n",
            "(epoch: 110, iters: 2300, time: 0.193) G_GAN: 1.230 G_GAN_Feat: 2.652 G_VGG: 1.145 D_real: 0.052 D_fake: 0.219 \n",
            "(epoch: 110, iters: 2400, time: 0.193) G_GAN: 1.510 G_GAN_Feat: 4.590 G_VGG: 1.204 D_real: 0.090 D_fake: 0.195 \n",
            "(epoch: 110, iters: 2500, time: 0.193) G_GAN: 1.403 G_GAN_Feat: 2.576 G_VGG: 0.925 D_real: 0.051 D_fake: 0.272 \n",
            "(epoch: 110, iters: 2600, time: 0.193) G_GAN: 1.741 G_GAN_Feat: 4.192 G_VGG: 1.478 D_real: 0.075 D_fake: 0.028 \n",
            "(epoch: 110, iters: 2700, time: 0.193) G_GAN: 1.925 G_GAN_Feat: 5.206 G_VGG: 1.399 D_real: 0.030 D_fake: 0.014 \n",
            "(epoch: 110, iters: 2800, time: 0.193) G_GAN: 2.103 G_GAN_Feat: 3.607 G_VGG: 1.134 D_real: 0.033 D_fake: 0.025 \n",
            "(epoch: 110, iters: 2900, time: 0.193) G_GAN: 1.899 G_GAN_Feat: 3.542 G_VGG: 1.147 D_real: 0.262 D_fake: 0.210 \n",
            "(epoch: 110, iters: 3000, time: 0.193) G_GAN: 0.853 G_GAN_Feat: 2.154 G_VGG: 1.023 D_real: 0.069 D_fake: 0.562 \n",
            "(epoch: 110, iters: 3100, time: 0.193) G_GAN: 1.680 G_GAN_Feat: 4.464 G_VGG: 1.209 D_real: 0.370 D_fake: 0.052 \n",
            "(epoch: 110, iters: 3200, time: 0.193) G_GAN: 2.211 G_GAN_Feat: 4.066 G_VGG: 1.046 D_real: 0.070 D_fake: 0.056 \n",
            "saving the latest model (epoch 110, total_steps 461000)\n",
            "(epoch: 110, iters: 3300, time: 0.193) G_GAN: 2.499 G_GAN_Feat: 4.601 G_VGG: 1.220 D_real: 0.121 D_fake: 0.047 \n",
            "(epoch: 110, iters: 3400, time: 0.193) G_GAN: 2.538 G_GAN_Feat: 3.267 G_VGG: 1.150 D_real: 0.197 D_fake: 0.054 \n",
            "(epoch: 110, iters: 3500, time: 0.193) G_GAN: 1.720 G_GAN_Feat: 3.072 G_VGG: 1.499 D_real: 0.034 D_fake: 0.113 \n",
            "(epoch: 110, iters: 3600, time: 0.193) G_GAN: 1.588 G_GAN_Feat: 2.894 G_VGG: 1.027 D_real: 0.066 D_fake: 0.139 \n",
            "(epoch: 110, iters: 3700, time: 0.193) G_GAN: 0.886 G_GAN_Feat: 2.259 G_VGG: 0.983 D_real: 0.021 D_fake: 0.520 \n",
            "(epoch: 110, iters: 3800, time: 0.193) G_GAN: 1.733 G_GAN_Feat: 4.263 G_VGG: 1.146 D_real: 0.573 D_fake: 0.163 \n",
            "(epoch: 110, iters: 3900, time: 0.193) G_GAN: 2.141 G_GAN_Feat: 4.098 G_VGG: 1.329 D_real: 0.160 D_fake: 0.020 \n",
            "(epoch: 110, iters: 4000, time: 0.193) G_GAN: 1.757 G_GAN_Feat: 3.572 G_VGG: 1.044 D_real: 0.174 D_fake: 0.048 \n",
            "(epoch: 110, iters: 4100, time: 0.193) G_GAN: 1.267 G_GAN_Feat: 2.915 G_VGG: 1.034 D_real: 0.145 D_fake: 0.232 \n",
            "(epoch: 110, iters: 4200, time: 0.193) G_GAN: 1.716 G_GAN_Feat: 3.331 G_VGG: 1.173 D_real: 0.208 D_fake: 0.091 \n",
            "saving the latest model (epoch 110, total_steps 462000)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 441, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 668, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/36: file write failed\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/train.py\", line 117, in <module>\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py\", line 274, in save\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/base_model.py\", line 45, in save_network\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 440, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 291, in __exit__\n",
            "    self.file_like.write_end_of_file()\n",
            "RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 1964315264 vs 1964315152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2Pix test\n",
        "test the model with image with different level of fog"
      ],
      "metadata": {
        "id": "qEuQqQPxGISq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m4nxHHLAtxf"
      },
      "outputs": [],
      "source": [
        "#se non è stato eseguito il download\n",
        "if os.getcwd() != \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD\":\n",
        "  os.chdir(\"pix2pixHD\")\n",
        "\n",
        "#!./scripts/test_1024p.sh\n",
        "!python test.py --dataroot ./datasets/indoor --name nebbia --resize_or_crop none --how_many 5 --label_nc 0 --no_instance --which_epoch 100 --results_dir ./datasets/indoor/synth\n",
        "!python test.py --dataroot ./datasets/outdoor --name nebbia --resize_or_crop none --how_many 5 --label_nc 0 --no_instance --which_epoch 100 --results_dir ./datasets/outdoor/synth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_indoor_outdoor(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder)"
      ],
      "metadata": {
        "id": "o-Uvp5SC9u4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train 3 GANS for each fog level"
      ],
      "metadata": {
        "id": "iJGXERGZCjVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pix2pixHD/"
      ],
      "metadata": {
        "id": "AQJ7ea0CCvN4",
        "outputId": "fec51e50-5d7f-4015-e96f-74015af458cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia_low --dataroot ./datasets/nebbia_low --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "zhRBwxm3Cxxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia_medium --dataroot ./datasets/nebbia_medium --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "KHhH8FILC0Od",
        "outputId": "81a4384a-b0e2-44d9-f26e-4a792cf547ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia_medium\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_medium\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:06<00:00, 83.3MB/s]\n",
            "create web directory ./checkpoints/nebbia_medium/web...\n",
            "(epoch: 1, iters: 100, time: 1.291) G_GAN: 1.763 G_GAN_Feat: 13.530 G_VGG: 6.301 D_real: 1.023 D_fake: 1.065 \n",
            "(epoch: 1, iters: 200, time: 0.761) G_GAN: 1.672 G_GAN_Feat: 10.032 G_VGG: 5.571 D_real: 0.855 D_fake: 0.851 \n",
            "(epoch: 1, iters: 300, time: 0.284) G_GAN: 0.988 G_GAN_Feat: 11.256 G_VGG: 5.624 D_real: 0.615 D_fake: 0.526 \n",
            "(epoch: 1, iters: 400, time: 0.279) G_GAN: 1.022 G_GAN_Feat: 8.650 G_VGG: 5.126 D_real: 0.630 D_fake: 0.429 \n",
            "(epoch: 1, iters: 500, time: 0.271) G_GAN: 0.971 G_GAN_Feat: 8.355 G_VGG: 5.674 D_real: 0.550 D_fake: 0.490 \n",
            "(epoch: 1, iters: 600, time: 0.273) G_GAN: 1.056 G_GAN_Feat: 7.493 G_VGG: 4.830 D_real: 0.596 D_fake: 0.349 \n",
            "(epoch: 1, iters: 700, time: 0.275) G_GAN: 1.069 G_GAN_Feat: 8.574 G_VGG: 5.990 D_real: 0.440 D_fake: 0.366 \n",
            "(epoch: 1, iters: 800, time: 0.291) G_GAN: 0.761 G_GAN_Feat: 9.679 G_VGG: 6.944 D_real: 0.282 D_fake: 0.539 \n",
            "(epoch: 1, iters: 900, time: 0.272) G_GAN: 1.003 G_GAN_Feat: 8.999 G_VGG: 5.255 D_real: 0.270 D_fake: 0.408 \n",
            "(epoch: 1, iters: 1000, time: 0.269) G_GAN: 1.185 G_GAN_Feat: 7.186 G_VGG: 5.258 D_real: 0.391 D_fake: 0.386 \n",
            "saving the latest model (epoch 1, total_steps 1000)\n",
            "(epoch: 1, iters: 1100, time: 0.270) G_GAN: 1.284 G_GAN_Feat: 7.968 G_VGG: 4.472 D_real: 0.469 D_fake: 0.241 \n",
            "(epoch: 1, iters: 1200, time: 0.285) G_GAN: 1.581 G_GAN_Feat: 7.644 G_VGG: 7.065 D_real: 0.247 D_fake: 0.158 \n",
            "(epoch: 1, iters: 1300, time: 0.281) G_GAN: 1.189 G_GAN_Feat: 7.732 G_VGG: 4.863 D_real: 0.403 D_fake: 0.451 \n",
            "(epoch: 1, iters: 1400, time: 0.285) G_GAN: 1.291 G_GAN_Feat: 7.975 G_VGG: 4.006 D_real: 0.303 D_fake: 0.203 \n",
            "(epoch: 1, iters: 1500, time: 0.279) G_GAN: 1.955 G_GAN_Feat: 7.830 G_VGG: 6.231 D_real: 0.563 D_fake: 0.169 \n",
            "(epoch: 1, iters: 1600, time: 0.266) G_GAN: 0.982 G_GAN_Feat: 7.742 G_VGG: 5.607 D_real: 0.225 D_fake: 0.384 \n",
            "(epoch: 1, iters: 1700, time: 0.272) G_GAN: 2.112 G_GAN_Feat: 8.638 G_VGG: 5.225 D_real: 0.244 D_fake: 0.103 \n",
            "(epoch: 1, iters: 1800, time: 0.287) G_GAN: 1.444 G_GAN_Feat: 10.033 G_VGG: 6.103 D_real: 0.313 D_fake: 0.323 \n",
            "(epoch: 1, iters: 1900, time: 0.363) G_GAN: 1.540 G_GAN_Feat: 7.437 G_VGG: 5.544 D_real: 0.337 D_fake: 0.140 \n",
            "(epoch: 1, iters: 2000, time: 0.284) G_GAN: 1.674 G_GAN_Feat: 6.979 G_VGG: 6.010 D_real: 0.545 D_fake: 0.115 \n",
            "saving the latest model (epoch 1, total_steps 2000)\n",
            "(epoch: 1, iters: 2100, time: 0.278) G_GAN: 1.653 G_GAN_Feat: 8.068 G_VGG: 6.499 D_real: 0.281 D_fake: 0.240 \n",
            "(epoch: 1, iters: 2200, time: 0.283) G_GAN: 1.016 G_GAN_Feat: 6.181 G_VGG: 6.394 D_real: 0.685 D_fake: 0.586 \n",
            "(epoch: 1, iters: 2300, time: 0.266) G_GAN: 0.712 G_GAN_Feat: 5.401 G_VGG: 5.079 D_real: 0.444 D_fake: 0.438 \n",
            "(epoch: 1, iters: 2400, time: 0.284) G_GAN: 1.008 G_GAN_Feat: 4.663 G_VGG: 4.430 D_real: 0.414 D_fake: 0.343 \n",
            "(epoch: 1, iters: 2500, time: 0.288) G_GAN: 1.600 G_GAN_Feat: 7.384 G_VGG: 4.689 D_real: 0.515 D_fake: 0.169 \n",
            "(epoch: 1, iters: 2600, time: 0.290) G_GAN: 1.444 G_GAN_Feat: 6.017 G_VGG: 5.463 D_real: 0.532 D_fake: 0.268 \n",
            "(epoch: 1, iters: 2700, time: 0.283) G_GAN: 1.164 G_GAN_Feat: 5.969 G_VGG: 4.665 D_real: 0.673 D_fake: 0.253 \n",
            "(epoch: 1, iters: 2800, time: 0.287) G_GAN: 1.456 G_GAN_Feat: 6.303 G_VGG: 5.170 D_real: 0.291 D_fake: 0.158 \n",
            "(epoch: 1, iters: 2900, time: 0.292) G_GAN: 2.436 G_GAN_Feat: 5.789 G_VGG: 5.737 D_real: 1.334 D_fake: 0.597 \n",
            "(epoch: 1, iters: 3000, time: 0.312) G_GAN: 1.372 G_GAN_Feat: 5.327 G_VGG: 4.765 D_real: 0.616 D_fake: 0.184 \n",
            "saving the latest model (epoch 1, total_steps 3000)\n",
            "(epoch: 1, iters: 3100, time: 0.261) G_GAN: 1.843 G_GAN_Feat: 5.884 G_VGG: 5.419 D_real: 0.535 D_fake: 0.120 \n",
            "(epoch: 1, iters: 3200, time: 0.282) G_GAN: 0.975 G_GAN_Feat: 7.360 G_VGG: 6.165 D_real: 0.202 D_fake: 0.284 \n",
            "(epoch: 1, iters: 3300, time: 0.293) G_GAN: 0.912 G_GAN_Feat: 5.008 G_VGG: 3.311 D_real: 0.534 D_fake: 0.441 \n",
            "(epoch: 1, iters: 3400, time: 0.282) G_GAN: 1.225 G_GAN_Feat: 5.878 G_VGG: 6.053 D_real: 0.153 D_fake: 0.235 \n",
            "(epoch: 1, iters: 3500, time: 0.281) G_GAN: 1.878 G_GAN_Feat: 6.938 G_VGG: 6.898 D_real: 0.336 D_fake: 0.330 \n",
            "(epoch: 1, iters: 3600, time: 0.311) G_GAN: 1.166 G_GAN_Feat: 6.355 G_VGG: 4.420 D_real: 0.245 D_fake: 0.422 \n",
            "(epoch: 1, iters: 3700, time: 0.303) G_GAN: 1.253 G_GAN_Feat: 6.032 G_VGG: 5.420 D_real: 0.216 D_fake: 0.331 \n",
            "(epoch: 1, iters: 3800, time: 0.287) G_GAN: 1.217 G_GAN_Feat: 6.188 G_VGG: 3.947 D_real: 0.301 D_fake: 0.188 \n",
            "(epoch: 1, iters: 3900, time: 0.295) G_GAN: 0.773 G_GAN_Feat: 5.418 G_VGG: 4.501 D_real: 0.221 D_fake: 0.377 \n",
            "(epoch: 1, iters: 4000, time: 0.503) G_GAN: 1.088 G_GAN_Feat: 4.937 G_VGG: 5.002 D_real: 0.350 D_fake: 0.226 \n",
            "saving the latest model (epoch 1, total_steps 4000)\n",
            "(epoch: 1, iters: 4100, time: 0.269) G_GAN: 1.694 G_GAN_Feat: 6.722 G_VGG: 4.696 D_real: 0.462 D_fake: 0.157 \n",
            "(epoch: 1, iters: 4200, time: 0.294) G_GAN: 1.351 G_GAN_Feat: 6.099 G_VGG: 4.577 D_real: 0.241 D_fake: 0.181 \n",
            "End of epoch 1 / 200 \t Time Taken: 1405 sec\n",
            "(epoch: 2, iters: 100, time: 0.157) G_GAN: 1.099 G_GAN_Feat: 4.959 G_VGG: 4.312 D_real: 0.113 D_fake: 0.335 \n",
            "(epoch: 2, iters: 200, time: 0.157) G_GAN: 1.890 G_GAN_Feat: 5.857 G_VGG: 4.225 D_real: 0.726 D_fake: 0.091 \n",
            "(epoch: 2, iters: 300, time: 0.157) G_GAN: 1.925 G_GAN_Feat: 5.029 G_VGG: 4.199 D_real: 0.445 D_fake: 0.069 \n",
            "(epoch: 2, iters: 400, time: 0.157) G_GAN: 1.515 G_GAN_Feat: 4.900 G_VGG: 4.093 D_real: 0.253 D_fake: 0.150 \n",
            "(epoch: 2, iters: 500, time: 0.157) G_GAN: 0.989 G_GAN_Feat: 5.149 G_VGG: 4.783 D_real: 0.066 D_fake: 0.462 \n",
            "(epoch: 2, iters: 600, time: 0.157) G_GAN: 1.211 G_GAN_Feat: 5.518 G_VGG: 4.037 D_real: 0.380 D_fake: 0.231 \n",
            "(epoch: 2, iters: 700, time: 0.157) G_GAN: 1.002 G_GAN_Feat: 5.044 G_VGG: 4.170 D_real: 0.158 D_fake: 0.341 \n",
            "(epoch: 2, iters: 800, time: 0.157) G_GAN: 1.601 G_GAN_Feat: 5.323 G_VGG: 3.422 D_real: 0.284 D_fake: 0.156 \n",
            "saving the latest model (epoch 2, total_steps 5000)\n",
            "(epoch: 2, iters: 900, time: 0.157) G_GAN: 2.087 G_GAN_Feat: 4.464 G_VGG: 3.739 D_real: 0.737 D_fake: 0.053 \n",
            "(epoch: 2, iters: 1000, time: 0.157) G_GAN: 1.105 G_GAN_Feat: 4.231 G_VGG: 3.512 D_real: 0.161 D_fake: 0.294 \n",
            "(epoch: 2, iters: 1100, time: 0.157) G_GAN: 1.557 G_GAN_Feat: 4.974 G_VGG: 3.935 D_real: 0.231 D_fake: 0.273 \n",
            "(epoch: 2, iters: 1200, time: 0.157) G_GAN: 1.241 G_GAN_Feat: 6.212 G_VGG: 4.041 D_real: 0.238 D_fake: 0.219 \n",
            "(epoch: 2, iters: 1300, time: 0.157) G_GAN: 1.416 G_GAN_Feat: 6.751 G_VGG: 4.588 D_real: 0.257 D_fake: 0.142 \n",
            "(epoch: 2, iters: 1400, time: 0.157) G_GAN: 1.986 G_GAN_Feat: 5.193 G_VGG: 3.442 D_real: 0.246 D_fake: 0.057 \n",
            "(epoch: 2, iters: 1500, time: 0.157) G_GAN: 1.184 G_GAN_Feat: 4.885 G_VGG: 3.759 D_real: 0.253 D_fake: 0.525 \n",
            "(epoch: 2, iters: 1600, time: 0.157) G_GAN: 1.380 G_GAN_Feat: 5.044 G_VGG: 4.144 D_real: 0.274 D_fake: 0.117 \n",
            "(epoch: 2, iters: 1700, time: 0.157) G_GAN: 1.530 G_GAN_Feat: 7.473 G_VGG: 3.780 D_real: 0.271 D_fake: 0.370 \n",
            "(epoch: 2, iters: 1800, time: 0.157) G_GAN: 0.959 G_GAN_Feat: 6.001 G_VGG: 4.136 D_real: 0.114 D_fake: 0.373 \n",
            "saving the latest model (epoch 2, total_steps 6000)\n",
            "(epoch: 2, iters: 1900, time: 0.157) G_GAN: 1.130 G_GAN_Feat: 4.237 G_VGG: 3.707 D_real: 0.353 D_fake: 0.246 \n",
            "(epoch: 2, iters: 2000, time: 0.157) G_GAN: 1.644 G_GAN_Feat: 4.858 G_VGG: 4.189 D_real: 0.301 D_fake: 0.164 \n",
            "(epoch: 2, iters: 2100, time: 0.157) G_GAN: 1.090 G_GAN_Feat: 4.216 G_VGG: 3.494 D_real: 0.163 D_fake: 0.333 \n",
            "(epoch: 2, iters: 2200, time: 0.157) G_GAN: 1.755 G_GAN_Feat: 5.332 G_VGG: 3.776 D_real: 0.226 D_fake: 0.132 \n",
            "(epoch: 2, iters: 2300, time: 0.157) G_GAN: 1.343 G_GAN_Feat: 4.956 G_VGG: 4.454 D_real: 0.145 D_fake: 0.152 \n",
            "(epoch: 2, iters: 2400, time: 0.157) G_GAN: 1.185 G_GAN_Feat: 4.446 G_VGG: 2.948 D_real: 0.536 D_fake: 0.532 \n",
            "(epoch: 2, iters: 2500, time: 0.157) G_GAN: 1.636 G_GAN_Feat: 4.949 G_VGG: 3.595 D_real: 0.386 D_fake: 0.164 \n",
            "(epoch: 2, iters: 2600, time: 0.157) G_GAN: 1.465 G_GAN_Feat: 5.141 G_VGG: 4.168 D_real: 0.514 D_fake: 0.109 \n",
            "(epoch: 2, iters: 2700, time: 0.157) G_GAN: 0.966 G_GAN_Feat: 4.568 G_VGG: 4.168 D_real: 0.105 D_fake: 0.436 \n",
            "(epoch: 2, iters: 2800, time: 0.157) G_GAN: 1.141 G_GAN_Feat: 4.952 G_VGG: 3.555 D_real: 0.117 D_fake: 0.439 \n",
            "saving the latest model (epoch 2, total_steps 7000)\n",
            "(epoch: 2, iters: 2900, time: 0.157) G_GAN: 1.446 G_GAN_Feat: 6.094 G_VGG: 3.779 D_real: 0.070 D_fake: 0.106 \n",
            "(epoch: 2, iters: 3000, time: 0.157) G_GAN: 1.166 G_GAN_Feat: 5.107 G_VGG: 4.248 D_real: 0.045 D_fake: 0.457 \n",
            "(epoch: 2, iters: 3100, time: 0.157) G_GAN: 1.254 G_GAN_Feat: 4.398 G_VGG: 3.619 D_real: 0.091 D_fake: 0.282 \n",
            "(epoch: 2, iters: 3200, time: 0.157) G_GAN: 1.973 G_GAN_Feat: 3.857 G_VGG: 3.879 D_real: 0.640 D_fake: 0.067 \n",
            "(epoch: 2, iters: 3300, time: 0.157) G_GAN: 1.319 G_GAN_Feat: 4.605 G_VGG: 2.919 D_real: 0.073 D_fake: 0.124 \n",
            "(epoch: 2, iters: 3400, time: 0.157) G_GAN: 0.476 G_GAN_Feat: 5.139 G_VGG: 4.055 D_real: 0.117 D_fake: 0.753 \n",
            "(epoch: 2, iters: 3500, time: 0.157) G_GAN: 1.510 G_GAN_Feat: 5.482 G_VGG: 3.220 D_real: 0.102 D_fake: 0.267 \n",
            "(epoch: 2, iters: 3600, time: 0.157) G_GAN: 1.663 G_GAN_Feat: 5.193 G_VGG: 3.257 D_real: 0.193 D_fake: 0.200 \n",
            "(epoch: 2, iters: 3700, time: 0.157) G_GAN: 1.791 G_GAN_Feat: 3.895 G_VGG: 3.462 D_real: 0.224 D_fake: 0.123 \n",
            "(epoch: 2, iters: 3800, time: 0.157) G_GAN: 1.911 G_GAN_Feat: 4.686 G_VGG: 2.846 D_real: 0.413 D_fake: 0.065 \n",
            "saving the latest model (epoch 2, total_steps 8000)\n",
            "(epoch: 2, iters: 3900, time: 0.157) G_GAN: 1.421 G_GAN_Feat: 4.037 G_VGG: 3.910 D_real: 0.168 D_fake: 0.193 \n",
            "(epoch: 2, iters: 4000, time: 0.157) G_GAN: 1.246 G_GAN_Feat: 3.902 G_VGG: 2.888 D_real: 0.474 D_fake: 0.155 \n",
            "(epoch: 2, iters: 4100, time: 0.157) G_GAN: 1.382 G_GAN_Feat: 3.719 G_VGG: 3.586 D_real: 0.153 D_fake: 0.258 \n",
            "(epoch: 2, iters: 4200, time: 0.157) G_GAN: 1.333 G_GAN_Feat: 5.131 G_VGG: 3.400 D_real: 0.123 D_fake: 0.231 \n",
            "End of epoch 2 / 200 \t Time Taken: 678 sec\n",
            "(epoch: 3, iters: 100, time: 0.157) G_GAN: 1.023 G_GAN_Feat: 3.834 G_VGG: 2.770 D_real: 0.210 D_fake: 0.262 \n",
            "(epoch: 3, iters: 200, time: 0.157) G_GAN: 1.762 G_GAN_Feat: 4.320 G_VGG: 3.505 D_real: 0.172 D_fake: 0.112 \n",
            "(epoch: 3, iters: 300, time: 0.157) G_GAN: 1.605 G_GAN_Feat: 4.248 G_VGG: 3.380 D_real: 0.324 D_fake: 0.194 \n",
            "(epoch: 3, iters: 400, time: 0.157) G_GAN: 1.143 G_GAN_Feat: 4.837 G_VGG: 3.455 D_real: 0.247 D_fake: 0.317 \n",
            "(epoch: 3, iters: 500, time: 0.157) G_GAN: 0.700 G_GAN_Feat: 3.820 G_VGG: 2.484 D_real: 0.139 D_fake: 0.552 \n",
            "(epoch: 3, iters: 600, time: 0.157) G_GAN: 0.811 G_GAN_Feat: 4.227 G_VGG: 3.698 D_real: 0.077 D_fake: 0.505 \n",
            "saving the latest model (epoch 3, total_steps 9000)\n",
            "(epoch: 3, iters: 700, time: 0.157) G_GAN: 1.611 G_GAN_Feat: 4.938 G_VGG: 3.374 D_real: 0.212 D_fake: 0.064 \n",
            "(epoch: 3, iters: 800, time: 0.157) G_GAN: 1.937 G_GAN_Feat: 4.605 G_VGG: 4.178 D_real: 0.220 D_fake: 0.091 \n",
            "(epoch: 3, iters: 900, time: 0.157) G_GAN: 1.378 G_GAN_Feat: 4.312 G_VGG: 3.974 D_real: 0.071 D_fake: 0.222 \n",
            "(epoch: 3, iters: 1000, time: 0.157) G_GAN: 1.340 G_GAN_Feat: 4.073 G_VGG: 2.962 D_real: 0.090 D_fake: 0.258 \n",
            "(epoch: 3, iters: 1100, time: 0.157) G_GAN: 1.558 G_GAN_Feat: 4.909 G_VGG: 2.745 D_real: 0.129 D_fake: 0.152 \n",
            "(epoch: 3, iters: 1200, time: 0.157) G_GAN: 1.695 G_GAN_Feat: 3.797 G_VGG: 2.305 D_real: 0.090 D_fake: 0.055 \n",
            "(epoch: 3, iters: 1300, time: 0.157) G_GAN: 0.836 G_GAN_Feat: 3.971 G_VGG: 2.754 D_real: 0.273 D_fake: 0.694 \n",
            "(epoch: 3, iters: 1400, time: 0.157) G_GAN: 2.289 G_GAN_Feat: 4.504 G_VGG: 4.596 D_real: 0.701 D_fake: 0.122 \n",
            "(epoch: 3, iters: 1500, time: 0.157) G_GAN: 1.234 G_GAN_Feat: 4.253 G_VGG: 4.417 D_real: 0.139 D_fake: 0.350 \n",
            "(epoch: 3, iters: 1600, time: 0.157) G_GAN: 0.921 G_GAN_Feat: 4.260 G_VGG: 3.164 D_real: 0.097 D_fake: 0.417 \n",
            "saving the latest model (epoch 3, total_steps 10000)\n",
            "(epoch: 3, iters: 1700, time: 0.157) G_GAN: 1.941 G_GAN_Feat: 4.306 G_VGG: 2.887 D_real: 0.470 D_fake: 0.106 \n",
            "(epoch: 3, iters: 1800, time: 0.157) G_GAN: 1.669 G_GAN_Feat: 3.961 G_VGG: 2.719 D_real: 0.185 D_fake: 0.090 \n",
            "(epoch: 3, iters: 1900, time: 0.157) G_GAN: 1.892 G_GAN_Feat: 4.583 G_VGG: 2.990 D_real: 0.545 D_fake: 0.080 \n",
            "(epoch: 3, iters: 2000, time: 0.157) G_GAN: 1.656 G_GAN_Feat: 4.077 G_VGG: 2.910 D_real: 0.727 D_fake: 0.066 \n",
            "(epoch: 3, iters: 2100, time: 0.157) G_GAN: 2.076 G_GAN_Feat: 5.243 G_VGG: 3.316 D_real: 0.287 D_fake: 0.033 \n",
            "(epoch: 3, iters: 2200, time: 0.158) G_GAN: 1.897 G_GAN_Feat: 4.641 G_VGG: 3.786 D_real: 0.119 D_fake: 0.063 \n",
            "(epoch: 3, iters: 2300, time: 0.157) G_GAN: 1.432 G_GAN_Feat: 4.980 G_VGG: 2.854 D_real: 0.092 D_fake: 0.136 \n",
            "(epoch: 3, iters: 2400, time: 0.157) G_GAN: 1.771 G_GAN_Feat: 5.240 G_VGG: 3.945 D_real: 0.166 D_fake: 0.061 \n",
            "(epoch: 3, iters: 2500, time: 0.157) G_GAN: 1.516 G_GAN_Feat: 4.490 G_VGG: 3.064 D_real: 0.126 D_fake: 0.094 \n",
            "(epoch: 3, iters: 2600, time: 0.158) G_GAN: 1.573 G_GAN_Feat: 4.932 G_VGG: 2.947 D_real: 0.123 D_fake: 0.366 \n",
            "saving the latest model (epoch 3, total_steps 11000)\n",
            "(epoch: 3, iters: 2700, time: 0.157) G_GAN: 2.126 G_GAN_Feat: 4.367 G_VGG: 2.820 D_real: 0.346 D_fake: 0.056 \n",
            "(epoch: 3, iters: 2800, time: 0.157) G_GAN: 2.019 G_GAN_Feat: 5.578 G_VGG: 3.668 D_real: 0.355 D_fake: 0.033 \n",
            "(epoch: 3, iters: 2900, time: 0.157) G_GAN: 1.805 G_GAN_Feat: 4.227 G_VGG: 3.043 D_real: 0.108 D_fake: 0.087 \n",
            "(epoch: 3, iters: 3000, time: 0.157) G_GAN: 1.719 G_GAN_Feat: 5.242 G_VGG: 2.983 D_real: 0.078 D_fake: 0.187 \n",
            "(epoch: 3, iters: 3100, time: 0.157) G_GAN: 1.315 G_GAN_Feat: 4.169 G_VGG: 2.747 D_real: 0.218 D_fake: 0.243 \n",
            "(epoch: 3, iters: 3200, time: 0.157) G_GAN: 2.010 G_GAN_Feat: 4.487 G_VGG: 3.191 D_real: 0.271 D_fake: 0.091 \n",
            "(epoch: 3, iters: 3300, time: 0.157) G_GAN: 1.246 G_GAN_Feat: 4.677 G_VGG: 2.557 D_real: 0.021 D_fake: 0.218 \n",
            "(epoch: 3, iters: 3400, time: 0.157) G_GAN: 1.669 G_GAN_Feat: 4.639 G_VGG: 2.494 D_real: 0.483 D_fake: 0.315 \n",
            "(epoch: 3, iters: 3500, time: 0.157) G_GAN: 1.031 G_GAN_Feat: 4.208 G_VGG: 2.807 D_real: 0.074 D_fake: 0.309 \n",
            "(epoch: 3, iters: 3600, time: 0.157) G_GAN: 0.816 G_GAN_Feat: 3.928 G_VGG: 3.005 D_real: 0.069 D_fake: 0.411 \n",
            "saving the latest model (epoch 3, total_steps 12000)\n",
            "(epoch: 3, iters: 3700, time: 0.157) G_GAN: 1.234 G_GAN_Feat: 4.626 G_VGG: 3.156 D_real: 0.031 D_fake: 0.319 \n",
            "(epoch: 3, iters: 3800, time: 0.157) G_GAN: 1.569 G_GAN_Feat: 4.544 G_VGG: 2.604 D_real: 0.097 D_fake: 0.194 \n",
            "(epoch: 3, iters: 3900, time: 0.157) G_GAN: 1.458 G_GAN_Feat: 3.640 G_VGG: 3.279 D_real: 0.458 D_fake: 0.176 \n",
            "(epoch: 3, iters: 4000, time: 0.158) G_GAN: 1.402 G_GAN_Feat: 4.096 G_VGG: 2.588 D_real: 0.139 D_fake: 0.154 \n",
            "(epoch: 3, iters: 4100, time: 0.157) G_GAN: 1.648 G_GAN_Feat: 4.433 G_VGG: 2.121 D_real: 0.104 D_fake: 0.239 \n",
            "(epoch: 3, iters: 4200, time: 0.157) G_GAN: 2.065 G_GAN_Feat: 3.751 G_VGG: 2.865 D_real: 0.876 D_fake: 0.135 \n",
            "End of epoch 3 / 200 \t Time Taken: 677 sec\n",
            "(epoch: 4, iters: 100, time: 0.157) G_GAN: 1.451 G_GAN_Feat: 5.202 G_VGG: 3.397 D_real: 0.049 D_fake: 0.110 \n",
            "(epoch: 4, iters: 200, time: 0.157) G_GAN: 2.020 G_GAN_Feat: 4.353 G_VGG: 2.765 D_real: 0.081 D_fake: 0.016 \n",
            "(epoch: 4, iters: 300, time: 0.157) G_GAN: 1.416 G_GAN_Feat: 5.158 G_VGG: 3.400 D_real: 0.158 D_fake: 0.209 \n",
            "(epoch: 4, iters: 400, time: 0.157) G_GAN: 2.164 G_GAN_Feat: 4.551 G_VGG: 2.480 D_real: 0.254 D_fake: 0.024 \n",
            "saving the latest model (epoch 4, total_steps 13000)\n",
            "(epoch: 4, iters: 500, time: 0.157) G_GAN: 1.548 G_GAN_Feat: 4.424 G_VGG: 2.931 D_real: 0.172 D_fake: 0.206 \n",
            "(epoch: 4, iters: 600, time: 0.157) G_GAN: 1.922 G_GAN_Feat: 4.358 G_VGG: 3.804 D_real: 0.764 D_fake: 0.068 \n",
            "(epoch: 4, iters: 700, time: 0.157) G_GAN: 1.956 G_GAN_Feat: 4.531 G_VGG: 2.704 D_real: 0.567 D_fake: 0.025 \n",
            "(epoch: 4, iters: 800, time: 0.157) G_GAN: 1.473 G_GAN_Feat: 3.817 G_VGG: 2.586 D_real: 0.108 D_fake: 0.167 \n",
            "(epoch: 4, iters: 900, time: 0.157) G_GAN: 2.030 G_GAN_Feat: 4.306 G_VGG: 2.372 D_real: 0.060 D_fake: 0.069 \n",
            "(epoch: 4, iters: 1000, time: 0.157) G_GAN: 1.695 G_GAN_Feat: 4.789 G_VGG: 3.186 D_real: 0.034 D_fake: 0.251 \n",
            "(epoch: 4, iters: 1100, time: 0.157) G_GAN: 1.527 G_GAN_Feat: 4.176 G_VGG: 3.417 D_real: 0.137 D_fake: 0.164 \n",
            "(epoch: 4, iters: 1200, time: 0.157) G_GAN: 1.699 G_GAN_Feat: 4.350 G_VGG: 2.580 D_real: 0.031 D_fake: 0.094 \n",
            "(epoch: 4, iters: 1300, time: 0.157) G_GAN: 1.157 G_GAN_Feat: 3.548 G_VGG: 2.942 D_real: 0.133 D_fake: 0.543 \n",
            "(epoch: 4, iters: 1400, time: 0.157) G_GAN: 1.505 G_GAN_Feat: 3.712 G_VGG: 2.924 D_real: 0.102 D_fake: 0.100 \n",
            "saving the latest model (epoch 4, total_steps 14000)\n",
            "(epoch: 4, iters: 1500, time: 0.157) G_GAN: 1.636 G_GAN_Feat: 3.918 G_VGG: 3.253 D_real: 0.087 D_fake: 0.097 \n",
            "(epoch: 4, iters: 1600, time: 0.157) G_GAN: 1.370 G_GAN_Feat: 3.502 G_VGG: 2.275 D_real: 0.059 D_fake: 0.302 \n",
            "(epoch: 4, iters: 1700, time: 0.157) G_GAN: 1.575 G_GAN_Feat: 4.588 G_VGG: 2.863 D_real: 0.082 D_fake: 0.092 \n",
            "(epoch: 4, iters: 1800, time: 0.157) G_GAN: 1.925 G_GAN_Feat: 4.151 G_VGG: 3.180 D_real: 0.064 D_fake: 0.026 \n",
            "(epoch: 4, iters: 1900, time: 0.157) G_GAN: 2.265 G_GAN_Feat: 4.824 G_VGG: 2.850 D_real: 0.153 D_fake: 0.049 \n",
            "(epoch: 4, iters: 2000, time: 0.157) G_GAN: 1.465 G_GAN_Feat: 4.115 G_VGG: 2.563 D_real: 0.037 D_fake: 0.311 \n",
            "(epoch: 4, iters: 2100, time: 0.157) G_GAN: 1.531 G_GAN_Feat: 4.265 G_VGG: 2.871 D_real: 0.079 D_fake: 0.335 \n",
            "(epoch: 4, iters: 2200, time: 0.157) G_GAN: 1.953 G_GAN_Feat: 4.433 G_VGG: 2.916 D_real: 0.023 D_fake: 0.022 \n",
            "(epoch: 4, iters: 2300, time: 0.157) G_GAN: 1.831 G_GAN_Feat: 5.047 G_VGG: 2.534 D_real: 0.032 D_fake: 0.210 \n",
            "(epoch: 4, iters: 2400, time: 0.157) G_GAN: 1.627 G_GAN_Feat: 4.726 G_VGG: 2.377 D_real: 0.127 D_fake: 0.149 \n",
            "saving the latest model (epoch 4, total_steps 15000)\n",
            "(epoch: 4, iters: 2500, time: 0.157) G_GAN: 1.887 G_GAN_Feat: 4.203 G_VGG: 2.380 D_real: 0.195 D_fake: 0.031 \n",
            "(epoch: 4, iters: 2600, time: 0.157) G_GAN: 1.142 G_GAN_Feat: 4.661 G_VGG: 3.682 D_real: 0.063 D_fake: 0.187 \n",
            "(epoch: 4, iters: 2700, time: 0.157) G_GAN: 1.252 G_GAN_Feat: 4.404 G_VGG: 3.189 D_real: 0.070 D_fake: 0.194 \n",
            "(epoch: 4, iters: 2800, time: 0.157) G_GAN: 1.565 G_GAN_Feat: 4.516 G_VGG: 3.491 D_real: 0.024 D_fake: 0.136 \n",
            "(epoch: 4, iters: 2900, time: 0.157) G_GAN: 1.955 G_GAN_Feat: 4.859 G_VGG: 2.577 D_real: 0.030 D_fake: 0.037 \n",
            "(epoch: 4, iters: 3000, time: 0.158) G_GAN: 1.484 G_GAN_Feat: 2.992 G_VGG: 2.684 D_real: 0.379 D_fake: 0.339 \n",
            "(epoch: 4, iters: 3100, time: 0.157) G_GAN: 0.725 G_GAN_Feat: 3.405 G_VGG: 2.631 D_real: 0.171 D_fake: 0.556 \n",
            "(epoch: 4, iters: 3200, time: 0.158) G_GAN: 1.480 G_GAN_Feat: 4.235 G_VGG: 2.444 D_real: 0.164 D_fake: 0.138 \n",
            "(epoch: 4, iters: 3300, time: 0.157) G_GAN: 1.855 G_GAN_Feat: 4.391 G_VGG: 2.854 D_real: 0.022 D_fake: 0.044 \n",
            "(epoch: 4, iters: 3400, time: 0.157) G_GAN: 2.003 G_GAN_Feat: 4.422 G_VGG: 2.588 D_real: 0.152 D_fake: 0.022 \n",
            "saving the latest model (epoch 4, total_steps 16000)\n",
            "(epoch: 4, iters: 3500, time: 0.157) G_GAN: 1.881 G_GAN_Feat: 4.616 G_VGG: 2.521 D_real: 0.024 D_fake: 0.037 \n",
            "(epoch: 4, iters: 3600, time: 0.157) G_GAN: 1.436 G_GAN_Feat: 4.550 G_VGG: 2.524 D_real: 0.068 D_fake: 0.206 \n",
            "(epoch: 4, iters: 3700, time: 0.157) G_GAN: 1.499 G_GAN_Feat: 5.192 G_VGG: 3.626 D_real: 0.030 D_fake: 0.151 \n",
            "(epoch: 4, iters: 3800, time: 0.157) G_GAN: 1.796 G_GAN_Feat: 5.341 G_VGG: 3.130 D_real: 0.037 D_fake: 0.034 \n",
            "(epoch: 4, iters: 3900, time: 0.157) G_GAN: 1.707 G_GAN_Feat: 4.244 G_VGG: 2.520 D_real: 0.027 D_fake: 0.064 \n",
            "(epoch: 4, iters: 4000, time: 0.158) G_GAN: 1.595 G_GAN_Feat: 4.388 G_VGG: 3.402 D_real: 0.130 D_fake: 0.114 \n",
            "(epoch: 4, iters: 4100, time: 0.157) G_GAN: 1.894 G_GAN_Feat: 4.254 G_VGG: 2.885 D_real: 0.330 D_fake: 0.030 \n",
            "(epoch: 4, iters: 4200, time: 0.157) G_GAN: 1.621 G_GAN_Feat: 4.152 G_VGG: 3.009 D_real: 0.031 D_fake: 0.079 \n",
            "End of epoch 4 / 200 \t Time Taken: 676 sec\n",
            "(epoch: 5, iters: 100, time: 0.157) G_GAN: 1.544 G_GAN_Feat: 3.847 G_VGG: 3.265 D_real: 0.067 D_fake: 0.126 \n",
            "(epoch: 5, iters: 200, time: 0.157) G_GAN: 2.270 G_GAN_Feat: 3.967 G_VGG: 3.226 D_real: 0.167 D_fake: 0.034 \n",
            "saving the latest model (epoch 5, total_steps 17000)\n",
            "(epoch: 5, iters: 300, time: 0.157) G_GAN: 2.177 G_GAN_Feat: 4.620 G_VGG: 2.400 D_real: 0.036 D_fake: 0.033 \n",
            "(epoch: 5, iters: 400, time: 0.157) G_GAN: 1.896 G_GAN_Feat: 4.473 G_VGG: 2.681 D_real: 0.027 D_fake: 0.018 \n",
            "(epoch: 5, iters: 500, time: 0.157) G_GAN: 1.979 G_GAN_Feat: 4.714 G_VGG: 2.637 D_real: 0.027 D_fake: 0.020 \n",
            "(epoch: 5, iters: 600, time: 0.157) G_GAN: 0.998 G_GAN_Feat: 4.354 G_VGG: 2.898 D_real: 0.065 D_fake: 0.451 \n",
            "(epoch: 5, iters: 700, time: 0.157) G_GAN: 2.224 G_GAN_Feat: 4.588 G_VGG: 2.578 D_real: 0.149 D_fake: 0.039 \n",
            "(epoch: 5, iters: 800, time: 0.157) G_GAN: 2.162 G_GAN_Feat: 4.211 G_VGG: 2.279 D_real: 0.136 D_fake: 0.021 \n",
            "(epoch: 5, iters: 900, time: 0.157) G_GAN: 1.586 G_GAN_Feat: 4.242 G_VGG: 2.685 D_real: 0.041 D_fake: 0.056 \n",
            "(epoch: 5, iters: 1000, time: 0.157) G_GAN: 1.037 G_GAN_Feat: 3.159 G_VGG: 1.947 D_real: 0.306 D_fake: 0.334 \n",
            "(epoch: 5, iters: 1100, time: 0.157) G_GAN: 0.590 G_GAN_Feat: 4.419 G_VGG: 2.773 D_real: 0.184 D_fake: 1.145 \n",
            "(epoch: 5, iters: 1200, time: 0.157) G_GAN: 1.882 G_GAN_Feat: 3.973 G_VGG: 3.057 D_real: 0.284 D_fake: 0.124 \n",
            "saving the latest model (epoch 5, total_steps 18000)\n",
            "(epoch: 5, iters: 1300, time: 0.157) G_GAN: 1.798 G_GAN_Feat: 3.815 G_VGG: 2.164 D_real: 0.240 D_fake: 0.093 \n",
            "(epoch: 5, iters: 1400, time: 0.157) G_GAN: 1.211 G_GAN_Feat: 3.532 G_VGG: 2.652 D_real: 0.445 D_fake: 0.160 \n",
            "(epoch: 5, iters: 1500, time: 0.157) G_GAN: 1.322 G_GAN_Feat: 2.823 G_VGG: 2.546 D_real: 0.298 D_fake: 0.184 \n",
            "(epoch: 5, iters: 1600, time: 0.157) G_GAN: 1.212 G_GAN_Feat: 3.342 G_VGG: 2.518 D_real: 0.070 D_fake: 0.177 \n",
            "(epoch: 5, iters: 1700, time: 0.158) G_GAN: 1.354 G_GAN_Feat: 3.723 G_VGG: 2.457 D_real: 0.050 D_fake: 0.253 \n",
            "(epoch: 5, iters: 1800, time: 0.157) G_GAN: 1.828 G_GAN_Feat: 3.944 G_VGG: 2.708 D_real: 0.417 D_fake: 0.054 \n",
            "(epoch: 5, iters: 1900, time: 0.157) G_GAN: 1.764 G_GAN_Feat: 4.281 G_VGG: 3.143 D_real: 0.207 D_fake: 0.043 \n",
            "(epoch: 5, iters: 2000, time: 0.157) G_GAN: 1.413 G_GAN_Feat: 3.259 G_VGG: 2.259 D_real: 0.033 D_fake: 0.136 \n",
            "(epoch: 5, iters: 2100, time: 0.157) G_GAN: 2.032 G_GAN_Feat: 3.482 G_VGG: 2.072 D_real: 0.276 D_fake: 0.051 \n",
            "(epoch: 5, iters: 2200, time: 0.157) G_GAN: 1.065 G_GAN_Feat: 3.534 G_VGG: 2.212 D_real: 0.224 D_fake: 0.383 \n",
            "saving the latest model (epoch 5, total_steps 19000)\n",
            "(epoch: 5, iters: 2300, time: 0.157) G_GAN: 1.172 G_GAN_Feat: 4.417 G_VGG: 2.381 D_real: 0.040 D_fake: 0.458 \n",
            "(epoch: 5, iters: 2400, time: 0.157) G_GAN: 1.436 G_GAN_Feat: 4.439 G_VGG: 2.437 D_real: 0.043 D_fake: 0.447 \n",
            "(epoch: 5, iters: 2500, time: 0.157) G_GAN: 1.718 G_GAN_Feat: 3.967 G_VGG: 2.589 D_real: 0.233 D_fake: 0.095 \n",
            "(epoch: 5, iters: 2600, time: 0.157) G_GAN: 1.977 G_GAN_Feat: 5.029 G_VGG: 2.698 D_real: 0.114 D_fake: 0.015 \n",
            "(epoch: 5, iters: 2700, time: 0.157) G_GAN: 2.008 G_GAN_Feat: 4.460 G_VGG: 2.319 D_real: 0.119 D_fake: 0.016 \n",
            "(epoch: 5, iters: 2800, time: 0.157) G_GAN: 1.792 G_GAN_Feat: 4.030 G_VGG: 2.622 D_real: 0.046 D_fake: 0.058 \n",
            "(epoch: 5, iters: 2900, time: 0.158) G_GAN: 1.295 G_GAN_Feat: 3.070 G_VGG: 2.226 D_real: 0.380 D_fake: 0.348 \n",
            "(epoch: 5, iters: 3000, time: 0.157) G_GAN: 1.121 G_GAN_Feat: 3.159 G_VGG: 2.451 D_real: 0.242 D_fake: 0.247 \n",
            "(epoch: 5, iters: 3100, time: 0.157) G_GAN: 1.341 G_GAN_Feat: 3.475 G_VGG: 2.244 D_real: 0.071 D_fake: 0.212 \n",
            "(epoch: 5, iters: 3200, time: 0.157) G_GAN: 2.143 G_GAN_Feat: 5.335 G_VGG: 2.750 D_real: 0.077 D_fake: 0.038 \n",
            "saving the latest model (epoch 5, total_steps 20000)\n",
            "(epoch: 5, iters: 3300, time: 0.157) G_GAN: 1.945 G_GAN_Feat: 4.284 G_VGG: 2.662 D_real: 0.056 D_fake: 0.017 \n",
            "(epoch: 5, iters: 3400, time: 0.157) G_GAN: 1.746 G_GAN_Feat: 4.022 G_VGG: 2.696 D_real: 0.105 D_fake: 0.036 \n",
            "(epoch: 5, iters: 3500, time: 0.157) G_GAN: 1.876 G_GAN_Feat: 4.275 G_VGG: 3.155 D_real: 0.088 D_fake: 0.017 \n",
            "(epoch: 5, iters: 3600, time: 0.157) G_GAN: 2.314 G_GAN_Feat: 2.858 G_VGG: 1.726 D_real: 0.907 D_fake: 0.421 \n",
            "(epoch: 5, iters: 3700, time: 0.157) G_GAN: 1.328 G_GAN_Feat: 4.324 G_VGG: 2.488 D_real: 0.352 D_fake: 0.281 \n",
            "(epoch: 5, iters: 3800, time: 0.158) G_GAN: 1.055 G_GAN_Feat: 4.060 G_VGG: 2.654 D_real: 0.284 D_fake: 0.347 \n",
            "(epoch: 5, iters: 3900, time: 0.157) G_GAN: 0.739 G_GAN_Feat: 3.155 G_VGG: 2.738 D_real: 0.543 D_fake: 0.403 \n",
            "(epoch: 5, iters: 4000, time: 0.157) G_GAN: 0.924 G_GAN_Feat: 3.094 G_VGG: 2.948 D_real: 0.400 D_fake: 0.355 \n",
            "(epoch: 5, iters: 4100, time: 0.157) G_GAN: 1.018 G_GAN_Feat: 2.978 G_VGG: 2.549 D_real: 0.295 D_fake: 0.287 \n",
            "(epoch: 5, iters: 4200, time: 0.157) G_GAN: 0.936 G_GAN_Feat: 3.379 G_VGG: 2.328 D_real: 0.341 D_fake: 0.370 \n",
            "saving the latest model (epoch 5, total_steps 21000)\n",
            "End of epoch 5 / 200 \t Time Taken: 679 sec\n",
            "(epoch: 6, iters: 100, time: 0.157) G_GAN: 0.994 G_GAN_Feat: 3.645 G_VGG: 2.980 D_real: 0.320 D_fake: 0.309 \n",
            "(epoch: 6, iters: 200, time: 0.157) G_GAN: 1.340 G_GAN_Feat: 3.031 G_VGG: 2.140 D_real: 0.433 D_fake: 0.259 \n",
            "(epoch: 6, iters: 300, time: 0.157) G_GAN: 0.696 G_GAN_Feat: 3.560 G_VGG: 2.888 D_real: 0.288 D_fake: 0.437 \n",
            "(epoch: 6, iters: 400, time: 0.157) G_GAN: 1.131 G_GAN_Feat: 3.170 G_VGG: 2.619 D_real: 0.409 D_fake: 0.325 \n",
            "(epoch: 6, iters: 500, time: 0.157) G_GAN: 1.016 G_GAN_Feat: 3.202 G_VGG: 2.030 D_real: 0.314 D_fake: 0.309 \n",
            "(epoch: 6, iters: 600, time: 0.157) G_GAN: 1.370 G_GAN_Feat: 4.202 G_VGG: 2.714 D_real: 0.326 D_fake: 0.248 \n",
            "(epoch: 6, iters: 700, time: 0.157) G_GAN: 1.234 G_GAN_Feat: 3.803 G_VGG: 2.491 D_real: 0.312 D_fake: 0.227 \n",
            "(epoch: 6, iters: 800, time: 0.157) G_GAN: 1.121 G_GAN_Feat: 3.874 G_VGG: 2.603 D_real: 0.294 D_fake: 0.266 \n",
            "(epoch: 6, iters: 900, time: 0.157) G_GAN: 1.360 G_GAN_Feat: 5.102 G_VGG: 2.761 D_real: 0.330 D_fake: 0.273 \n",
            "(epoch: 6, iters: 1000, time: 0.157) G_GAN: 1.376 G_GAN_Feat: 3.236 G_VGG: 2.839 D_real: 0.402 D_fake: 0.281 \n",
            "saving the latest model (epoch 6, total_steps 22000)\n",
            "(epoch: 6, iters: 1100, time: 0.157) G_GAN: 1.301 G_GAN_Feat: 3.448 G_VGG: 2.140 D_real: 0.442 D_fake: 0.301 \n",
            "(epoch: 6, iters: 1200, time: 0.157) G_GAN: 1.155 G_GAN_Feat: 4.447 G_VGG: 2.435 D_real: 0.250 D_fake: 0.290 \n",
            "(epoch: 6, iters: 1300, time: 0.157) G_GAN: 1.125 G_GAN_Feat: 3.433 G_VGG: 2.837 D_real: 0.279 D_fake: 0.269 \n",
            "(epoch: 6, iters: 1400, time: 0.157) G_GAN: 1.644 G_GAN_Feat: 3.850 G_VGG: 3.243 D_real: 0.513 D_fake: 0.294 \n",
            "(epoch: 6, iters: 1500, time: 0.157) G_GAN: 0.898 G_GAN_Feat: 3.721 G_VGG: 2.354 D_real: 0.283 D_fake: 0.378 \n",
            "(epoch: 6, iters: 1600, time: 0.157) G_GAN: 0.716 G_GAN_Feat: 3.335 G_VGG: 2.398 D_real: 0.328 D_fake: 0.531 \n",
            "(epoch: 6, iters: 1700, time: 0.157) G_GAN: 1.000 G_GAN_Feat: 2.549 G_VGG: 2.228 D_real: 0.783 D_fake: 0.286 \n",
            "(epoch: 6, iters: 1800, time: 0.157) G_GAN: 1.153 G_GAN_Feat: 4.105 G_VGG: 2.689 D_real: 0.261 D_fake: 0.451 \n",
            "(epoch: 6, iters: 1900, time: 0.158) G_GAN: 0.735 G_GAN_Feat: 3.227 G_VGG: 2.907 D_real: 0.230 D_fake: 0.378 \n",
            "(epoch: 6, iters: 2000, time: 0.157) G_GAN: 1.128 G_GAN_Feat: 3.600 G_VGG: 2.473 D_real: 0.218 D_fake: 0.267 \n",
            "saving the latest model (epoch 6, total_steps 23000)\n",
            "(epoch: 6, iters: 2100, time: 0.157) G_GAN: 1.271 G_GAN_Feat: 3.617 G_VGG: 2.404 D_real: 0.265 D_fake: 0.247 \n",
            "(epoch: 6, iters: 2200, time: 0.157) G_GAN: 0.983 G_GAN_Feat: 2.925 G_VGG: 2.394 D_real: 0.377 D_fake: 0.331 \n",
            "(epoch: 6, iters: 2300, time: 0.157) G_GAN: 0.959 G_GAN_Feat: 3.121 G_VGG: 2.641 D_real: 0.259 D_fake: 0.460 \n",
            "(epoch: 6, iters: 2400, time: 0.157) G_GAN: 1.361 G_GAN_Feat: 4.377 G_VGG: 2.571 D_real: 0.229 D_fake: 0.264 \n",
            "(epoch: 6, iters: 2500, time: 0.157) G_GAN: 1.151 G_GAN_Feat: 4.242 G_VGG: 2.718 D_real: 0.267 D_fake: 0.259 \n",
            "(epoch: 6, iters: 2600, time: 0.157) G_GAN: 0.479 G_GAN_Feat: 3.093 G_VGG: 2.143 D_real: 0.398 D_fake: 0.665 \n",
            "(epoch: 6, iters: 2700, time: 0.157) G_GAN: 1.272 G_GAN_Feat: 3.894 G_VGG: 2.615 D_real: 0.221 D_fake: 0.237 \n",
            "(epoch: 6, iters: 2800, time: 0.157) G_GAN: 0.912 G_GAN_Feat: 3.642 G_VGG: 2.956 D_real: 0.240 D_fake: 0.417 \n",
            "(epoch: 6, iters: 2900, time: 0.157) G_GAN: 0.844 G_GAN_Feat: 3.085 G_VGG: 2.018 D_real: 0.369 D_fake: 0.337 \n",
            "(epoch: 6, iters: 3000, time: 0.158) G_GAN: 1.023 G_GAN_Feat: 3.662 G_VGG: 2.371 D_real: 0.142 D_fake: 0.355 \n",
            "saving the latest model (epoch 6, total_steps 24000)\n",
            "(epoch: 6, iters: 3100, time: 0.157) G_GAN: 1.377 G_GAN_Feat: 4.019 G_VGG: 2.058 D_real: 0.349 D_fake: 0.232 \n",
            "(epoch: 6, iters: 3200, time: 0.157) G_GAN: 1.472 G_GAN_Feat: 3.418 G_VGG: 2.317 D_real: 0.357 D_fake: 0.326 \n",
            "(epoch: 6, iters: 3300, time: 0.157) G_GAN: 1.222 G_GAN_Feat: 3.882 G_VGG: 1.909 D_real: 0.223 D_fake: 0.316 \n",
            "(epoch: 6, iters: 3400, time: 0.157) G_GAN: 1.099 G_GAN_Feat: 3.923 G_VGG: 3.197 D_real: 0.244 D_fake: 0.225 \n",
            "(epoch: 6, iters: 3500, time: 0.157) G_GAN: 1.312 G_GAN_Feat: 3.564 G_VGG: 3.462 D_real: 0.595 D_fake: 0.326 \n",
            "(epoch: 6, iters: 3600, time: 0.157) G_GAN: 1.289 G_GAN_Feat: 3.410 G_VGG: 2.627 D_real: 0.244 D_fake: 0.181 \n",
            "(epoch: 6, iters: 3700, time: 0.158) G_GAN: 0.982 G_GAN_Feat: 3.545 G_VGG: 2.558 D_real: 0.223 D_fake: 0.336 \n",
            "(epoch: 6, iters: 3800, time: 0.157) G_GAN: 0.571 G_GAN_Feat: 3.091 G_VGG: 2.814 D_real: 0.307 D_fake: 0.549 \n",
            "(epoch: 6, iters: 3900, time: 0.157) G_GAN: 0.949 G_GAN_Feat: 3.339 G_VGG: 2.514 D_real: 0.325 D_fake: 0.281 \n",
            "(epoch: 6, iters: 4000, time: 0.157) G_GAN: 1.143 G_GAN_Feat: 3.698 G_VGG: 2.768 D_real: 0.251 D_fake: 0.321 \n",
            "saving the latest model (epoch 6, total_steps 25000)\n",
            "(epoch: 6, iters: 4100, time: 0.157) G_GAN: 0.794 G_GAN_Feat: 2.375 G_VGG: 2.516 D_real: 0.402 D_fake: 0.453 \n",
            "(epoch: 6, iters: 4200, time: 0.157) G_GAN: 1.506 G_GAN_Feat: 4.053 G_VGG: 2.323 D_real: 0.409 D_fake: 0.206 \n",
            "End of epoch 6 / 200 \t Time Taken: 675 sec\n",
            "(epoch: 7, iters: 100, time: 0.157) G_GAN: 0.900 G_GAN_Feat: 3.299 G_VGG: 1.899 D_real: 0.113 D_fake: 0.571 \n",
            "(epoch: 7, iters: 200, time: 0.157) G_GAN: 0.451 G_GAN_Feat: 2.494 G_VGG: 2.415 D_real: 0.283 D_fake: 0.714 \n",
            "(epoch: 7, iters: 300, time: 0.157) G_GAN: 0.773 G_GAN_Feat: 3.268 G_VGG: 2.401 D_real: 0.195 D_fake: 0.381 \n",
            "(epoch: 7, iters: 400, time: 0.157) G_GAN: 1.381 G_GAN_Feat: 3.225 G_VGG: 2.349 D_real: 0.266 D_fake: 0.166 \n",
            "(epoch: 7, iters: 500, time: 0.157) G_GAN: 0.821 G_GAN_Feat: 3.341 G_VGG: 2.318 D_real: 0.213 D_fake: 0.481 \n",
            "(epoch: 7, iters: 600, time: 0.157) G_GAN: 1.107 G_GAN_Feat: 4.602 G_VGG: 3.089 D_real: 0.244 D_fake: 0.291 \n",
            "(epoch: 7, iters: 700, time: 0.157) G_GAN: 1.431 G_GAN_Feat: 3.805 G_VGG: 2.084 D_real: 0.208 D_fake: 0.193 \n",
            "(epoch: 7, iters: 800, time: 0.157) G_GAN: 1.435 G_GAN_Feat: 3.575 G_VGG: 2.478 D_real: 0.269 D_fake: 0.144 \n",
            "saving the latest model (epoch 7, total_steps 26000)\n",
            "(epoch: 7, iters: 900, time: 0.157) G_GAN: 1.291 G_GAN_Feat: 3.338 G_VGG: 2.292 D_real: 0.217 D_fake: 0.206 \n",
            "(epoch: 7, iters: 1000, time: 0.157) G_GAN: 1.140 G_GAN_Feat: 3.076 G_VGG: 2.337 D_real: 0.265 D_fake: 0.245 \n",
            "(epoch: 7, iters: 1100, time: 0.157) G_GAN: 1.148 G_GAN_Feat: 3.401 G_VGG: 2.414 D_real: 0.449 D_fake: 0.270 \n",
            "(epoch: 7, iters: 1200, time: 0.157) G_GAN: 1.336 G_GAN_Feat: 3.701 G_VGG: 2.881 D_real: 0.187 D_fake: 0.293 \n",
            "(epoch: 7, iters: 1300, time: 0.157) G_GAN: 0.476 G_GAN_Feat: 2.531 G_VGG: 2.526 D_real: 0.307 D_fake: 0.884 \n",
            "(epoch: 7, iters: 1400, time: 0.157) G_GAN: 1.490 G_GAN_Feat: 3.244 G_VGG: 1.959 D_real: 0.439 D_fake: 0.101 \n",
            "(epoch: 7, iters: 1500, time: 0.157) G_GAN: 1.568 G_GAN_Feat: 4.378 G_VGG: 2.053 D_real: 0.243 D_fake: 0.152 \n",
            "(epoch: 7, iters: 1600, time: 0.157) G_GAN: 1.598 G_GAN_Feat: 3.452 G_VGG: 2.522 D_real: 0.497 D_fake: 0.063 \n",
            "(epoch: 7, iters: 1700, time: 0.157) G_GAN: 1.515 G_GAN_Feat: 3.542 G_VGG: 2.487 D_real: 0.305 D_fake: 0.223 \n",
            "(epoch: 7, iters: 1800, time: 0.157) G_GAN: 1.175 G_GAN_Feat: 3.917 G_VGG: 2.553 D_real: 0.089 D_fake: 0.292 \n",
            "saving the latest model (epoch 7, total_steps 27000)\n",
            "(epoch: 7, iters: 1900, time: 0.157) G_GAN: 1.310 G_GAN_Feat: 2.652 G_VGG: 2.132 D_real: 0.601 D_fake: 0.130 \n",
            "(epoch: 7, iters: 2000, time: 0.157) G_GAN: 1.662 G_GAN_Feat: 3.946 G_VGG: 3.080 D_real: 0.213 D_fake: 0.122 \n",
            "(epoch: 7, iters: 2100, time: 0.157) G_GAN: 1.382 G_GAN_Feat: 3.531 G_VGG: 3.086 D_real: 0.170 D_fake: 0.416 \n",
            "(epoch: 7, iters: 2200, time: 0.157) G_GAN: 1.184 G_GAN_Feat: 3.171 G_VGG: 2.931 D_real: 0.042 D_fake: 0.210 \n",
            "(epoch: 7, iters: 2300, time: 0.157) G_GAN: 1.866 G_GAN_Feat: 4.314 G_VGG: 2.969 D_real: 0.131 D_fake: 0.034 \n",
            "(epoch: 7, iters: 2400, time: 0.157) G_GAN: 1.358 G_GAN_Feat: 3.879 G_VGG: 3.132 D_real: 0.048 D_fake: 0.105 \n",
            "(epoch: 7, iters: 2500, time: 0.157) G_GAN: 1.796 G_GAN_Feat: 4.842 G_VGG: 2.607 D_real: 0.297 D_fake: 0.073 \n",
            "(epoch: 7, iters: 2600, time: 0.157) G_GAN: 1.089 G_GAN_Feat: 3.742 G_VGG: 2.650 D_real: 0.322 D_fake: 0.214 \n",
            "(epoch: 7, iters: 2700, time: 0.157) G_GAN: 1.413 G_GAN_Feat: 5.155 G_VGG: 2.622 D_real: 0.068 D_fake: 0.113 \n",
            "(epoch: 7, iters: 2800, time: 0.157) G_GAN: 1.892 G_GAN_Feat: 4.145 G_VGG: 2.098 D_real: 0.065 D_fake: 0.059 \n",
            "saving the latest model (epoch 7, total_steps 28000)\n",
            "(epoch: 7, iters: 2900, time: 0.157) G_GAN: 1.905 G_GAN_Feat: 4.119 G_VGG: 2.871 D_real: 0.298 D_fake: 0.034 \n",
            "(epoch: 7, iters: 3000, time: 0.157) G_GAN: 1.474 G_GAN_Feat: 3.452 G_VGG: 2.491 D_real: 0.123 D_fake: 0.096 \n",
            "(epoch: 7, iters: 3100, time: 0.157) G_GAN: 2.058 G_GAN_Feat: 2.776 G_VGG: 2.125 D_real: 0.372 D_fake: 0.030 \n",
            "(epoch: 7, iters: 3200, time: 0.157) G_GAN: 1.575 G_GAN_Feat: 3.377 G_VGG: 2.240 D_real: 0.178 D_fake: 0.126 \n",
            "(epoch: 7, iters: 3300, time: 0.157) G_GAN: 1.910 G_GAN_Feat: 3.787 G_VGG: 3.291 D_real: 0.071 D_fake: 0.025 \n",
            "(epoch: 7, iters: 3400, time: 0.157) G_GAN: 1.682 G_GAN_Feat: 4.498 G_VGG: 2.709 D_real: 0.073 D_fake: 0.046 \n",
            "(epoch: 7, iters: 3500, time: 0.157) G_GAN: 1.717 G_GAN_Feat: 4.213 G_VGG: 2.585 D_real: 0.035 D_fake: 0.074 \n",
            "(epoch: 7, iters: 3600, time: 0.157) G_GAN: 1.446 G_GAN_Feat: 4.384 G_VGG: 2.945 D_real: 0.021 D_fake: 0.160 \n",
            "(epoch: 7, iters: 3700, time: 0.157) G_GAN: 1.218 G_GAN_Feat: 3.960 G_VGG: 2.533 D_real: 0.033 D_fake: 0.446 \n",
            "(epoch: 7, iters: 3800, time: 0.157) G_GAN: 2.065 G_GAN_Feat: 4.290 G_VGG: 2.030 D_real: 0.282 D_fake: 0.028 \n",
            "saving the latest model (epoch 7, total_steps 29000)\n",
            "(epoch: 7, iters: 3900, time: 0.157) G_GAN: 1.705 G_GAN_Feat: 3.972 G_VGG: 2.403 D_real: 0.048 D_fake: 0.048 \n",
            "(epoch: 7, iters: 4000, time: 0.157) G_GAN: 0.959 G_GAN_Feat: 3.129 G_VGG: 2.871 D_real: 0.150 D_fake: 0.475 \n",
            "(epoch: 7, iters: 4100, time: 0.157) G_GAN: 1.627 G_GAN_Feat: 3.686 G_VGG: 2.353 D_real: 0.096 D_fake: 0.126 \n",
            "(epoch: 7, iters: 4200, time: 0.157) G_GAN: 1.666 G_GAN_Feat: 4.487 G_VGG: 2.674 D_real: 0.015 D_fake: 0.048 \n",
            "End of epoch 7 / 200 \t Time Taken: 676 sec\n",
            "(epoch: 8, iters: 100, time: 0.157) G_GAN: 2.128 G_GAN_Feat: 3.658 G_VGG: 2.424 D_real: 0.264 D_fake: 0.111 \n",
            "(epoch: 8, iters: 200, time: 0.157) G_GAN: 1.680 G_GAN_Feat: 3.599 G_VGG: 2.847 D_real: 0.256 D_fake: 0.170 \n",
            "(epoch: 8, iters: 300, time: 0.157) G_GAN: 1.564 G_GAN_Feat: 4.393 G_VGG: 2.287 D_real: 0.034 D_fake: 0.078 \n",
            "(epoch: 8, iters: 400, time: 0.157) G_GAN: 1.490 G_GAN_Feat: 4.338 G_VGG: 2.639 D_real: 0.169 D_fake: 0.135 \n",
            "(epoch: 8, iters: 500, time: 0.157) G_GAN: 1.649 G_GAN_Feat: 3.306 G_VGG: 1.884 D_real: 0.106 D_fake: 0.084 \n",
            "(epoch: 8, iters: 600, time: 0.157) G_GAN: 1.374 G_GAN_Feat: 3.538 G_VGG: 2.324 D_real: 0.076 D_fake: 0.138 \n",
            "saving the latest model (epoch 8, total_steps 30000)\n",
            "(epoch: 8, iters: 700, time: 0.157) G_GAN: 0.943 G_GAN_Feat: 3.171 G_VGG: 2.097 D_real: 0.025 D_fake: 0.336 \n",
            "(epoch: 8, iters: 800, time: 0.157) G_GAN: 1.717 G_GAN_Feat: 3.996 G_VGG: 1.927 D_real: 0.010 D_fake: 0.105 \n",
            "(epoch: 8, iters: 900, time: 0.157) G_GAN: 1.724 G_GAN_Feat: 3.425 G_VGG: 1.779 D_real: 0.020 D_fake: 0.113 \n",
            "(epoch: 8, iters: 1000, time: 0.157) G_GAN: 1.698 G_GAN_Feat: 3.416 G_VGG: 2.537 D_real: 0.338 D_fake: 0.165 \n",
            "(epoch: 8, iters: 1100, time: 0.157) G_GAN: 1.416 G_GAN_Feat: 4.033 G_VGG: 2.918 D_real: 0.051 D_fake: 0.220 \n",
            "(epoch: 8, iters: 1200, time: 0.157) G_GAN: 1.879 G_GAN_Feat: 3.808 G_VGG: 2.377 D_real: 0.048 D_fake: 0.077 \n",
            "(epoch: 8, iters: 1300, time: 0.157) G_GAN: 1.554 G_GAN_Feat: 3.466 G_VGG: 2.069 D_real: 0.044 D_fake: 0.232 \n",
            "(epoch: 8, iters: 1400, time: 0.157) G_GAN: 1.727 G_GAN_Feat: 3.586 G_VGG: 2.173 D_real: 0.036 D_fake: 0.137 \n",
            "(epoch: 8, iters: 1500, time: 0.157) G_GAN: 1.883 G_GAN_Feat: 3.901 G_VGG: 2.396 D_real: 0.020 D_fake: 0.012 \n",
            "(epoch: 8, iters: 1600, time: 0.157) G_GAN: 1.215 G_GAN_Feat: 2.514 G_VGG: 2.034 D_real: 0.340 D_fake: 0.209 \n",
            "saving the latest model (epoch 8, total_steps 31000)\n",
            "(epoch: 8, iters: 1700, time: 0.157) G_GAN: 1.288 G_GAN_Feat: 2.311 G_VGG: 2.487 D_real: 0.459 D_fake: 0.142 \n",
            "(epoch: 8, iters: 1800, time: 0.157) G_GAN: 1.264 G_GAN_Feat: 3.721 G_VGG: 3.104 D_real: 0.022 D_fake: 0.262 \n",
            "(epoch: 8, iters: 1900, time: 0.157) G_GAN: 1.447 G_GAN_Feat: 3.785 G_VGG: 2.172 D_real: 0.015 D_fake: 0.085 \n",
            "(epoch: 8, iters: 2000, time: 0.157) G_GAN: 1.805 G_GAN_Feat: 4.027 G_VGG: 2.772 D_real: 0.205 D_fake: 0.024 \n",
            "(epoch: 8, iters: 2100, time: 0.157) G_GAN: 1.747 G_GAN_Feat: 4.763 G_VGG: 2.264 D_real: 0.026 D_fake: 0.219 \n",
            "(epoch: 8, iters: 2200, time: 0.157) G_GAN: 1.693 G_GAN_Feat: 4.844 G_VGG: 2.379 D_real: 0.128 D_fake: 0.068 \n",
            "(epoch: 8, iters: 2300, time: 0.157) G_GAN: 2.244 G_GAN_Feat: 4.009 G_VGG: 2.015 D_real: 0.081 D_fake: 0.020 \n",
            "(epoch: 8, iters: 2400, time: 0.157) G_GAN: 1.395 G_GAN_Feat: 3.937 G_VGG: 2.422 D_real: 0.080 D_fake: 0.443 \n",
            "(epoch: 8, iters: 2500, time: 0.157) G_GAN: 1.667 G_GAN_Feat: 3.988 G_VGG: 2.521 D_real: 0.046 D_fake: 0.044 \n",
            "(epoch: 8, iters: 2600, time: 0.158) G_GAN: 1.760 G_GAN_Feat: 4.020 G_VGG: 2.710 D_real: 0.120 D_fake: 0.087 \n",
            "saving the latest model (epoch 8, total_steps 32000)\n",
            "(epoch: 8, iters: 2700, time: 0.157) G_GAN: 2.052 G_GAN_Feat: 4.326 G_VGG: 2.177 D_real: 0.040 D_fake: 0.055 \n",
            "(epoch: 8, iters: 2800, time: 0.157) G_GAN: 1.513 G_GAN_Feat: 3.735 G_VGG: 2.148 D_real: 0.026 D_fake: 0.079 \n",
            "(epoch: 8, iters: 2900, time: 0.157) G_GAN: 2.188 G_GAN_Feat: 4.402 G_VGG: 2.403 D_real: 0.029 D_fake: 0.017 \n",
            "(epoch: 8, iters: 3000, time: 0.157) G_GAN: 2.022 G_GAN_Feat: 5.184 G_VGG: 2.604 D_real: 0.093 D_fake: 0.034 \n",
            "(epoch: 8, iters: 3100, time: 0.157) G_GAN: 1.192 G_GAN_Feat: 2.950 G_VGG: 2.443 D_real: 0.303 D_fake: 0.275 \n",
            "(epoch: 8, iters: 3200, time: 0.157) G_GAN: 1.073 G_GAN_Feat: 3.329 G_VGG: 1.885 D_real: 0.017 D_fake: 0.365 \n",
            "(epoch: 8, iters: 3300, time: 0.157) G_GAN: 1.298 G_GAN_Feat: 3.718 G_VGG: 2.004 D_real: 0.126 D_fake: 0.207 \n",
            "(epoch: 8, iters: 3400, time: 0.157) G_GAN: 1.916 G_GAN_Feat: 4.437 G_VGG: 2.037 D_real: 0.138 D_fake: 0.011 \n",
            "(epoch: 8, iters: 3500, time: 0.157) G_GAN: 1.559 G_GAN_Feat: 3.527 G_VGG: 2.467 D_real: 0.023 D_fake: 0.080 \n",
            "(epoch: 8, iters: 3600, time: 0.158) G_GAN: 1.127 G_GAN_Feat: 3.793 G_VGG: 2.793 D_real: 0.022 D_fake: 0.310 \n",
            "saving the latest model (epoch 8, total_steps 33000)\n",
            "(epoch: 8, iters: 3700, time: 0.157) G_GAN: 1.870 G_GAN_Feat: 3.781 G_VGG: 2.700 D_real: 0.048 D_fake: 0.153 \n",
            "(epoch: 8, iters: 3800, time: 0.157) G_GAN: 1.855 G_GAN_Feat: 4.190 G_VGG: 3.035 D_real: 0.031 D_fake: 0.015 \n",
            "(epoch: 8, iters: 3900, time: 0.157) G_GAN: 1.653 G_GAN_Feat: 4.398 G_VGG: 1.782 D_real: 0.665 D_fake: 0.041 \n",
            "(epoch: 8, iters: 4000, time: 0.157) G_GAN: 1.740 G_GAN_Feat: 4.172 G_VGG: 2.677 D_real: 0.113 D_fake: 0.066 \n",
            "(epoch: 8, iters: 4100, time: 0.157) G_GAN: 1.714 G_GAN_Feat: 4.539 G_VGG: 3.087 D_real: 0.018 D_fake: 0.273 \n",
            "(epoch: 8, iters: 4200, time: 0.157) G_GAN: 1.507 G_GAN_Feat: 3.595 G_VGG: 2.815 D_real: 0.022 D_fake: 0.163 \n",
            "End of epoch 8 / 200 \t Time Taken: 677 sec\n",
            "(epoch: 9, iters: 100, time: 0.157) G_GAN: 1.462 G_GAN_Feat: 3.560 G_VGG: 1.812 D_real: 0.404 D_fake: 0.257 \n",
            "(epoch: 9, iters: 200, time: 0.157) G_GAN: 1.924 G_GAN_Feat: 4.379 G_VGG: 2.657 D_real: 0.074 D_fake: 0.021 \n",
            "(epoch: 9, iters: 300, time: 0.157) G_GAN: 1.872 G_GAN_Feat: 4.168 G_VGG: 2.509 D_real: 0.021 D_fake: 0.019 \n",
            "(epoch: 9, iters: 400, time: 0.157) G_GAN: 2.366 G_GAN_Feat: 3.922 G_VGG: 1.917 D_real: 0.136 D_fake: 0.041 \n",
            "saving the latest model (epoch 9, total_steps 34000)\n",
            "(epoch: 9, iters: 500, time: 0.157) G_GAN: 1.684 G_GAN_Feat: 3.540 G_VGG: 2.772 D_real: 0.181 D_fake: 0.164 \n",
            "(epoch: 9, iters: 600, time: 0.157) G_GAN: 1.616 G_GAN_Feat: 3.270 G_VGG: 2.290 D_real: 0.021 D_fake: 0.112 \n",
            "(epoch: 9, iters: 700, time: 0.157) G_GAN: 2.254 G_GAN_Feat: 4.339 G_VGG: 2.188 D_real: 0.089 D_fake: 0.040 \n",
            "(epoch: 9, iters: 800, time: 0.157) G_GAN: 1.020 G_GAN_Feat: 3.346 G_VGG: 2.601 D_real: 0.112 D_fake: 0.267 \n",
            "(epoch: 9, iters: 900, time: 0.157) G_GAN: 1.643 G_GAN_Feat: 3.850 G_VGG: 2.426 D_real: 0.124 D_fake: 0.085 \n",
            "(epoch: 9, iters: 1000, time: 0.157) G_GAN: 1.750 G_GAN_Feat: 3.742 G_VGG: 2.244 D_real: 0.012 D_fake: 0.028 \n",
            "(epoch: 9, iters: 1100, time: 0.157) G_GAN: 2.226 G_GAN_Feat: 3.469 G_VGG: 2.207 D_real: 0.144 D_fake: 0.077 \n",
            "(epoch: 9, iters: 1200, time: 0.157) G_GAN: 1.773 G_GAN_Feat: 3.766 G_VGG: 1.661 D_real: 0.022 D_fake: 0.032 \n",
            "(epoch: 9, iters: 1300, time: 0.157) G_GAN: 1.472 G_GAN_Feat: 3.760 G_VGG: 2.374 D_real: 0.027 D_fake: 0.308 \n",
            "(epoch: 9, iters: 1400, time: 0.158) G_GAN: 1.754 G_GAN_Feat: 3.675 G_VGG: 1.930 D_real: 0.015 D_fake: 0.050 \n",
            "saving the latest model (epoch 9, total_steps 35000)\n",
            "(epoch: 9, iters: 1500, time: 0.157) G_GAN: 1.290 G_GAN_Feat: 3.712 G_VGG: 2.104 D_real: 0.032 D_fake: 0.482 \n",
            "(epoch: 9, iters: 1600, time: 0.157) G_GAN: 1.373 G_GAN_Feat: 3.662 G_VGG: 1.946 D_real: 0.017 D_fake: 0.187 \n",
            "(epoch: 9, iters: 1700, time: 0.157) G_GAN: 1.752 G_GAN_Feat: 3.322 G_VGG: 2.134 D_real: 0.566 D_fake: 0.149 \n",
            "(epoch: 9, iters: 1800, time: 0.157) G_GAN: 1.558 G_GAN_Feat: 4.317 G_VGG: 2.017 D_real: 0.041 D_fake: 0.191 \n",
            "(epoch: 9, iters: 1900, time: 0.157) G_GAN: 1.591 G_GAN_Feat: 3.586 G_VGG: 2.157 D_real: 0.238 D_fake: 0.107 \n",
            "(epoch: 9, iters: 2000, time: 0.157) G_GAN: 1.136 G_GAN_Feat: 2.777 G_VGG: 2.431 D_real: 0.216 D_fake: 0.290 \n",
            "(epoch: 9, iters: 2100, time: 0.157) G_GAN: 1.401 G_GAN_Feat: 3.613 G_VGG: 2.927 D_real: 0.027 D_fake: 0.228 \n",
            "(epoch: 9, iters: 2200, time: 0.157) G_GAN: 1.924 G_GAN_Feat: 3.311 G_VGG: 2.134 D_real: 0.424 D_fake: 0.056 \n",
            "(epoch: 9, iters: 2300, time: 0.157) G_GAN: 1.830 G_GAN_Feat: 3.885 G_VGG: 2.407 D_real: 0.392 D_fake: 0.027 \n",
            "(epoch: 9, iters: 2400, time: 0.157) G_GAN: 1.385 G_GAN_Feat: 3.677 G_VGG: 2.453 D_real: 0.030 D_fake: 0.167 \n",
            "saving the latest model (epoch 9, total_steps 36000)\n",
            "(epoch: 9, iters: 2500, time: 0.157) G_GAN: 1.555 G_GAN_Feat: 3.779 G_VGG: 2.128 D_real: 0.081 D_fake: 0.145 \n",
            "(epoch: 9, iters: 2600, time: 0.157) G_GAN: 1.944 G_GAN_Feat: 3.652 G_VGG: 1.977 D_real: 0.044 D_fake: 0.035 \n",
            "(epoch: 9, iters: 2700, time: 0.157) G_GAN: 1.958 G_GAN_Feat: 4.165 G_VGG: 2.418 D_real: 0.044 D_fake: 0.041 \n",
            "(epoch: 9, iters: 2800, time: 0.157) G_GAN: 2.237 G_GAN_Feat: 4.468 G_VGG: 2.765 D_real: 0.151 D_fake: 0.019 \n",
            "(epoch: 9, iters: 2900, time: 0.157) G_GAN: 2.523 G_GAN_Feat: 4.775 G_VGG: 2.820 D_real: 0.180 D_fake: 0.061 \n",
            "(epoch: 9, iters: 3000, time: 0.157) G_GAN: 1.630 G_GAN_Feat: 3.542 G_VGG: 1.317 D_real: 0.052 D_fake: 0.082 \n",
            "(epoch: 9, iters: 3100, time: 0.157) G_GAN: 1.859 G_GAN_Feat: 4.234 G_VGG: 2.501 D_real: 0.028 D_fake: 0.053 \n",
            "(epoch: 9, iters: 3200, time: 0.157) G_GAN: 2.132 G_GAN_Feat: 3.874 G_VGG: 2.808 D_real: 0.193 D_fake: 0.050 \n",
            "(epoch: 9, iters: 3300, time: 0.158) G_GAN: 1.594 G_GAN_Feat: 4.343 G_VGG: 2.504 D_real: 0.036 D_fake: 0.108 \n",
            "(epoch: 9, iters: 3400, time: 0.157) G_GAN: 1.858 G_GAN_Feat: 4.716 G_VGG: 2.803 D_real: 0.190 D_fake: 0.022 \n",
            "saving the latest model (epoch 9, total_steps 37000)\n",
            "(epoch: 9, iters: 3500, time: 0.157) G_GAN: 1.841 G_GAN_Feat: 4.098 G_VGG: 2.533 D_real: 0.017 D_fake: 0.100 \n",
            "(epoch: 9, iters: 3600, time: 0.157) G_GAN: 2.000 G_GAN_Feat: 4.091 G_VGG: 2.491 D_real: 0.062 D_fake: 0.020 \n",
            "(epoch: 9, iters: 3700, time: 0.157) G_GAN: 1.814 G_GAN_Feat: 4.626 G_VGG: 2.022 D_real: 0.025 D_fake: 0.018 \n",
            "(epoch: 9, iters: 3800, time: 0.157) G_GAN: 1.631 G_GAN_Feat: 3.803 G_VGG: 2.180 D_real: 0.025 D_fake: 0.170 \n",
            "(epoch: 9, iters: 3900, time: 0.157) G_GAN: 1.950 G_GAN_Feat: 4.349 G_VGG: 2.488 D_real: 0.191 D_fake: 0.037 \n",
            "(epoch: 9, iters: 4000, time: 0.158) G_GAN: 1.487 G_GAN_Feat: 4.060 G_VGG: 2.405 D_real: 0.046 D_fake: 0.114 \n",
            "(epoch: 9, iters: 4100, time: 0.157) G_GAN: 2.097 G_GAN_Feat: 4.598 G_VGG: 2.188 D_real: 0.016 D_fake: 0.012 \n",
            "(epoch: 9, iters: 4200, time: 0.157) G_GAN: 2.041 G_GAN_Feat: 3.426 G_VGG: 2.078 D_real: 0.043 D_fake: 0.037 \n",
            "End of epoch 9 / 200 \t Time Taken: 675 sec\n",
            "(epoch: 10, iters: 100, time: 0.157) G_GAN: 1.442 G_GAN_Feat: 3.906 G_VGG: 2.421 D_real: 0.100 D_fake: 0.130 \n",
            "(epoch: 10, iters: 200, time: 0.157) G_GAN: 1.774 G_GAN_Feat: 3.616 G_VGG: 1.980 D_real: 0.040 D_fake: 0.292 \n",
            "saving the latest model (epoch 10, total_steps 38000)\n",
            "(epoch: 10, iters: 300, time: 0.157) G_GAN: 1.354 G_GAN_Feat: 3.666 G_VGG: 3.080 D_real: 0.070 D_fake: 0.219 \n",
            "(epoch: 10, iters: 400, time: 0.157) G_GAN: 1.818 G_GAN_Feat: 4.480 G_VGG: 2.387 D_real: 0.226 D_fake: 0.033 \n",
            "(epoch: 10, iters: 500, time: 0.157) G_GAN: 1.905 G_GAN_Feat: 4.358 G_VGG: 2.734 D_real: 0.077 D_fake: 0.020 \n",
            "(epoch: 10, iters: 600, time: 0.157) G_GAN: 1.821 G_GAN_Feat: 3.703 G_VGG: 2.087 D_real: 0.255 D_fake: 0.023 \n",
            "(epoch: 10, iters: 700, time: 0.157) G_GAN: 1.656 G_GAN_Feat: 3.985 G_VGG: 2.290 D_real: 0.159 D_fake: 0.042 \n",
            "(epoch: 10, iters: 800, time: 0.157) G_GAN: 2.159 G_GAN_Feat: 4.168 G_VGG: 2.599 D_real: 0.131 D_fake: 0.043 \n",
            "(epoch: 10, iters: 900, time: 0.157) G_GAN: 1.944 G_GAN_Feat: 4.146 G_VGG: 2.029 D_real: 0.063 D_fake: 0.009 \n",
            "(epoch: 10, iters: 1000, time: 0.157) G_GAN: 1.223 G_GAN_Feat: 3.344 G_VGG: 1.908 D_real: 0.019 D_fake: 0.221 \n",
            "(epoch: 10, iters: 1100, time: 0.157) G_GAN: 2.037 G_GAN_Feat: 4.045 G_VGG: 2.496 D_real: 0.022 D_fake: 0.016 \n",
            "(epoch: 10, iters: 1200, time: 0.157) G_GAN: 2.098 G_GAN_Feat: 4.388 G_VGG: 1.964 D_real: 0.031 D_fake: 0.037 \n",
            "saving the latest model (epoch 10, total_steps 39000)\n",
            "(epoch: 10, iters: 1300, time: 0.157) G_GAN: 1.571 G_GAN_Feat: 3.863 G_VGG: 2.262 D_real: 0.035 D_fake: 0.254 \n",
            "(epoch: 10, iters: 1400, time: 0.157) G_GAN: 1.703 G_GAN_Feat: 4.008 G_VGG: 2.441 D_real: 0.013 D_fake: 0.065 \n",
            "(epoch: 10, iters: 1500, time: 0.157) G_GAN: 2.134 G_GAN_Feat: 3.532 G_VGG: 2.069 D_real: 0.453 D_fake: 0.135 \n",
            "(epoch: 10, iters: 1600, time: 0.157) G_GAN: 1.006 G_GAN_Feat: 3.145 G_VGG: 2.008 D_real: 0.462 D_fake: 0.538 \n",
            "(epoch: 10, iters: 1700, time: 0.157) G_GAN: 1.791 G_GAN_Feat: 3.738 G_VGG: 1.742 D_real: 0.281 D_fake: 0.043 \n",
            "(epoch: 10, iters: 1800, time: 0.158) G_GAN: 1.541 G_GAN_Feat: 2.621 G_VGG: 1.588 D_real: 0.114 D_fake: 0.186 \n",
            "(epoch: 10, iters: 1900, time: 0.157) G_GAN: 2.222 G_GAN_Feat: 4.171 G_VGG: 2.226 D_real: 0.533 D_fake: 0.044 \n",
            "(epoch: 10, iters: 2000, time: 0.157) G_GAN: 1.519 G_GAN_Feat: 3.742 G_VGG: 2.202 D_real: 0.038 D_fake: 0.152 \n",
            "(epoch: 10, iters: 2100, time: 0.157) G_GAN: 1.988 G_GAN_Feat: 4.643 G_VGG: 2.359 D_real: 0.035 D_fake: 0.052 \n",
            "(epoch: 10, iters: 2200, time: 0.157) G_GAN: 1.783 G_GAN_Feat: 3.711 G_VGG: 2.031 D_real: 0.029 D_fake: 0.050 \n",
            "saving the latest model (epoch 10, total_steps 40000)\n",
            "(epoch: 10, iters: 2300, time: 0.157) G_GAN: 1.965 G_GAN_Feat: 3.379 G_VGG: 1.609 D_real: 0.042 D_fake: 0.011 \n",
            "(epoch: 10, iters: 2400, time: 0.157) G_GAN: 2.045 G_GAN_Feat: 4.003 G_VGG: 2.824 D_real: 0.151 D_fake: 0.109 \n",
            "(epoch: 10, iters: 2500, time: 0.157) G_GAN: 2.335 G_GAN_Feat: 5.188 G_VGG: 2.587 D_real: 0.490 D_fake: 0.054 \n",
            "(epoch: 10, iters: 2600, time: 0.157) G_GAN: 2.144 G_GAN_Feat: 4.718 G_VGG: 2.397 D_real: 0.021 D_fake: 0.020 \n",
            "(epoch: 10, iters: 2700, time: 0.158) G_GAN: 1.760 G_GAN_Feat: 4.042 G_VGG: 1.726 D_real: 0.027 D_fake: 0.039 \n",
            "(epoch: 10, iters: 2800, time: 0.158) G_GAN: 2.079 G_GAN_Feat: 3.910 G_VGG: 2.076 D_real: 0.095 D_fake: 0.025 \n",
            "(epoch: 10, iters: 2900, time: 0.157) G_GAN: 2.104 G_GAN_Feat: 3.148 G_VGG: 1.735 D_real: 0.037 D_fake: 0.016 \n",
            "(epoch: 10, iters: 3000, time: 0.157) G_GAN: 1.152 G_GAN_Feat: 3.405 G_VGG: 2.141 D_real: 0.033 D_fake: 0.444 \n",
            "(epoch: 10, iters: 3100, time: 0.158) G_GAN: 1.945 G_GAN_Feat: 4.265 G_VGG: 2.227 D_real: 0.073 D_fake: 0.010 \n",
            "(epoch: 10, iters: 3200, time: 0.158) G_GAN: 1.556 G_GAN_Feat: 3.998 G_VGG: 2.620 D_real: 0.019 D_fake: 0.084 \n",
            "saving the latest model (epoch 10, total_steps 41000)\n",
            "(epoch: 10, iters: 3300, time: 0.157) G_GAN: 1.944 G_GAN_Feat: 3.877 G_VGG: 1.836 D_real: 0.140 D_fake: 0.138 \n",
            "(epoch: 10, iters: 3400, time: 0.157) G_GAN: 1.243 G_GAN_Feat: 4.117 G_VGG: 1.862 D_real: 0.110 D_fake: 0.322 \n",
            "(epoch: 10, iters: 3500, time: 0.157) G_GAN: 1.942 G_GAN_Feat: 4.711 G_VGG: 2.741 D_real: 0.309 D_fake: 0.024 \n",
            "(epoch: 10, iters: 3600, time: 0.157) G_GAN: 1.821 G_GAN_Feat: 4.363 G_VGG: 1.490 D_real: 0.038 D_fake: 0.039 \n",
            "(epoch: 10, iters: 3700, time: 0.157) G_GAN: 1.013 G_GAN_Feat: 3.347 G_VGG: 2.334 D_real: 0.276 D_fake: 0.488 \n",
            "(epoch: 10, iters: 3800, time: 0.157) G_GAN: 1.738 G_GAN_Feat: 3.320 G_VGG: 2.485 D_real: 0.273 D_fake: 0.085 \n",
            "(epoch: 10, iters: 3900, time: 0.157) G_GAN: 1.600 G_GAN_Feat: 4.137 G_VGG: 2.324 D_real: 0.023 D_fake: 0.102 \n",
            "(epoch: 10, iters: 4000, time: 0.158) G_GAN: 2.050 G_GAN_Feat: 3.526 G_VGG: 2.497 D_real: 0.290 D_fake: 0.045 \n",
            "(epoch: 10, iters: 4100, time: 0.157) G_GAN: 1.700 G_GAN_Feat: 2.960 G_VGG: 1.954 D_real: 0.138 D_fake: 0.127 \n",
            "(epoch: 10, iters: 4200, time: 0.157) G_GAN: 2.092 G_GAN_Feat: 4.015 G_VGG: 2.652 D_real: 0.086 D_fake: 0.010 \n",
            "saving the latest model (epoch 10, total_steps 42000)\n",
            "End of epoch 10 / 200 \t Time Taken: 680 sec\n",
            "saving the model at the end of epoch 10, iters 42000\n",
            "(epoch: 11, iters: 100, time: 0.157) G_GAN: 2.044 G_GAN_Feat: 4.024 G_VGG: 2.511 D_real: 0.053 D_fake: 0.023 \n",
            "(epoch: 11, iters: 200, time: 0.157) G_GAN: 1.839 G_GAN_Feat: 4.462 G_VGG: 2.227 D_real: 0.058 D_fake: 0.048 \n",
            "(epoch: 11, iters: 300, time: 0.157) G_GAN: 1.434 G_GAN_Feat: 3.475 G_VGG: 2.232 D_real: 0.052 D_fake: 0.128 \n",
            "(epoch: 11, iters: 400, time: 0.157) G_GAN: 1.845 G_GAN_Feat: 3.918 G_VGG: 2.034 D_real: 0.030 D_fake: 0.075 \n",
            "(epoch: 11, iters: 500, time: 0.158) G_GAN: 2.072 G_GAN_Feat: 3.917 G_VGG: 2.651 D_real: 0.479 D_fake: 0.014 \n",
            "(epoch: 11, iters: 600, time: 0.158) G_GAN: 1.261 G_GAN_Feat: 3.170 G_VGG: 2.817 D_real: 0.335 D_fake: 0.310 \n",
            "(epoch: 11, iters: 700, time: 0.157) G_GAN: 1.658 G_GAN_Feat: 3.730 G_VGG: 1.915 D_real: 0.026 D_fake: 0.126 \n",
            "(epoch: 11, iters: 800, time: 0.157) G_GAN: 1.956 G_GAN_Feat: 3.869 G_VGG: 2.473 D_real: 0.028 D_fake: 0.092 \n",
            "(epoch: 11, iters: 900, time: 0.158) G_GAN: 1.592 G_GAN_Feat: 4.037 G_VGG: 2.275 D_real: 0.031 D_fake: 0.066 \n",
            "(epoch: 11, iters: 1000, time: 0.158) G_GAN: 1.850 G_GAN_Feat: 3.922 G_VGG: 1.852 D_real: 0.129 D_fake: 0.021 \n",
            "saving the latest model (epoch 11, total_steps 43000)\n",
            "(epoch: 11, iters: 1100, time: 0.157) G_GAN: 1.906 G_GAN_Feat: 4.793 G_VGG: 2.604 D_real: 0.071 D_fake: 0.023 \n",
            "(epoch: 11, iters: 1200, time: 0.157) G_GAN: 1.865 G_GAN_Feat: 4.073 G_VGG: 1.912 D_real: 0.022 D_fake: 0.019 \n",
            "(epoch: 11, iters: 1300, time: 0.157) G_GAN: 1.544 G_GAN_Feat: 3.134 G_VGG: 1.758 D_real: 0.064 D_fake: 0.199 \n",
            "(epoch: 11, iters: 1400, time: 0.157) G_GAN: 1.644 G_GAN_Feat: 3.575 G_VGG: 2.385 D_real: 0.014 D_fake: 0.188 \n",
            "(epoch: 11, iters: 1500, time: 0.157) G_GAN: 1.679 G_GAN_Feat: 3.437 G_VGG: 1.941 D_real: 0.013 D_fake: 0.152 \n",
            "(epoch: 11, iters: 1600, time: 0.157) G_GAN: 2.138 G_GAN_Feat: 4.447 G_VGG: 2.554 D_real: 0.013 D_fake: 0.011 \n",
            "(epoch: 11, iters: 1700, time: 0.157) G_GAN: 1.420 G_GAN_Feat: 3.816 G_VGG: 2.386 D_real: 0.049 D_fake: 0.255 \n",
            "(epoch: 11, iters: 1800, time: 0.157) G_GAN: 1.899 G_GAN_Feat: 4.761 G_VGG: 2.408 D_real: 0.025 D_fake: 0.021 \n",
            "(epoch: 11, iters: 1900, time: 0.157) G_GAN: 2.154 G_GAN_Feat: 4.269 G_VGG: 2.016 D_real: 0.017 D_fake: 0.016 \n",
            "(epoch: 11, iters: 2000, time: 0.157) G_GAN: 2.105 G_GAN_Feat: 3.976 G_VGG: 2.182 D_real: 0.088 D_fake: 0.012 \n",
            "saving the latest model (epoch 11, total_steps 44000)\n",
            "(epoch: 11, iters: 2100, time: 0.157) G_GAN: 2.052 G_GAN_Feat: 4.033 G_VGG: 2.656 D_real: 0.031 D_fake: 0.012 \n",
            "(epoch: 11, iters: 2200, time: 0.157) G_GAN: 1.889 G_GAN_Feat: 4.409 G_VGG: 2.752 D_real: 0.167 D_fake: 0.012 \n",
            "(epoch: 11, iters: 2300, time: 0.157) G_GAN: 1.283 G_GAN_Feat: 3.331 G_VGG: 2.471 D_real: 0.113 D_fake: 0.389 \n",
            "(epoch: 11, iters: 2400, time: 0.157) G_GAN: 1.860 G_GAN_Feat: 3.564 G_VGG: 1.932 D_real: 0.018 D_fake: 0.048 \n",
            "(epoch: 11, iters: 2500, time: 0.158) G_GAN: 1.798 G_GAN_Feat: 4.138 G_VGG: 2.015 D_real: 0.021 D_fake: 0.052 \n",
            "(epoch: 11, iters: 2600, time: 0.157) G_GAN: 1.653 G_GAN_Feat: 3.662 G_VGG: 1.652 D_real: 0.021 D_fake: 0.050 \n",
            "(epoch: 11, iters: 2700, time: 0.157) G_GAN: 2.162 G_GAN_Feat: 4.552 G_VGG: 2.350 D_real: 0.030 D_fake: 0.031 \n",
            "(epoch: 11, iters: 2800, time: 0.157) G_GAN: 1.199 G_GAN_Feat: 3.658 G_VGG: 1.861 D_real: 0.013 D_fake: 0.206 \n",
            "(epoch: 11, iters: 2900, time: 0.157) G_GAN: 2.011 G_GAN_Feat: 3.819 G_VGG: 1.619 D_real: 0.230 D_fake: 0.021 \n",
            "(epoch: 11, iters: 3000, time: 0.158) G_GAN: 1.732 G_GAN_Feat: 3.704 G_VGG: 2.234 D_real: 0.020 D_fake: 0.061 \n",
            "saving the latest model (epoch 11, total_steps 45000)\n",
            "(epoch: 11, iters: 3100, time: 0.157) G_GAN: 1.718 G_GAN_Feat: 3.746 G_VGG: 1.931 D_real: 0.012 D_fake: 0.070 \n",
            "(epoch: 11, iters: 3200, time: 0.157) G_GAN: 2.390 G_GAN_Feat: 3.597 G_VGG: 2.022 D_real: 0.108 D_fake: 0.032 \n",
            "(epoch: 11, iters: 3300, time: 0.157) G_GAN: 1.827 G_GAN_Feat: 4.059 G_VGG: 2.067 D_real: 0.016 D_fake: 0.038 \n",
            "(epoch: 11, iters: 3400, time: 0.157) G_GAN: 2.248 G_GAN_Feat: 4.051 G_VGG: 2.621 D_real: 0.132 D_fake: 0.060 \n",
            "(epoch: 11, iters: 3500, time: 0.157) G_GAN: 1.999 G_GAN_Feat: 4.187 G_VGG: 2.301 D_real: 0.015 D_fake: 0.014 \n",
            "(epoch: 11, iters: 3600, time: 0.157) G_GAN: 1.827 G_GAN_Feat: 3.775 G_VGG: 2.607 D_real: 0.029 D_fake: 0.031 \n",
            "(epoch: 11, iters: 3700, time: 0.157) G_GAN: 1.797 G_GAN_Feat: 4.171 G_VGG: 1.948 D_real: 0.007 D_fake: 0.037 \n",
            "(epoch: 11, iters: 3800, time: 0.157) G_GAN: 2.194 G_GAN_Feat: 4.188 G_VGG: 2.576 D_real: 0.140 D_fake: 0.041 \n",
            "(epoch: 11, iters: 3900, time: 0.157) G_GAN: 1.355 G_GAN_Feat: 2.514 G_VGG: 2.057 D_real: 0.299 D_fake: 0.224 \n",
            "(epoch: 11, iters: 4000, time: 0.157) G_GAN: 0.943 G_GAN_Feat: 2.387 G_VGG: 1.588 D_real: 0.306 D_fake: 0.280 \n",
            "saving the latest model (epoch 11, total_steps 46000)\n",
            "(epoch: 11, iters: 4100, time: 0.157) G_GAN: 1.027 G_GAN_Feat: 2.708 G_VGG: 1.997 D_real: 0.605 D_fake: 0.356 \n",
            "(epoch: 11, iters: 4200, time: 0.157) G_GAN: 1.285 G_GAN_Feat: 2.078 G_VGG: 1.880 D_real: 0.342 D_fake: 0.266 \n",
            "End of epoch 11 / 200 \t Time Taken: 676 sec\n",
            "(epoch: 12, iters: 100, time: 0.157) G_GAN: 0.975 G_GAN_Feat: 2.562 G_VGG: 1.913 D_real: 0.294 D_fake: 0.373 \n",
            "(epoch: 12, iters: 200, time: 0.157) G_GAN: 1.319 G_GAN_Feat: 3.811 G_VGG: 2.377 D_real: 0.029 D_fake: 0.346 \n",
            "(epoch: 12, iters: 300, time: 0.157) G_GAN: 1.310 G_GAN_Feat: 3.197 G_VGG: 1.751 D_real: 0.018 D_fake: 0.220 \n",
            "(epoch: 12, iters: 400, time: 0.157) G_GAN: 2.122 G_GAN_Feat: 4.165 G_VGG: 2.379 D_real: 0.033 D_fake: 0.013 \n",
            "(epoch: 12, iters: 500, time: 0.157) G_GAN: 1.999 G_GAN_Feat: 4.054 G_VGG: 2.292 D_real: 0.341 D_fake: 0.037 \n",
            "(epoch: 12, iters: 600, time: 0.157) G_GAN: 1.831 G_GAN_Feat: 3.865 G_VGG: 2.049 D_real: 0.015 D_fake: 0.058 \n",
            "(epoch: 12, iters: 700, time: 0.157) G_GAN: 2.168 G_GAN_Feat: 3.794 G_VGG: 2.604 D_real: 0.380 D_fake: 0.029 \n",
            "(epoch: 12, iters: 800, time: 0.158) G_GAN: 2.022 G_GAN_Feat: 4.348 G_VGG: 2.199 D_real: 0.021 D_fake: 0.022 \n",
            "saving the latest model (epoch 12, total_steps 47000)\n",
            "(epoch: 12, iters: 900, time: 0.157) G_GAN: 1.695 G_GAN_Feat: 3.606 G_VGG: 1.838 D_real: 0.018 D_fake: 0.050 \n",
            "(epoch: 12, iters: 1000, time: 0.157) G_GAN: 2.111 G_GAN_Feat: 3.296 G_VGG: 1.969 D_real: 0.134 D_fake: 0.010 \n",
            "(epoch: 12, iters: 1100, time: 0.157) G_GAN: 1.983 G_GAN_Feat: 3.834 G_VGG: 1.922 D_real: 0.223 D_fake: 0.031 \n",
            "(epoch: 12, iters: 1200, time: 0.157) G_GAN: 1.626 G_GAN_Feat: 3.387 G_VGG: 1.964 D_real: 0.044 D_fake: 0.093 \n",
            "(epoch: 12, iters: 1300, time: 0.157) G_GAN: 1.705 G_GAN_Feat: 3.472 G_VGG: 1.486 D_real: 0.010 D_fake: 0.032 \n",
            "(epoch: 12, iters: 1400, time: 0.157) G_GAN: 1.834 G_GAN_Feat: 3.385 G_VGG: 1.977 D_real: 0.077 D_fake: 0.016 \n",
            "(epoch: 12, iters: 1500, time: 0.157) G_GAN: 1.095 G_GAN_Feat: 3.445 G_VGG: 2.305 D_real: 0.027 D_fake: 0.369 \n",
            "(epoch: 12, iters: 1600, time: 0.157) G_GAN: 1.816 G_GAN_Feat: 4.070 G_VGG: 2.338 D_real: 0.040 D_fake: 0.046 \n",
            "(epoch: 12, iters: 1700, time: 0.157) G_GAN: 1.806 G_GAN_Feat: 3.505 G_VGG: 1.882 D_real: 0.023 D_fake: 0.022 \n",
            "(epoch: 12, iters: 1800, time: 0.158) G_GAN: 1.989 G_GAN_Feat: 4.991 G_VGG: 3.000 D_real: 0.058 D_fake: 0.006 \n",
            "saving the latest model (epoch 12, total_steps 48000)\n",
            "(epoch: 12, iters: 1900, time: 0.157) G_GAN: 1.829 G_GAN_Feat: 4.747 G_VGG: 2.316 D_real: 0.035 D_fake: 0.030 \n",
            "(epoch: 12, iters: 2000, time: 0.157) G_GAN: 2.148 G_GAN_Feat: 4.061 G_VGG: 2.414 D_real: 0.006 D_fake: 0.008 \n",
            "(epoch: 12, iters: 2100, time: 0.157) G_GAN: 2.115 G_GAN_Feat: 4.225 G_VGG: 2.274 D_real: 0.279 D_fake: 0.023 \n",
            "(epoch: 12, iters: 2200, time: 0.157) G_GAN: 1.327 G_GAN_Feat: 3.473 G_VGG: 2.308 D_real: 0.062 D_fake: 0.270 \n",
            "(epoch: 12, iters: 2300, time: 0.157) G_GAN: 1.565 G_GAN_Feat: 3.758 G_VGG: 1.659 D_real: 0.023 D_fake: 0.136 \n",
            "(epoch: 12, iters: 2400, time: 0.157) G_GAN: 1.866 G_GAN_Feat: 4.146 G_VGG: 2.000 D_real: 0.046 D_fake: 0.023 \n",
            "(epoch: 12, iters: 2500, time: 0.157) G_GAN: 1.699 G_GAN_Feat: 3.506 G_VGG: 2.012 D_real: 0.019 D_fake: 0.062 \n",
            "(epoch: 12, iters: 2600, time: 0.157) G_GAN: 1.608 G_GAN_Feat: 3.682 G_VGG: 1.783 D_real: 0.015 D_fake: 0.058 \n",
            "(epoch: 12, iters: 2700, time: 0.157) G_GAN: 1.452 G_GAN_Feat: 2.864 G_VGG: 1.730 D_real: 0.144 D_fake: 0.137 \n",
            "(epoch: 12, iters: 2800, time: 0.157) G_GAN: 2.071 G_GAN_Feat: 4.108 G_VGG: 2.070 D_real: 0.234 D_fake: 0.016 \n",
            "saving the latest model (epoch 12, total_steps 49000)\n",
            "(epoch: 12, iters: 2900, time: 0.157) G_GAN: 1.529 G_GAN_Feat: 3.514 G_VGG: 1.963 D_real: 0.021 D_fake: 0.154 \n",
            "(epoch: 12, iters: 3000, time: 0.157) G_GAN: 1.725 G_GAN_Feat: 3.850 G_VGG: 2.048 D_real: 0.110 D_fake: 0.048 \n",
            "(epoch: 12, iters: 3100, time: 0.157) G_GAN: 1.669 G_GAN_Feat: 3.858 G_VGG: 2.093 D_real: 0.065 D_fake: 0.091 \n",
            "(epoch: 12, iters: 3200, time: 0.157) G_GAN: 1.679 G_GAN_Feat: 3.440 G_VGG: 2.162 D_real: 0.009 D_fake: 0.049 \n",
            "(epoch: 12, iters: 3300, time: 0.157) G_GAN: 1.640 G_GAN_Feat: 3.790 G_VGG: 2.418 D_real: 0.107 D_fake: 0.034 \n",
            "(epoch: 12, iters: 3400, time: 0.157) G_GAN: 2.159 G_GAN_Feat: 4.013 G_VGG: 2.038 D_real: 0.588 D_fake: 0.026 \n",
            "(epoch: 12, iters: 3500, time: 0.157) G_GAN: 2.012 G_GAN_Feat: 3.574 G_VGG: 1.948 D_real: 0.017 D_fake: 0.008 \n",
            "(epoch: 12, iters: 3600, time: 0.157) G_GAN: 1.854 G_GAN_Feat: 3.785 G_VGG: 2.238 D_real: 0.012 D_fake: 0.020 \n",
            "(epoch: 12, iters: 3700, time: 0.157) G_GAN: 1.978 G_GAN_Feat: 3.616 G_VGG: 1.930 D_real: 0.023 D_fake: 0.026 \n",
            "(epoch: 12, iters: 3800, time: 0.157) G_GAN: 1.604 G_GAN_Feat: 3.927 G_VGG: 2.492 D_real: 0.023 D_fake: 0.057 \n",
            "saving the latest model (epoch 12, total_steps 50000)\n",
            "(epoch: 12, iters: 3900, time: 0.157) G_GAN: 1.970 G_GAN_Feat: 3.800 G_VGG: 2.035 D_real: 0.037 D_fake: 0.019 \n",
            "(epoch: 12, iters: 4000, time: 0.157) G_GAN: 1.752 G_GAN_Feat: 3.132 G_VGG: 2.465 D_real: 0.865 D_fake: 0.660 \n",
            "(epoch: 12, iters: 4100, time: 0.157) G_GAN: 1.703 G_GAN_Feat: 3.427 G_VGG: 2.566 D_real: 0.070 D_fake: 0.062 \n",
            "(epoch: 12, iters: 4200, time: 0.157) G_GAN: 1.755 G_GAN_Feat: 3.339 G_VGG: 1.961 D_real: 0.016 D_fake: 0.071 \n",
            "End of epoch 12 / 200 \t Time Taken: 677 sec\n",
            "(epoch: 13, iters: 100, time: 0.157) G_GAN: 1.951 G_GAN_Feat: 3.908 G_VGG: 2.356 D_real: 0.026 D_fake: 0.015 \n",
            "(epoch: 13, iters: 200, time: 0.157) G_GAN: 1.976 G_GAN_Feat: 3.026 G_VGG: 2.177 D_real: 0.019 D_fake: 0.032 \n",
            "(epoch: 13, iters: 300, time: 0.157) G_GAN: 1.517 G_GAN_Feat: 3.679 G_VGG: 2.149 D_real: 0.292 D_fake: 0.090 \n",
            "(epoch: 13, iters: 400, time: 0.157) G_GAN: 1.479 G_GAN_Feat: 3.409 G_VGG: 2.004 D_real: 0.011 D_fake: 0.110 \n",
            "(epoch: 13, iters: 500, time: 0.157) G_GAN: 1.852 G_GAN_Feat: 3.916 G_VGG: 1.799 D_real: 0.024 D_fake: 0.019 \n",
            "(epoch: 13, iters: 600, time: 0.157) G_GAN: 1.976 G_GAN_Feat: 3.290 G_VGG: 1.867 D_real: 0.106 D_fake: 0.005 \n",
            "saving the latest model (epoch 13, total_steps 51000)\n",
            "(epoch: 13, iters: 700, time: 0.157) G_GAN: 1.556 G_GAN_Feat: 3.657 G_VGG: 2.485 D_real: 0.013 D_fake: 0.052 \n",
            "(epoch: 13, iters: 800, time: 0.157) G_GAN: 1.937 G_GAN_Feat: 3.611 G_VGG: 1.965 D_real: 0.011 D_fake: 0.034 \n",
            "(epoch: 13, iters: 900, time: 0.157) G_GAN: 2.063 G_GAN_Feat: 4.305 G_VGG: 2.467 D_real: 0.012 D_fake: 0.009 \n",
            "(epoch: 13, iters: 1000, time: 0.157) G_GAN: 1.816 G_GAN_Feat: 3.383 G_VGG: 1.712 D_real: 0.015 D_fake: 0.160 \n",
            "(epoch: 13, iters: 1100, time: 0.157) G_GAN: 1.893 G_GAN_Feat: 3.131 G_VGG: 1.948 D_real: 0.401 D_fake: 0.120 \n",
            "(epoch: 13, iters: 1200, time: 0.157) G_GAN: 2.027 G_GAN_Feat: 3.433 G_VGG: 1.869 D_real: 0.081 D_fake: 0.022 \n",
            "(epoch: 13, iters: 1300, time: 0.157) G_GAN: 1.930 G_GAN_Feat: 3.900 G_VGG: 2.036 D_real: 0.163 D_fake: 0.012 \n",
            "(epoch: 13, iters: 1400, time: 0.157) G_GAN: 1.594 G_GAN_Feat: 3.594 G_VGG: 2.394 D_real: 0.021 D_fake: 0.039 \n",
            "(epoch: 13, iters: 1500, time: 0.157) G_GAN: 1.855 G_GAN_Feat: 2.909 G_VGG: 1.734 D_real: 0.017 D_fake: 0.027 \n",
            "(epoch: 13, iters: 1600, time: 0.157) G_GAN: 2.139 G_GAN_Feat: 4.325 G_VGG: 1.710 D_real: 0.013 D_fake: 0.013 \n",
            "saving the latest model (epoch 13, total_steps 52000)\n",
            "(epoch: 13, iters: 1700, time: 0.157) G_GAN: 1.548 G_GAN_Feat: 3.229 G_VGG: 2.169 D_real: 0.068 D_fake: 0.179 \n",
            "(epoch: 13, iters: 1800, time: 0.157) G_GAN: 2.137 G_GAN_Feat: 3.506 G_VGG: 2.334 D_real: 0.804 D_fake: 0.072 \n",
            "(epoch: 13, iters: 1900, time: 0.157) G_GAN: 1.998 G_GAN_Feat: 3.362 G_VGG: 1.745 D_real: 0.326 D_fake: 0.060 \n",
            "(epoch: 13, iters: 2000, time: 0.157) G_GAN: 1.658 G_GAN_Feat: 3.096 G_VGG: 1.798 D_real: 0.040 D_fake: 0.090 \n",
            "(epoch: 13, iters: 2100, time: 0.157) G_GAN: 1.644 G_GAN_Feat: 3.180 G_VGG: 2.155 D_real: 0.068 D_fake: 0.047 \n",
            "(epoch: 13, iters: 2200, time: 0.157) G_GAN: 1.833 G_GAN_Feat: 3.571 G_VGG: 1.985 D_real: 0.008 D_fake: 0.039 \n",
            "(epoch: 13, iters: 2300, time: 0.157) G_GAN: 1.244 G_GAN_Feat: 2.384 G_VGG: 1.871 D_real: 0.202 D_fake: 0.298 \n",
            "(epoch: 13, iters: 2400, time: 0.157) G_GAN: 1.479 G_GAN_Feat: 3.182 G_VGG: 1.720 D_real: 0.139 D_fake: 0.109 \n",
            "(epoch: 13, iters: 2500, time: 0.157) G_GAN: 1.650 G_GAN_Feat: 2.988 G_VGG: 1.785 D_real: 0.079 D_fake: 0.044 \n",
            "(epoch: 13, iters: 2600, time: 0.157) G_GAN: 2.014 G_GAN_Feat: 3.413 G_VGG: 2.112 D_real: 0.013 D_fake: 0.007 \n",
            "saving the latest model (epoch 13, total_steps 53000)\n",
            "(epoch: 13, iters: 2700, time: 0.157) G_GAN: 2.126 G_GAN_Feat: 3.730 G_VGG: 2.181 D_real: 0.007 D_fake: 0.008 \n",
            "(epoch: 13, iters: 2800, time: 0.157) G_GAN: 1.705 G_GAN_Feat: 3.595 G_VGG: 2.252 D_real: 0.035 D_fake: 0.018 \n",
            "(epoch: 13, iters: 2900, time: 0.157) G_GAN: 1.649 G_GAN_Feat: 4.431 G_VGG: 2.393 D_real: 0.099 D_fake: 0.219 \n",
            "(epoch: 13, iters: 3000, time: 0.157) G_GAN: 2.210 G_GAN_Feat: 4.328 G_VGG: 1.768 D_real: 0.324 D_fake: 0.019 \n",
            "(epoch: 13, iters: 3100, time: 0.157) G_GAN: 1.949 G_GAN_Feat: 3.896 G_VGG: 1.965 D_real: 0.009 D_fake: 0.013 \n",
            "(epoch: 13, iters: 3200, time: 0.158) G_GAN: 1.944 G_GAN_Feat: 4.223 G_VGG: 2.470 D_real: 0.014 D_fake: 0.013 \n",
            "(epoch: 13, iters: 3300, time: 0.157) G_GAN: 1.746 G_GAN_Feat: 3.490 G_VGG: 2.213 D_real: 0.044 D_fake: 0.047 \n",
            "(epoch: 13, iters: 3400, time: 0.157) G_GAN: 1.988 G_GAN_Feat: 4.216 G_VGG: 2.013 D_real: 0.011 D_fake: 0.023 \n",
            "(epoch: 13, iters: 3500, time: 0.157) G_GAN: 1.326 G_GAN_Feat: 2.992 G_VGG: 1.937 D_real: 0.051 D_fake: 0.270 \n",
            "(epoch: 13, iters: 3600, time: 0.157) G_GAN: 1.866 G_GAN_Feat: 4.280 G_VGG: 2.365 D_real: 0.014 D_fake: 0.016 \n",
            "saving the latest model (epoch 13, total_steps 54000)\n",
            "(epoch: 13, iters: 3700, time: 0.157) G_GAN: 1.203 G_GAN_Feat: 3.852 G_VGG: 1.591 D_real: 0.021 D_fake: 0.354 \n",
            "(epoch: 13, iters: 3800, time: 0.157) G_GAN: 2.043 G_GAN_Feat: 3.784 G_VGG: 1.883 D_real: 0.128 D_fake: 0.033 \n",
            "(epoch: 13, iters: 3900, time: 0.157) G_GAN: 1.907 G_GAN_Feat: 4.301 G_VGG: 1.854 D_real: 0.016 D_fake: 0.012 \n",
            "(epoch: 13, iters: 4000, time: 0.157) G_GAN: 1.731 G_GAN_Feat: 4.585 G_VGG: 1.937 D_real: 0.023 D_fake: 0.029 \n",
            "(epoch: 13, iters: 4100, time: 0.157) G_GAN: 1.893 G_GAN_Feat: 3.476 G_VGG: 1.854 D_real: 0.212 D_fake: 0.016 \n",
            "(epoch: 13, iters: 4200, time: 0.157) G_GAN: 1.593 G_GAN_Feat: 3.281 G_VGG: 1.750 D_real: 0.212 D_fake: 0.153 \n",
            "End of epoch 13 / 200 \t Time Taken: 677 sec\n",
            "(epoch: 14, iters: 100, time: 0.157) G_GAN: 1.566 G_GAN_Feat: 3.475 G_VGG: 1.971 D_real: 0.074 D_fake: 0.141 \n",
            "(epoch: 14, iters: 200, time: 0.157) G_GAN: 1.980 G_GAN_Feat: 2.943 G_VGG: 1.941 D_real: 0.361 D_fake: 0.020 \n",
            "(epoch: 14, iters: 300, time: 0.157) G_GAN: 1.957 G_GAN_Feat: 3.398 G_VGG: 2.055 D_real: 0.022 D_fake: 0.009 \n",
            "(epoch: 14, iters: 400, time: 0.157) G_GAN: 1.934 G_GAN_Feat: 3.215 G_VGG: 2.324 D_real: 0.377 D_fake: 0.103 \n",
            "saving the latest model (epoch 14, total_steps 55000)\n",
            "(epoch: 14, iters: 500, time: 0.157) G_GAN: 2.010 G_GAN_Feat: 3.333 G_VGG: 1.980 D_real: 0.008 D_fake: 0.009 \n",
            "(epoch: 14, iters: 600, time: 0.157) G_GAN: 1.882 G_GAN_Feat: 3.250 G_VGG: 2.167 D_real: 0.064 D_fake: 0.048 \n",
            "(epoch: 14, iters: 700, time: 0.157) G_GAN: 1.947 G_GAN_Feat: 4.051 G_VGG: 1.647 D_real: 0.013 D_fake: 0.011 \n",
            "(epoch: 14, iters: 800, time: 0.157) G_GAN: 1.630 G_GAN_Feat: 4.303 G_VGG: 2.006 D_real: 0.017 D_fake: 0.112 \n",
            "(epoch: 14, iters: 900, time: 0.157) G_GAN: 1.848 G_GAN_Feat: 3.710 G_VGG: 2.033 D_real: 0.028 D_fake: 0.043 \n",
            "(epoch: 14, iters: 1000, time: 0.157) G_GAN: 1.699 G_GAN_Feat: 3.903 G_VGG: 2.070 D_real: 0.009 D_fake: 0.079 \n",
            "(epoch: 14, iters: 1100, time: 0.157) G_GAN: 1.723 G_GAN_Feat: 3.654 G_VGG: 2.444 D_real: 0.024 D_fake: 0.077 \n",
            "(epoch: 14, iters: 1200, time: 0.157) G_GAN: 1.943 G_GAN_Feat: 4.211 G_VGG: 2.150 D_real: 0.271 D_fake: 0.008 \n",
            "(epoch: 14, iters: 1300, time: 0.157) G_GAN: 1.556 G_GAN_Feat: 3.492 G_VGG: 2.091 D_real: 0.047 D_fake: 0.165 \n",
            "(epoch: 14, iters: 1400, time: 0.157) G_GAN: 1.943 G_GAN_Feat: 3.301 G_VGG: 2.272 D_real: 0.064 D_fake: 0.100 \n",
            "saving the latest model (epoch 14, total_steps 56000)\n",
            "(epoch: 14, iters: 1500, time: 0.157) G_GAN: 2.084 G_GAN_Feat: 3.436 G_VGG: 2.107 D_real: 0.042 D_fake: 0.008 \n",
            "(epoch: 14, iters: 1600, time: 0.157) G_GAN: 2.043 G_GAN_Feat: 4.626 G_VGG: 2.290 D_real: 0.067 D_fake: 0.009 \n",
            "(epoch: 14, iters: 1700, time: 0.157) G_GAN: 2.055 G_GAN_Feat: 3.472 G_VGG: 2.314 D_real: 0.022 D_fake: 0.010 \n",
            "(epoch: 14, iters: 1800, time: 0.157) G_GAN: 1.629 G_GAN_Feat: 3.424 G_VGG: 1.697 D_real: 0.017 D_fake: 0.123 \n",
            "(epoch: 14, iters: 1900, time: 0.157) G_GAN: 2.124 G_GAN_Feat: 4.016 G_VGG: 2.563 D_real: 0.059 D_fake: 0.037 \n",
            "(epoch: 14, iters: 2000, time: 0.157) G_GAN: 1.903 G_GAN_Feat: 3.793 G_VGG: 2.023 D_real: 0.040 D_fake: 0.025 \n",
            "(epoch: 14, iters: 2100, time: 0.157) G_GAN: 2.188 G_GAN_Feat: 3.730 G_VGG: 2.272 D_real: 0.023 D_fake: 0.017 \n",
            "(epoch: 14, iters: 2200, time: 0.157) G_GAN: 1.832 G_GAN_Feat: 3.604 G_VGG: 2.295 D_real: 0.020 D_fake: 0.096 \n",
            "(epoch: 14, iters: 2300, time: 0.157) G_GAN: 1.939 G_GAN_Feat: 3.709 G_VGG: 2.257 D_real: 0.105 D_fake: 0.009 \n",
            "(epoch: 14, iters: 2400, time: 0.157) G_GAN: 1.771 G_GAN_Feat: 3.523 G_VGG: 2.312 D_real: 0.017 D_fake: 0.021 \n",
            "saving the latest model (epoch 14, total_steps 57000)\n",
            "(epoch: 14, iters: 2500, time: 0.157) G_GAN: 1.668 G_GAN_Feat: 3.447 G_VGG: 1.755 D_real: 0.019 D_fake: 0.030 \n",
            "(epoch: 14, iters: 2600, time: 0.157) G_GAN: 1.855 G_GAN_Feat: 3.336 G_VGG: 2.007 D_real: 0.017 D_fake: 0.050 \n",
            "(epoch: 14, iters: 2700, time: 0.157) G_GAN: 1.823 G_GAN_Feat: 3.445 G_VGG: 2.292 D_real: 0.082 D_fake: 0.022 \n",
            "(epoch: 14, iters: 2800, time: 0.157) G_GAN: 1.234 G_GAN_Feat: 3.775 G_VGG: 1.827 D_real: 0.111 D_fake: 0.226 \n",
            "(epoch: 14, iters: 2900, time: 0.157) G_GAN: 1.837 G_GAN_Feat: 3.523 G_VGG: 1.699 D_real: 0.014 D_fake: 0.019 \n",
            "(epoch: 14, iters: 3000, time: 0.157) G_GAN: 1.952 G_GAN_Feat: 4.080 G_VGG: 2.146 D_real: 0.018 D_fake: 0.013 \n",
            "(epoch: 14, iters: 3100, time: 0.158) G_GAN: 1.758 G_GAN_Feat: 3.587 G_VGG: 2.235 D_real: 0.014 D_fake: 0.021 \n",
            "(epoch: 14, iters: 3200, time: 0.157) G_GAN: 1.449 G_GAN_Feat: 2.928 G_VGG: 1.934 D_real: 0.042 D_fake: 0.154 \n",
            "(epoch: 14, iters: 3300, time: 0.157) G_GAN: 1.795 G_GAN_Feat: 3.571 G_VGG: 1.944 D_real: 0.018 D_fake: 0.038 \n",
            "(epoch: 14, iters: 3400, time: 0.157) G_GAN: 1.259 G_GAN_Feat: 3.096 G_VGG: 1.908 D_real: 0.039 D_fake: 0.263 \n",
            "saving the latest model (epoch 14, total_steps 58000)\n",
            "(epoch: 14, iters: 3500, time: 0.157) G_GAN: 1.640 G_GAN_Feat: 3.370 G_VGG: 1.906 D_real: 0.104 D_fake: 0.289 \n",
            "(epoch: 14, iters: 3600, time: 0.157) G_GAN: 2.116 G_GAN_Feat: 3.894 G_VGG: 1.936 D_real: 0.469 D_fake: 0.010 \n",
            "(epoch: 14, iters: 3700, time: 0.157) G_GAN: 1.702 G_GAN_Feat: 2.462 G_VGG: 1.446 D_real: 0.151 D_fake: 0.032 \n",
            "(epoch: 14, iters: 3800, time: 0.157) G_GAN: 1.942 G_GAN_Feat: 3.406 G_VGG: 2.049 D_real: 0.124 D_fake: 0.021 \n",
            "(epoch: 14, iters: 3900, time: 0.157) G_GAN: 1.815 G_GAN_Feat: 3.348 G_VGG: 1.568 D_real: 0.013 D_fake: 0.023 \n",
            "(epoch: 14, iters: 4000, time: 0.157) G_GAN: 2.240 G_GAN_Feat: 4.982 G_VGG: 2.313 D_real: 0.308 D_fake: 0.013 \n",
            "(epoch: 14, iters: 4100, time: 0.157) G_GAN: 1.827 G_GAN_Feat: 3.844 G_VGG: 2.013 D_real: 0.019 D_fake: 0.014 \n",
            "(epoch: 14, iters: 4200, time: 0.157) G_GAN: 2.078 G_GAN_Feat: 2.878 G_VGG: 1.699 D_real: 0.217 D_fake: 0.059 \n",
            "End of epoch 14 / 200 \t Time Taken: 677 sec\n",
            "(epoch: 15, iters: 100, time: 0.157) G_GAN: 1.363 G_GAN_Feat: 2.861 G_VGG: 1.962 D_real: 0.442 D_fake: 0.190 \n",
            "(epoch: 15, iters: 200, time: 0.157) G_GAN: 2.173 G_GAN_Feat: 2.790 G_VGG: 2.079 D_real: 0.292 D_fake: 0.120 \n",
            "saving the latest model (epoch 15, total_steps 59000)\n",
            "(epoch: 15, iters: 300, time: 0.157) G_GAN: 1.572 G_GAN_Feat: 3.073 G_VGG: 2.259 D_real: 0.017 D_fake: 0.134 \n",
            "(epoch: 15, iters: 400, time: 0.157) G_GAN: 2.051 G_GAN_Feat: 4.060 G_VGG: 2.076 D_real: 0.045 D_fake: 0.013 \n",
            "(epoch: 15, iters: 500, time: 0.157) G_GAN: 1.762 G_GAN_Feat: 3.640 G_VGG: 1.762 D_real: 0.027 D_fake: 0.027 \n",
            "(epoch: 15, iters: 600, time: 0.157) G_GAN: 1.844 G_GAN_Feat: 3.230 G_VGG: 1.560 D_real: 0.050 D_fake: 0.059 \n",
            "(epoch: 15, iters: 700, time: 0.157) G_GAN: 1.812 G_GAN_Feat: 3.470 G_VGG: 2.205 D_real: 0.011 D_fake: 0.062 \n",
            "(epoch: 15, iters: 800, time: 0.157) G_GAN: 2.042 G_GAN_Feat: 3.635 G_VGG: 1.881 D_real: 0.011 D_fake: 0.023 \n",
            "(epoch: 15, iters: 900, time: 0.157) G_GAN: 1.806 G_GAN_Feat: 3.045 G_VGG: 1.821 D_real: 0.088 D_fake: 0.051 \n",
            "(epoch: 15, iters: 1000, time: 0.157) G_GAN: 1.978 G_GAN_Feat: 3.642 G_VGG: 2.280 D_real: 0.016 D_fake: 0.007 \n",
            "(epoch: 15, iters: 1100, time: 0.157) G_GAN: 2.115 G_GAN_Feat: 3.741 G_VGG: 2.214 D_real: 0.456 D_fake: 0.231 \n",
            "(epoch: 15, iters: 1200, time: 0.157) G_GAN: 1.724 G_GAN_Feat: 3.567 G_VGG: 2.411 D_real: 0.036 D_fake: 0.031 \n",
            "saving the latest model (epoch 15, total_steps 60000)\n",
            "(epoch: 15, iters: 1300, time: 0.157) G_GAN: 2.146 G_GAN_Feat: 3.842 G_VGG: 2.273 D_real: 0.016 D_fake: 0.008 \n",
            "(epoch: 15, iters: 1400, time: 0.157) G_GAN: 1.557 G_GAN_Feat: 3.010 G_VGG: 1.801 D_real: 0.021 D_fake: 0.061 \n",
            "(epoch: 15, iters: 1500, time: 0.157) G_GAN: 0.856 G_GAN_Feat: 3.006 G_VGG: 2.042 D_real: 0.135 D_fake: 0.412 \n",
            "(epoch: 15, iters: 1600, time: 0.157) G_GAN: 1.846 G_GAN_Feat: 3.446 G_VGG: 2.238 D_real: 0.137 D_fake: 0.071 \n",
            "(epoch: 15, iters: 1700, time: 0.157) G_GAN: 1.819 G_GAN_Feat: 2.734 G_VGG: 1.928 D_real: 0.023 D_fake: 0.036 \n",
            "(epoch: 15, iters: 1800, time: 0.157) G_GAN: 1.696 G_GAN_Feat: 3.264 G_VGG: 1.676 D_real: 0.209 D_fake: 0.045 \n",
            "(epoch: 15, iters: 1900, time: 0.157) G_GAN: 1.898 G_GAN_Feat: 3.605 G_VGG: 1.850 D_real: 0.311 D_fake: 0.038 \n",
            "(epoch: 15, iters: 2000, time: 0.157) G_GAN: 1.912 G_GAN_Feat: 4.332 G_VGG: 2.282 D_real: 0.031 D_fake: 0.008 \n",
            "(epoch: 15, iters: 2100, time: 0.157) G_GAN: 2.056 G_GAN_Feat: 2.996 G_VGG: 1.820 D_real: 0.108 D_fake: 0.008 \n",
            "(epoch: 15, iters: 2200, time: 0.157) G_GAN: 1.919 G_GAN_Feat: 3.935 G_VGG: 2.336 D_real: 0.009 D_fake: 0.012 \n",
            "saving the latest model (epoch 15, total_steps 61000)\n",
            "(epoch: 15, iters: 2300, time: 0.157) G_GAN: 2.065 G_GAN_Feat: 4.046 G_VGG: 2.396 D_real: 0.027 D_fake: 0.010 \n",
            "(epoch: 15, iters: 2400, time: 0.157) G_GAN: 2.002 G_GAN_Feat: 4.291 G_VGG: 2.244 D_real: 0.007 D_fake: 0.005 \n",
            "(epoch: 15, iters: 2500, time: 0.157) G_GAN: 1.823 G_GAN_Feat: 3.851 G_VGG: 2.335 D_real: 0.100 D_fake: 0.090 \n",
            "(epoch: 15, iters: 2600, time: 0.157) G_GAN: 1.959 G_GAN_Feat: 2.848 G_VGG: 1.842 D_real: 0.323 D_fake: 0.013 \n",
            "(epoch: 15, iters: 2700, time: 0.157) G_GAN: 1.999 G_GAN_Feat: 3.054 G_VGG: 1.683 D_real: 0.497 D_fake: 0.010 \n",
            "(epoch: 15, iters: 2800, time: 0.157) G_GAN: 1.722 G_GAN_Feat: 2.856 G_VGG: 1.817 D_real: 0.076 D_fake: 0.032 \n",
            "(epoch: 15, iters: 2900, time: 0.157) G_GAN: 1.963 G_GAN_Feat: 4.127 G_VGG: 2.706 D_real: 0.030 D_fake: 0.033 \n",
            "(epoch: 15, iters: 3000, time: 0.157) G_GAN: 1.643 G_GAN_Feat: 3.474 G_VGG: 1.816 D_real: 0.101 D_fake: 0.086 \n",
            "(epoch: 15, iters: 3100, time: 0.157) G_GAN: 1.972 G_GAN_Feat: 3.477 G_VGG: 2.045 D_real: 0.009 D_fake: 0.010 \n",
            "(epoch: 15, iters: 3200, time: 0.157) G_GAN: 1.496 G_GAN_Feat: 3.717 G_VGG: 1.847 D_real: 0.020 D_fake: 0.098 \n",
            "saving the latest model (epoch 15, total_steps 62000)\n",
            "(epoch: 15, iters: 3300, time: 0.157) G_GAN: 1.852 G_GAN_Feat: 3.809 G_VGG: 1.946 D_real: 0.109 D_fake: 0.013 \n",
            "(epoch: 15, iters: 3400, time: 0.157) G_GAN: 2.106 G_GAN_Feat: 3.637 G_VGG: 1.855 D_real: 0.009 D_fake: 0.007 \n",
            "(epoch: 15, iters: 3500, time: 0.157) G_GAN: 1.811 G_GAN_Feat: 3.914 G_VGG: 1.999 D_real: 0.035 D_fake: 0.040 \n",
            "(epoch: 15, iters: 3600, time: 0.157) G_GAN: 1.544 G_GAN_Feat: 3.601 G_VGG: 2.023 D_real: 0.289 D_fake: 0.136 \n",
            "(epoch: 15, iters: 3700, time: 0.157) G_GAN: 1.559 G_GAN_Feat: 3.454 G_VGG: 1.717 D_real: 0.016 D_fake: 0.104 \n",
            "(epoch: 15, iters: 3800, time: 0.157) G_GAN: 1.655 G_GAN_Feat: 3.359 G_VGG: 2.356 D_real: 0.016 D_fake: 0.039 \n",
            "(epoch: 15, iters: 3900, time: 0.157) G_GAN: 2.138 G_GAN_Feat: 3.797 G_VGG: 2.173 D_real: 0.138 D_fake: 0.016 \n",
            "(epoch: 15, iters: 4000, time: 0.157) G_GAN: 2.041 G_GAN_Feat: 3.594 G_VGG: 2.342 D_real: 0.009 D_fake: 0.009 \n",
            "(epoch: 15, iters: 4100, time: 0.157) G_GAN: 1.745 G_GAN_Feat: 3.959 G_VGG: 1.829 D_real: 0.081 D_fake: 0.109 \n",
            "(epoch: 15, iters: 4200, time: 0.157) G_GAN: 2.091 G_GAN_Feat: 3.427 G_VGG: 2.304 D_real: 0.017 D_fake: 0.010 \n",
            "saving the latest model (epoch 15, total_steps 63000)\n",
            "End of epoch 15 / 200 \t Time Taken: 680 sec\n",
            "(epoch: 16, iters: 100, time: 0.158) G_GAN: 2.047 G_GAN_Feat: 2.952 G_VGG: 2.002 D_real: 0.075 D_fake: 0.011 \n",
            "(epoch: 16, iters: 200, time: 0.157) G_GAN: 2.040 G_GAN_Feat: 3.305 G_VGG: 1.603 D_real: 0.080 D_fake: 0.062 \n",
            "(epoch: 16, iters: 300, time: 0.157) G_GAN: 1.676 G_GAN_Feat: 4.107 G_VGG: 2.054 D_real: 0.017 D_fake: 0.028 \n",
            "(epoch: 16, iters: 400, time: 0.157) G_GAN: 1.999 G_GAN_Feat: 2.848 G_VGG: 1.683 D_real: 0.016 D_fake: 0.030 \n",
            "(epoch: 16, iters: 500, time: 0.157) G_GAN: 2.077 G_GAN_Feat: 2.893 G_VGG: 1.535 D_real: 0.251 D_fake: 0.040 \n",
            "(epoch: 16, iters: 600, time: 0.157) G_GAN: 1.881 G_GAN_Feat: 3.487 G_VGG: 2.009 D_real: 0.170 D_fake: 0.008 \n",
            "(epoch: 16, iters: 700, time: 0.157) G_GAN: 1.919 G_GAN_Feat: 3.688 G_VGG: 2.129 D_real: 0.209 D_fake: 0.054 \n",
            "(epoch: 16, iters: 800, time: 0.157) G_GAN: 1.994 G_GAN_Feat: 2.805 G_VGG: 2.000 D_real: 0.021 D_fake: 0.010 \n",
            "(epoch: 16, iters: 900, time: 0.157) G_GAN: 1.966 G_GAN_Feat: 3.110 G_VGG: 1.581 D_real: 0.093 D_fake: 0.005 \n",
            "(epoch: 16, iters: 1000, time: 0.157) G_GAN: 1.478 G_GAN_Feat: 2.549 G_VGG: 1.237 D_real: 0.006 D_fake: 0.278 \n",
            "saving the latest model (epoch 16, total_steps 64000)\n",
            "(epoch: 16, iters: 1100, time: 0.157) G_GAN: 1.741 G_GAN_Feat: 3.800 G_VGG: 1.867 D_real: 0.015 D_fake: 0.054 \n",
            "(epoch: 16, iters: 1200, time: 0.157) G_GAN: 2.006 G_GAN_Feat: 3.621 G_VGG: 1.840 D_real: 0.405 D_fake: 0.025 \n",
            "(epoch: 16, iters: 1300, time: 0.157) G_GAN: 1.870 G_GAN_Feat: 3.439 G_VGG: 1.996 D_real: 0.091 D_fake: 0.015 \n",
            "(epoch: 16, iters: 1400, time: 0.157) G_GAN: 1.827 G_GAN_Feat: 3.114 G_VGG: 2.258 D_real: 0.045 D_fake: 0.039 \n",
            "(epoch: 16, iters: 1500, time: 0.157) G_GAN: 2.416 G_GAN_Feat: 3.417 G_VGG: 2.117 D_real: 0.089 D_fake: 0.041 \n",
            "(epoch: 16, iters: 1600, time: 0.157) G_GAN: 1.666 G_GAN_Feat: 3.846 G_VGG: 1.878 D_real: 0.168 D_fake: 0.094 \n",
            "(epoch: 16, iters: 1700, time: 0.157) G_GAN: 2.004 G_GAN_Feat: 2.852 G_VGG: 1.431 D_real: 0.179 D_fake: 0.032 \n",
            "(epoch: 16, iters: 1800, time: 0.157) G_GAN: 2.432 G_GAN_Feat: 2.948 G_VGG: 2.036 D_real: 0.118 D_fake: 0.051 \n",
            "(epoch: 16, iters: 1900, time: 0.157) G_GAN: 1.821 G_GAN_Feat: 2.600 G_VGG: 1.984 D_real: 0.478 D_fake: 0.068 \n",
            "(epoch: 16, iters: 2000, time: 0.157) G_GAN: 2.279 G_GAN_Feat: 3.673 G_VGG: 2.091 D_real: 0.207 D_fake: 0.018 \n",
            "saving the latest model (epoch 16, total_steps 65000)\n",
            "(epoch: 16, iters: 2100, time: 0.157) G_GAN: 1.539 G_GAN_Feat: 2.787 G_VGG: 1.556 D_real: 0.035 D_fake: 0.108 \n",
            "(epoch: 16, iters: 2200, time: 0.157) G_GAN: 2.189 G_GAN_Feat: 3.627 G_VGG: 2.034 D_real: 0.029 D_fake: 0.037 \n",
            "(epoch: 16, iters: 2300, time: 0.157) G_GAN: 1.836 G_GAN_Feat: 3.173 G_VGG: 1.828 D_real: 0.035 D_fake: 0.047 \n",
            "(epoch: 16, iters: 2400, time: 0.157) G_GAN: 2.038 G_GAN_Feat: 3.100 G_VGG: 2.048 D_real: 0.082 D_fake: 0.008 \n",
            "(epoch: 16, iters: 2500, time: 0.157) G_GAN: 1.802 G_GAN_Feat: 3.485 G_VGG: 1.833 D_real: 0.077 D_fake: 0.069 \n",
            "(epoch: 16, iters: 2600, time: 0.157) G_GAN: 1.689 G_GAN_Feat: 3.021 G_VGG: 1.656 D_real: 0.011 D_fake: 0.038 \n",
            "(epoch: 16, iters: 2700, time: 0.157) G_GAN: 2.135 G_GAN_Feat: 3.361 G_VGG: 1.797 D_real: 0.326 D_fake: 0.009 \n",
            "(epoch: 16, iters: 2800, time: 0.157) G_GAN: 1.548 G_GAN_Feat: 3.427 G_VGG: 1.752 D_real: 0.183 D_fake: 0.089 \n",
            "(epoch: 16, iters: 2900, time: 0.157) G_GAN: 2.116 G_GAN_Feat: 3.323 G_VGG: 2.407 D_real: 0.013 D_fake: 0.017 \n",
            "(epoch: 16, iters: 3000, time: 0.157) G_GAN: 2.062 G_GAN_Feat: 3.418 G_VGG: 2.214 D_real: 0.163 D_fake: 0.032 \n",
            "saving the latest model (epoch 16, total_steps 66000)\n",
            "(epoch: 16, iters: 3100, time: 0.157) G_GAN: 1.878 G_GAN_Feat: 3.762 G_VGG: 2.600 D_real: 0.033 D_fake: 0.011 \n",
            "(epoch: 16, iters: 3200, time: 0.157) G_GAN: 1.613 G_GAN_Feat: 2.550 G_VGG: 1.941 D_real: 0.009 D_fake: 0.061 \n",
            "(epoch: 16, iters: 3300, time: 0.157) G_GAN: 1.824 G_GAN_Feat: 3.403 G_VGG: 1.925 D_real: 0.019 D_fake: 0.018 \n",
            "(epoch: 16, iters: 3400, time: 0.157) G_GAN: 1.952 G_GAN_Feat: 3.927 G_VGG: 2.025 D_real: 0.033 D_fake: 0.005 \n",
            "(epoch: 16, iters: 3500, time: 0.157) G_GAN: 1.310 G_GAN_Feat: 3.019 G_VGG: 1.741 D_real: 0.029 D_fake: 0.098 \n",
            "(epoch: 16, iters: 3600, time: 0.157) G_GAN: 2.090 G_GAN_Feat: 3.941 G_VGG: 1.693 D_real: 0.279 D_fake: 0.005 \n",
            "(epoch: 16, iters: 3700, time: 0.157) G_GAN: 1.688 G_GAN_Feat: 2.605 G_VGG: 1.805 D_real: 0.032 D_fake: 0.107 \n",
            "(epoch: 16, iters: 3800, time: 0.157) G_GAN: 2.014 G_GAN_Feat: 3.530 G_VGG: 2.069 D_real: 0.040 D_fake: 0.030 \n",
            "(epoch: 16, iters: 3900, time: 0.157) G_GAN: 1.935 G_GAN_Feat: 3.258 G_VGG: 1.722 D_real: 0.076 D_fake: 0.013 \n",
            "(epoch: 16, iters: 4000, time: 0.157) G_GAN: 1.860 G_GAN_Feat: 3.407 G_VGG: 1.677 D_real: 0.158 D_fake: 0.031 \n",
            "saving the latest model (epoch 16, total_steps 67000)\n",
            "(epoch: 16, iters: 4100, time: 0.157) G_GAN: 2.032 G_GAN_Feat: 3.176 G_VGG: 2.170 D_real: 0.093 D_fake: 0.006 \n",
            "(epoch: 16, iters: 4200, time: 0.157) G_GAN: 1.596 G_GAN_Feat: 3.242 G_VGG: 1.942 D_real: 0.025 D_fake: 0.051 \n",
            "End of epoch 16 / 200 \t Time Taken: 676 sec\n",
            "(epoch: 17, iters: 100, time: 0.157) G_GAN: 1.966 G_GAN_Feat: 3.123 G_VGG: 1.669 D_real: 0.339 D_fake: 0.008 \n",
            "(epoch: 17, iters: 200, time: 0.157) G_GAN: 1.744 G_GAN_Feat: 3.043 G_VGG: 2.248 D_real: 0.019 D_fake: 0.053 \n",
            "(epoch: 17, iters: 300, time: 0.157) G_GAN: 1.686 G_GAN_Feat: 3.212 G_VGG: 1.650 D_real: 0.007 D_fake: 0.073 \n",
            "(epoch: 17, iters: 400, time: 0.157) G_GAN: 1.996 G_GAN_Feat: 2.975 G_VGG: 2.254 D_real: 0.009 D_fake: 0.010 \n",
            "(epoch: 17, iters: 500, time: 0.157) G_GAN: 2.063 G_GAN_Feat: 3.353 G_VGG: 2.222 D_real: 0.022 D_fake: 0.009 \n",
            "(epoch: 17, iters: 600, time: 0.157) G_GAN: 1.874 G_GAN_Feat: 3.157 G_VGG: 2.138 D_real: 0.007 D_fake: 0.025 \n",
            "(epoch: 17, iters: 700, time: 0.157) G_GAN: 1.956 G_GAN_Feat: 3.670 G_VGG: 2.480 D_real: 0.094 D_fake: 0.005 \n",
            "(epoch: 17, iters: 800, time: 0.157) G_GAN: 1.736 G_GAN_Feat: 3.460 G_VGG: 1.979 D_real: 0.008 D_fake: 0.043 \n",
            "saving the latest model (epoch 17, total_steps 68000)\n",
            "(epoch: 17, iters: 900, time: 0.157) G_GAN: 1.905 G_GAN_Feat: 3.054 G_VGG: 1.912 D_real: 0.022 D_fake: 0.017 \n",
            "(epoch: 17, iters: 1000, time: 0.157) G_GAN: 1.627 G_GAN_Feat: 4.359 G_VGG: 2.226 D_real: 0.087 D_fake: 0.052 \n",
            "(epoch: 17, iters: 1100, time: 0.157) G_GAN: 1.798 G_GAN_Feat: 2.806 G_VGG: 1.833 D_real: 0.010 D_fake: 0.024 \n",
            "(epoch: 17, iters: 1200, time: 0.157) G_GAN: 1.850 G_GAN_Feat: 2.824 G_VGG: 1.625 D_real: 0.116 D_fake: 0.028 \n",
            "(epoch: 17, iters: 1300, time: 0.157) G_GAN: 2.232 G_GAN_Feat: 3.839 G_VGG: 2.015 D_real: 1.092 D_fake: 0.056 \n",
            "(epoch: 17, iters: 1400, time: 0.157) G_GAN: 1.264 G_GAN_Feat: 3.379 G_VGG: 1.943 D_real: 0.070 D_fake: 0.342 \n",
            "(epoch: 17, iters: 1500, time: 0.157) G_GAN: 1.578 G_GAN_Feat: 3.123 G_VGG: 2.010 D_real: 0.011 D_fake: 0.179 \n",
            "(epoch: 17, iters: 1600, time: 0.157) G_GAN: 2.069 G_GAN_Feat: 3.631 G_VGG: 2.197 D_real: 0.270 D_fake: 0.007 \n",
            "(epoch: 17, iters: 1700, time: 0.157) G_GAN: 2.124 G_GAN_Feat: 3.928 G_VGG: 1.986 D_real: 0.119 D_fake: 0.008 \n",
            "(epoch: 17, iters: 1800, time: 0.157) G_GAN: 1.231 G_GAN_Feat: 3.123 G_VGG: 1.886 D_real: 0.015 D_fake: 0.219 \n",
            "saving the latest model (epoch 17, total_steps 69000)\n",
            "(epoch: 17, iters: 1900, time: 0.157) G_GAN: 1.786 G_GAN_Feat: 3.855 G_VGG: 2.057 D_real: 0.254 D_fake: 0.037 \n",
            "(epoch: 17, iters: 2000, time: 0.157) G_GAN: 1.661 G_GAN_Feat: 3.046 G_VGG: 2.024 D_real: 0.193 D_fake: 0.049 \n",
            "(epoch: 17, iters: 2100, time: 0.157) G_GAN: 1.605 G_GAN_Feat: 2.977 G_VGG: 1.869 D_real: 0.012 D_fake: 0.044 \n",
            "(epoch: 17, iters: 2200, time: 0.157) G_GAN: 1.818 G_GAN_Feat: 3.528 G_VGG: 2.421 D_real: 0.014 D_fake: 0.012 \n",
            "(epoch: 17, iters: 2300, time: 0.157) G_GAN: 1.842 G_GAN_Feat: 2.669 G_VGG: 1.549 D_real: 0.394 D_fake: 0.035 \n",
            "(epoch: 17, iters: 2400, time: 0.157) G_GAN: 1.733 G_GAN_Feat: 3.007 G_VGG: 2.307 D_real: 0.040 D_fake: 0.023 \n",
            "(epoch: 17, iters: 2500, time: 0.157) G_GAN: 1.747 G_GAN_Feat: 3.541 G_VGG: 2.394 D_real: 0.021 D_fake: 0.022 \n",
            "(epoch: 17, iters: 2600, time: 0.157) G_GAN: 1.675 G_GAN_Feat: 3.175 G_VGG: 2.051 D_real: 0.030 D_fake: 0.033 \n",
            "(epoch: 17, iters: 2700, time: 0.157) G_GAN: 1.589 G_GAN_Feat: 3.144 G_VGG: 1.929 D_real: 0.156 D_fake: 0.096 \n",
            "(epoch: 17, iters: 2800, time: 0.157) G_GAN: 1.869 G_GAN_Feat: 3.121 G_VGG: 1.436 D_real: 0.023 D_fake: 0.082 \n",
            "saving the latest model (epoch 17, total_steps 70000)\n",
            "(epoch: 17, iters: 2900, time: 0.157) G_GAN: 2.290 G_GAN_Feat: 3.520 G_VGG: 2.268 D_real: 0.134 D_fake: 0.022 \n",
            "(epoch: 17, iters: 3000, time: 0.157) G_GAN: 1.963 G_GAN_Feat: 2.910 G_VGG: 1.695 D_real: 0.028 D_fake: 0.053 \n",
            "(epoch: 17, iters: 3100, time: 0.157) G_GAN: 1.803 G_GAN_Feat: 3.966 G_VGG: 1.913 D_real: 0.009 D_fake: 0.036 \n",
            "(epoch: 17, iters: 3200, time: 0.157) G_GAN: 1.787 G_GAN_Feat: 2.874 G_VGG: 1.331 D_real: 0.037 D_fake: 0.096 \n",
            "(epoch: 17, iters: 3300, time: 0.157) G_GAN: 2.197 G_GAN_Feat: 3.850 G_VGG: 1.929 D_real: 0.055 D_fake: 0.035 \n",
            "(epoch: 17, iters: 3400, time: 0.157) G_GAN: 1.935 G_GAN_Feat: 2.956 G_VGG: 1.617 D_real: 0.037 D_fake: 0.020 \n",
            "(epoch: 17, iters: 3500, time: 0.157) G_GAN: 1.832 G_GAN_Feat: 3.698 G_VGG: 1.979 D_real: 0.009 D_fake: 0.052 \n",
            "(epoch: 17, iters: 3600, time: 0.157) G_GAN: 2.080 G_GAN_Feat: 3.481 G_VGG: 2.258 D_real: 0.077 D_fake: 0.013 \n",
            "(epoch: 17, iters: 3700, time: 0.157) G_GAN: 1.787 G_GAN_Feat: 4.139 G_VGG: 2.059 D_real: 0.034 D_fake: 0.058 \n",
            "(epoch: 17, iters: 3800, time: 0.157) G_GAN: 1.798 G_GAN_Feat: 2.479 G_VGG: 1.424 D_real: 0.068 D_fake: 0.130 \n",
            "saving the latest model (epoch 17, total_steps 71000)\n",
            "(epoch: 17, iters: 3900, time: 0.157) G_GAN: 1.996 G_GAN_Feat: 3.781 G_VGG: 1.702 D_real: 0.032 D_fake: 0.014 \n",
            "(epoch: 17, iters: 4000, time: 0.157) G_GAN: 1.874 G_GAN_Feat: 3.301 G_VGG: 1.895 D_real: 0.009 D_fake: 0.008 \n",
            "(epoch: 17, iters: 4100, time: 0.157) G_GAN: 2.267 G_GAN_Feat: 3.423 G_VGG: 2.046 D_real: 0.135 D_fake: 0.015 \n",
            "(epoch: 17, iters: 4200, time: 0.157) G_GAN: 2.125 G_GAN_Feat: 3.523 G_VGG: 2.095 D_real: 0.014 D_fake: 0.013 \n",
            "End of epoch 17 / 200 \t Time Taken: 677 sec\n",
            "(epoch: 18, iters: 100, time: 0.157) G_GAN: 1.974 G_GAN_Feat: 3.792 G_VGG: 2.341 D_real: 0.007 D_fake: 0.006 \n",
            "(epoch: 18, iters: 200, time: 0.157) G_GAN: 2.054 G_GAN_Feat: 3.397 G_VGG: 2.261 D_real: 0.009 D_fake: 0.007 \n",
            "(epoch: 18, iters: 300, time: 0.157) G_GAN: 1.645 G_GAN_Feat: 3.418 G_VGG: 1.979 D_real: 0.012 D_fake: 0.060 \n",
            "(epoch: 18, iters: 400, time: 0.157) G_GAN: 2.022 G_GAN_Feat: 3.584 G_VGG: 2.485 D_real: 0.037 D_fake: 0.017 \n",
            "(epoch: 18, iters: 500, time: 0.157) G_GAN: 1.826 G_GAN_Feat: 2.748 G_VGG: 1.577 D_real: 0.249 D_fake: 0.013 \n",
            "(epoch: 18, iters: 600, time: 0.157) G_GAN: 1.228 G_GAN_Feat: 3.282 G_VGG: 1.833 D_real: 0.255 D_fake: 0.234 \n",
            "saving the latest model (epoch 18, total_steps 72000)\n",
            "(epoch: 18, iters: 700, time: 0.157) G_GAN: 1.598 G_GAN_Feat: 3.394 G_VGG: 1.846 D_real: 0.013 D_fake: 0.170 \n",
            "(epoch: 18, iters: 800, time: 0.157) G_GAN: 1.518 G_GAN_Feat: 3.089 G_VGG: 1.521 D_real: 0.012 D_fake: 0.072 \n",
            "(epoch: 18, iters: 900, time: 0.157) G_GAN: 2.008 G_GAN_Feat: 3.107 G_VGG: 1.596 D_real: 0.075 D_fake: 0.115 \n",
            "(epoch: 18, iters: 1000, time: 0.157) G_GAN: 2.414 G_GAN_Feat: 2.873 G_VGG: 2.057 D_real: 0.360 D_fake: 0.068 \n",
            "(epoch: 18, iters: 1100, time: 0.157) G_GAN: 1.348 G_GAN_Feat: 3.295 G_VGG: 2.097 D_real: 0.010 D_fake: 0.202 \n",
            "(epoch: 18, iters: 1200, time: 0.157) G_GAN: 1.832 G_GAN_Feat: 3.561 G_VGG: 1.822 D_real: 0.008 D_fake: 0.010 \n",
            "(epoch: 18, iters: 1300, time: 0.157) G_GAN: 1.939 G_GAN_Feat: 3.532 G_VGG: 2.237 D_real: 0.064 D_fake: 0.023 \n",
            "(epoch: 18, iters: 1400, time: 0.157) G_GAN: 2.233 G_GAN_Feat: 3.701 G_VGG: 2.711 D_real: 0.022 D_fake: 0.015 \n",
            "(epoch: 18, iters: 1500, time: 0.157) G_GAN: 1.899 G_GAN_Feat: 3.177 G_VGG: 1.912 D_real: 0.008 D_fake: 0.051 \n",
            "(epoch: 18, iters: 1600, time: 0.157) G_GAN: 1.257 G_GAN_Feat: 2.940 G_VGG: 1.966 D_real: 0.368 D_fake: 0.254 \n",
            "saving the latest model (epoch 18, total_steps 73000)\n",
            "(epoch: 18, iters: 1700, time: 0.157) G_GAN: 1.799 G_GAN_Feat: 3.008 G_VGG: 1.627 D_real: 0.127 D_fake: 0.166 \n",
            "(epoch: 18, iters: 1800, time: 0.157) G_GAN: 1.870 G_GAN_Feat: 3.026 G_VGG: 1.717 D_real: 0.022 D_fake: 0.058 \n",
            "(epoch: 18, iters: 1900, time: 0.157) G_GAN: 1.530 G_GAN_Feat: 2.590 G_VGG: 1.677 D_real: 0.010 D_fake: 0.121 \n",
            "(epoch: 18, iters: 2000, time: 0.157) G_GAN: 2.229 G_GAN_Feat: 3.836 G_VGG: 2.127 D_real: 0.035 D_fake: 0.013 \n",
            "(epoch: 18, iters: 2100, time: 0.157) G_GAN: 2.090 G_GAN_Feat: 3.557 G_VGG: 1.815 D_real: 0.026 D_fake: 0.019 \n",
            "(epoch: 18, iters: 2200, time: 0.157) G_GAN: 1.865 G_GAN_Feat: 3.417 G_VGG: 2.026 D_real: 0.010 D_fake: 0.030 \n",
            "(epoch: 18, iters: 2300, time: 0.157) G_GAN: 1.206 G_GAN_Feat: 2.782 G_VGG: 2.042 D_real: 0.375 D_fake: 0.305 \n",
            "(epoch: 18, iters: 2400, time: 0.157) G_GAN: 1.360 G_GAN_Feat: 2.189 G_VGG: 1.719 D_real: 0.261 D_fake: 0.266 \n",
            "(epoch: 18, iters: 2500, time: 0.157) G_GAN: 1.287 G_GAN_Feat: 2.098 G_VGG: 2.052 D_real: 0.248 D_fake: 0.249 \n",
            "(epoch: 18, iters: 2600, time: 0.157) G_GAN: 1.097 G_GAN_Feat: 2.758 G_VGG: 1.709 D_real: 0.413 D_fake: 0.233 \n",
            "saving the latest model (epoch 18, total_steps 74000)\n",
            "(epoch: 18, iters: 2700, time: 0.157) G_GAN: 1.734 G_GAN_Feat: 3.266 G_VGG: 2.226 D_real: 0.385 D_fake: 0.072 \n",
            "(epoch: 18, iters: 2800, time: 0.157) G_GAN: 1.492 G_GAN_Feat: 2.263 G_VGG: 1.696 D_real: 0.023 D_fake: 0.089 \n",
            "(epoch: 18, iters: 2900, time: 0.157) G_GAN: 1.909 G_GAN_Feat: 3.080 G_VGG: 2.036 D_real: 0.047 D_fake: 0.030 \n",
            "(epoch: 18, iters: 3000, time: 0.157) G_GAN: 1.817 G_GAN_Feat: 3.643 G_VGG: 2.012 D_real: 0.163 D_fake: 0.032 \n",
            "(epoch: 18, iters: 3100, time: 0.157) G_GAN: 2.026 G_GAN_Feat: 3.456 G_VGG: 1.888 D_real: 0.018 D_fake: 0.012 \n",
            "(epoch: 18, iters: 3200, time: 0.157) G_GAN: 1.473 G_GAN_Feat: 2.718 G_VGG: 1.817 D_real: 0.012 D_fake: 0.202 \n",
            "(epoch: 18, iters: 3300, time: 0.157) G_GAN: 1.818 G_GAN_Feat: 2.666 G_VGG: 2.070 D_real: 0.560 D_fake: 0.015 \n",
            "(epoch: 18, iters: 3400, time: 0.157) G_GAN: 1.843 G_GAN_Feat: 3.185 G_VGG: 1.354 D_real: 0.007 D_fake: 0.009 \n",
            "(epoch: 18, iters: 3500, time: 0.157) G_GAN: 1.940 G_GAN_Feat: 3.429 G_VGG: 2.161 D_real: 0.006 D_fake: 0.008 \n",
            "(epoch: 18, iters: 3600, time: 0.157) G_GAN: 1.200 G_GAN_Feat: 2.863 G_VGG: 1.954 D_real: 0.299 D_fake: 0.240 \n",
            "saving the latest model (epoch 18, total_steps 75000)\n",
            "(epoch: 18, iters: 3700, time: 0.157) G_GAN: 1.689 G_GAN_Feat: 3.008 G_VGG: 2.025 D_real: 0.007 D_fake: 0.087 \n",
            "(epoch: 18, iters: 3800, time: 0.157) G_GAN: 1.979 G_GAN_Feat: 3.112 G_VGG: 2.198 D_real: 0.074 D_fake: 0.017 \n",
            "(epoch: 18, iters: 3900, time: 0.157) G_GAN: 1.609 G_GAN_Feat: 3.546 G_VGG: 1.719 D_real: 0.018 D_fake: 0.045 \n",
            "(epoch: 18, iters: 4000, time: 0.157) G_GAN: 1.009 G_GAN_Feat: 3.128 G_VGG: 2.119 D_real: 0.042 D_fake: 0.307 \n",
            "(epoch: 18, iters: 4100, time: 0.157) G_GAN: 2.022 G_GAN_Feat: 3.893 G_VGG: 2.004 D_real: 0.010 D_fake: 0.007 \n",
            "(epoch: 18, iters: 4200, time: 0.157) G_GAN: 1.885 G_GAN_Feat: 3.159 G_VGG: 1.749 D_real: 0.007 D_fake: 0.016 \n",
            "End of epoch 18 / 200 \t Time Taken: 676 sec\n",
            "(epoch: 19, iters: 100, time: 0.157) G_GAN: 1.743 G_GAN_Feat: 2.606 G_VGG: 1.629 D_real: 0.021 D_fake: 0.028 \n",
            "(epoch: 19, iters: 200, time: 0.157) G_GAN: 1.977 G_GAN_Feat: 3.298 G_VGG: 1.805 D_real: 0.005 D_fake: 0.003 \n",
            "(epoch: 19, iters: 300, time: 0.157) G_GAN: 1.864 G_GAN_Feat: 4.049 G_VGG: 2.893 D_real: 0.013 D_fake: 0.112 \n",
            "(epoch: 19, iters: 400, time: 0.157) G_GAN: 1.668 G_GAN_Feat: 2.865 G_VGG: 1.768 D_real: 0.210 D_fake: 0.105 \n",
            "saving the latest model (epoch 19, total_steps 76000)\n",
            "(epoch: 19, iters: 500, time: 0.157) G_GAN: 1.120 G_GAN_Feat: 2.594 G_VGG: 1.744 D_real: 0.186 D_fake: 0.252 \n",
            "(epoch: 19, iters: 600, time: 0.157) G_GAN: 1.820 G_GAN_Feat: 2.945 G_VGG: 1.597 D_real: 0.009 D_fake: 0.121 \n",
            "(epoch: 19, iters: 700, time: 0.157) G_GAN: 1.818 G_GAN_Feat: 2.908 G_VGG: 1.988 D_real: 0.133 D_fake: 0.026 \n",
            "(epoch: 19, iters: 800, time: 0.157) G_GAN: 2.051 G_GAN_Feat: 4.060 G_VGG: 1.728 D_real: 0.008 D_fake: 0.005 \n",
            "(epoch: 19, iters: 900, time: 0.157) G_GAN: 1.800 G_GAN_Feat: 3.238 G_VGG: 2.350 D_real: 0.029 D_fake: 0.027 \n",
            "(epoch: 19, iters: 1000, time: 0.157) G_GAN: 1.915 G_GAN_Feat: 2.942 G_VGG: 1.897 D_real: 0.020 D_fake: 0.094 \n",
            "(epoch: 19, iters: 1100, time: 0.157) G_GAN: 1.766 G_GAN_Feat: 2.792 G_VGG: 1.877 D_real: 0.007 D_fake: 0.063 \n",
            "(epoch: 19, iters: 1200, time: 0.157) G_GAN: 2.095 G_GAN_Feat: 2.923 G_VGG: 1.923 D_real: 0.063 D_fake: 0.013 \n",
            "(epoch: 19, iters: 1300, time: 0.157) G_GAN: 1.642 G_GAN_Feat: 3.337 G_VGG: 1.985 D_real: 0.069 D_fake: 0.175 \n",
            "(epoch: 19, iters: 1400, time: 0.157) G_GAN: 2.099 G_GAN_Feat: 2.792 G_VGG: 1.258 D_real: 0.060 D_fake: 0.013 \n",
            "saving the latest model (epoch 19, total_steps 77000)\n",
            "(epoch: 19, iters: 1500, time: 0.157) G_GAN: 2.056 G_GAN_Feat: 2.862 G_VGG: 1.671 D_real: 0.007 D_fake: 0.009 \n",
            "(epoch: 19, iters: 1600, time: 0.157) G_GAN: 2.069 G_GAN_Feat: 2.910 G_VGG: 1.502 D_real: 0.099 D_fake: 0.023 \n",
            "(epoch: 19, iters: 1700, time: 0.157) G_GAN: 1.251 G_GAN_Feat: 3.457 G_VGG: 1.721 D_real: 0.010 D_fake: 0.326 \n",
            "(epoch: 19, iters: 1800, time: 0.157) G_GAN: 1.802 G_GAN_Feat: 3.734 G_VGG: 1.892 D_real: 0.021 D_fake: 0.035 \n",
            "(epoch: 19, iters: 1900, time: 0.157) G_GAN: 1.783 G_GAN_Feat: 3.451 G_VGG: 2.166 D_real: 0.013 D_fake: 0.026 \n",
            "(epoch: 19, iters: 2000, time: 0.157) G_GAN: 1.671 G_GAN_Feat: 3.077 G_VGG: 1.867 D_real: 0.027 D_fake: 0.030 \n",
            "(epoch: 19, iters: 2100, time: 0.157) G_GAN: 1.615 G_GAN_Feat: 3.010 G_VGG: 1.977 D_real: 0.206 D_fake: 0.107 \n",
            "(epoch: 19, iters: 2200, time: 0.157) G_GAN: 1.961 G_GAN_Feat: 3.116 G_VGG: 1.763 D_real: 0.011 D_fake: 0.013 \n",
            "(epoch: 19, iters: 2300, time: 0.157) G_GAN: 1.616 G_GAN_Feat: 3.102 G_VGG: 1.934 D_real: 0.129 D_fake: 0.052 \n",
            "(epoch: 19, iters: 2400, time: 0.157) G_GAN: 1.505 G_GAN_Feat: 2.795 G_VGG: 1.573 D_real: 0.017 D_fake: 0.135 \n",
            "saving the latest model (epoch 19, total_steps 78000)\n",
            "(epoch: 19, iters: 2500, time: 0.157) G_GAN: 1.428 G_GAN_Feat: 3.057 G_VGG: 1.911 D_real: 0.108 D_fake: 0.125 \n",
            "(epoch: 19, iters: 2600, time: 0.157) G_GAN: 1.669 G_GAN_Feat: 2.830 G_VGG: 1.532 D_real: 0.017 D_fake: 0.142 \n",
            "(epoch: 19, iters: 2700, time: 0.157) G_GAN: 2.002 G_GAN_Feat: 3.214 G_VGG: 1.912 D_real: 0.008 D_fake: 0.004 \n",
            "(epoch: 19, iters: 2800, time: 0.157) G_GAN: 1.692 G_GAN_Feat: 3.166 G_VGG: 1.907 D_real: 0.283 D_fake: 0.024 \n",
            "(epoch: 19, iters: 2900, time: 0.157) G_GAN: 1.540 G_GAN_Feat: 2.744 G_VGG: 1.508 D_real: 0.012 D_fake: 0.114 \n",
            "(epoch: 19, iters: 3000, time: 0.157) G_GAN: 2.191 G_GAN_Feat: 3.071 G_VGG: 1.957 D_real: 0.028 D_fake: 0.016 \n",
            "(epoch: 19, iters: 3100, time: 0.157) G_GAN: 2.034 G_GAN_Feat: 3.016 G_VGG: 1.553 D_real: 0.006 D_fake: 0.006 \n",
            "(epoch: 19, iters: 3200, time: 0.157) G_GAN: 2.154 G_GAN_Feat: 2.909 G_VGG: 1.890 D_real: 0.264 D_fake: 0.021 \n",
            "(epoch: 19, iters: 3300, time: 0.157) G_GAN: 1.970 G_GAN_Feat: 2.906 G_VGG: 2.067 D_real: 0.009 D_fake: 0.012 \n",
            "(epoch: 19, iters: 3400, time: 0.157) G_GAN: 1.815 G_GAN_Feat: 3.160 G_VGG: 1.532 D_real: 0.009 D_fake: 0.012 \n",
            "saving the latest model (epoch 19, total_steps 79000)\n",
            "(epoch: 19, iters: 3500, time: 0.157) G_GAN: 1.948 G_GAN_Feat: 3.096 G_VGG: 1.891 D_real: 0.006 D_fake: 0.013 \n",
            "(epoch: 19, iters: 3600, time: 0.157) G_GAN: 1.639 G_GAN_Feat: 2.667 G_VGG: 1.746 D_real: 0.014 D_fake: 0.163 \n",
            "(epoch: 19, iters: 3700, time: 0.157) G_GAN: 1.448 G_GAN_Feat: 3.101 G_VGG: 1.968 D_real: 0.066 D_fake: 0.140 \n",
            "(epoch: 19, iters: 3800, time: 0.157) G_GAN: 1.919 G_GAN_Feat: 3.443 G_VGG: 1.851 D_real: 0.059 D_fake: 0.010 \n",
            "(epoch: 19, iters: 3900, time: 0.157) G_GAN: 1.760 G_GAN_Feat: 2.759 G_VGG: 2.086 D_real: 0.012 D_fake: 0.052 \n",
            "(epoch: 19, iters: 4000, time: 0.157) G_GAN: 1.610 G_GAN_Feat: 2.428 G_VGG: 1.440 D_real: 0.014 D_fake: 0.278 \n",
            "(epoch: 19, iters: 4100, time: 0.157) G_GAN: 1.692 G_GAN_Feat: 2.864 G_VGG: 1.858 D_real: 0.005 D_fake: 0.121 \n",
            "(epoch: 19, iters: 4200, time: 0.157) G_GAN: 1.806 G_GAN_Feat: 3.407 G_VGG: 2.081 D_real: 0.015 D_fake: 0.022 \n",
            "End of epoch 19 / 200 \t Time Taken: 674 sec\n",
            "(epoch: 20, iters: 100, time: 0.157) G_GAN: 1.294 G_GAN_Feat: 2.983 G_VGG: 1.915 D_real: 0.013 D_fake: 0.316 \n",
            "(epoch: 20, iters: 200, time: 0.157) G_GAN: 1.886 G_GAN_Feat: 2.827 G_VGG: 1.908 D_real: 0.023 D_fake: 0.008 \n",
            "saving the latest model (epoch 20, total_steps 80000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia_high --dataroot ./datasets/nebbia_high --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "QnrdU2E9C0hN",
        "outputId": "4dbcc130-5193-4c86-feda-ee3ebc29259b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia_high\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_high\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 20 at iteration 1200\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:02<00:00, 284MB/s]\n",
            "create web directory ./checkpoints/nebbia_high/web...\n",
            "(epoch: 20, iters: 1300, time: 2.153) G_GAN: 1.593 G_GAN_Feat: 2.792 G_VGG: 1.890 D_real: 0.400 D_fake: 0.147 \n",
            "(epoch: 20, iters: 1400, time: 0.532) G_GAN: 1.589 G_GAN_Feat: 3.059 G_VGG: 1.858 D_real: 0.199 D_fake: 0.175 \n",
            "(epoch: 20, iters: 1500, time: 0.546) G_GAN: 1.519 G_GAN_Feat: 3.343 G_VGG: 2.250 D_real: 0.119 D_fake: 0.235 \n",
            "(epoch: 20, iters: 1600, time: 0.557) G_GAN: 1.944 G_GAN_Feat: 3.474 G_VGG: 2.097 D_real: 0.070 D_fake: 0.019 \n",
            "(epoch: 20, iters: 1700, time: 0.564) G_GAN: 1.840 G_GAN_Feat: 2.922 G_VGG: 2.247 D_real: 0.844 D_fake: 0.089 \n",
            "(epoch: 20, iters: 1800, time: 0.562) G_GAN: 1.776 G_GAN_Feat: 2.835 G_VGG: 1.703 D_real: 0.261 D_fake: 0.064 \n",
            "(epoch: 20, iters: 1900, time: 0.560) G_GAN: 1.585 G_GAN_Feat: 3.170 G_VGG: 2.038 D_real: 0.172 D_fake: 0.054 \n",
            "(epoch: 20, iters: 2000, time: 0.561) G_GAN: 1.843 G_GAN_Feat: 3.308 G_VGG: 1.751 D_real: 0.038 D_fake: 0.062 \n",
            "(epoch: 20, iters: 2100, time: 0.562) G_GAN: 1.148 G_GAN_Feat: 2.627 G_VGG: 1.330 D_real: 0.108 D_fake: 0.355 \n",
            "(epoch: 20, iters: 2200, time: 0.564) G_GAN: 1.436 G_GAN_Feat: 3.632 G_VGG: 2.177 D_real: 0.034 D_fake: 0.143 \n",
            "saving the latest model (epoch 20, total_steps 82000)\n",
            "(epoch: 20, iters: 2300, time: 0.561) G_GAN: 1.983 G_GAN_Feat: 3.515 G_VGG: 2.000 D_real: 0.096 D_fake: 0.025 \n",
            "(epoch: 20, iters: 2400, time: 0.561) G_GAN: 1.324 G_GAN_Feat: 3.073 G_VGG: 1.928 D_real: 0.061 D_fake: 0.192 \n",
            "(epoch: 20, iters: 2500, time: 0.562) G_GAN: 1.905 G_GAN_Feat: 3.344 G_VGG: 1.983 D_real: 0.086 D_fake: 0.040 \n",
            "(epoch: 20, iters: 2600, time: 0.562) G_GAN: 1.287 G_GAN_Feat: 3.186 G_VGG: 2.400 D_real: 0.169 D_fake: 0.255 \n",
            "(epoch: 20, iters: 2700, time: 0.562) G_GAN: 1.692 G_GAN_Feat: 3.494 G_VGG: 2.072 D_real: 0.035 D_fake: 0.082 \n",
            "(epoch: 20, iters: 2800, time: 0.562) G_GAN: 2.030 G_GAN_Feat: 3.164 G_VGG: 1.827 D_real: 0.737 D_fake: 0.126 \n",
            "(epoch: 20, iters: 2900, time: 0.562) G_GAN: 1.731 G_GAN_Feat: 2.726 G_VGG: 1.647 D_real: 0.166 D_fake: 0.056 \n",
            "(epoch: 20, iters: 3000, time: 0.562) G_GAN: 1.623 G_GAN_Feat: 3.012 G_VGG: 1.614 D_real: 0.207 D_fake: 0.127 \n",
            "(epoch: 20, iters: 3100, time: 0.563) G_GAN: 1.810 G_GAN_Feat: 3.015 G_VGG: 1.920 D_real: 0.485 D_fake: 0.046 \n",
            "(epoch: 20, iters: 3200, time: 0.564) G_GAN: 1.427 G_GAN_Feat: 2.891 G_VGG: 1.833 D_real: 0.047 D_fake: 0.142 \n",
            "saving the latest model (epoch 20, total_steps 83000)\n",
            "(epoch: 20, iters: 3300, time: 0.561) G_GAN: 1.634 G_GAN_Feat: 3.212 G_VGG: 1.694 D_real: 0.031 D_fake: 0.091 \n",
            "(epoch: 20, iters: 3400, time: 0.563) G_GAN: 2.016 G_GAN_Feat: 3.539 G_VGG: 1.941 D_real: 0.168 D_fake: 0.022 \n",
            "(epoch: 20, iters: 3500, time: 0.564) G_GAN: 1.978 G_GAN_Feat: 3.340 G_VGG: 1.633 D_real: 0.061 D_fake: 0.231 \n",
            "(epoch: 20, iters: 3600, time: 0.564) G_GAN: 1.492 G_GAN_Feat: 3.345 G_VGG: 1.875 D_real: 0.075 D_fake: 0.197 \n",
            "(epoch: 20, iters: 3700, time: 0.565) G_GAN: 1.867 G_GAN_Feat: 3.257 G_VGG: 2.151 D_real: 0.212 D_fake: 0.093 \n",
            "(epoch: 20, iters: 3800, time: 0.563) G_GAN: 1.805 G_GAN_Feat: 3.492 G_VGG: 1.680 D_real: 0.102 D_fake: 0.094 \n",
            "(epoch: 20, iters: 3900, time: 0.562) G_GAN: 1.454 G_GAN_Feat: 3.263 G_VGG: 1.784 D_real: 0.020 D_fake: 0.321 \n",
            "(epoch: 20, iters: 4000, time: 0.561) G_GAN: 1.573 G_GAN_Feat: 3.043 G_VGG: 1.783 D_real: 0.128 D_fake: 0.179 \n",
            "(epoch: 20, iters: 4100, time: 0.563) G_GAN: 1.397 G_GAN_Feat: 3.074 G_VGG: 1.612 D_real: 0.195 D_fake: 0.177 \n",
            "(epoch: 20, iters: 4200, time: 0.564) G_GAN: 1.646 G_GAN_Feat: 4.088 G_VGG: 1.702 D_real: 0.235 D_fake: 0.063 \n",
            "saving the latest model (epoch 20, total_steps 84000)\n",
            "End of epoch 20 / 200 \t Time Taken: 1861 sec\n",
            "saving the model at the end of epoch 20, iters 84000\n",
            "(epoch: 21, iters: 100, time: 0.562) G_GAN: 1.627 G_GAN_Feat: 2.661 G_VGG: 1.785 D_real: 0.217 D_fake: 0.089 \n",
            "(epoch: 21, iters: 200, time: 0.560) G_GAN: 1.263 G_GAN_Feat: 2.976 G_VGG: 1.698 D_real: 0.091 D_fake: 0.300 \n",
            "(epoch: 21, iters: 300, time: 0.562) G_GAN: 1.799 G_GAN_Feat: 3.163 G_VGG: 1.831 D_real: 0.053 D_fake: 0.065 \n",
            "(epoch: 21, iters: 400, time: 0.579) G_GAN: 1.998 G_GAN_Feat: 2.765 G_VGG: 1.595 D_real: 0.150 D_fake: 0.055 \n",
            "(epoch: 21, iters: 500, time: 0.573) G_GAN: 1.929 G_GAN_Feat: 3.565 G_VGG: 2.148 D_real: 0.025 D_fake: 0.056 \n",
            "(epoch: 21, iters: 600, time: 0.576) G_GAN: 1.722 G_GAN_Feat: 2.947 G_VGG: 1.827 D_real: 0.204 D_fake: 0.088 \n",
            "(epoch: 21, iters: 700, time: 0.575) G_GAN: 2.205 G_GAN_Feat: 2.896 G_VGG: 1.887 D_real: 0.292 D_fake: 0.054 \n",
            "(epoch: 21, iters: 800, time: 0.577) G_GAN: 1.676 G_GAN_Feat: 3.245 G_VGG: 1.971 D_real: 0.019 D_fake: 0.036 \n",
            "(epoch: 21, iters: 900, time: 0.575) G_GAN: 1.876 G_GAN_Feat: 2.757 G_VGG: 1.654 D_real: 0.147 D_fake: 0.027 \n",
            "(epoch: 21, iters: 1000, time: 0.575) G_GAN: 1.237 G_GAN_Feat: 3.145 G_VGG: 1.632 D_real: 0.094 D_fake: 0.196 \n",
            "saving the latest model (epoch 21, total_steps 85000)\n",
            "(epoch: 21, iters: 1100, time: 0.578) G_GAN: 1.303 G_GAN_Feat: 2.882 G_VGG: 1.863 D_real: 0.118 D_fake: 0.206 \n",
            "(epoch: 21, iters: 1200, time: 0.573) G_GAN: 1.445 G_GAN_Feat: 3.535 G_VGG: 2.046 D_real: 0.046 D_fake: 0.264 \n",
            "(epoch: 21, iters: 1300, time: 0.577) G_GAN: 1.438 G_GAN_Feat: 3.063 G_VGG: 1.646 D_real: 0.121 D_fake: 0.128 \n",
            "(epoch: 21, iters: 1400, time: 0.575) G_GAN: 1.489 G_GAN_Feat: 3.670 G_VGG: 2.317 D_real: 0.027 D_fake: 0.120 \n",
            "(epoch: 21, iters: 1500, time: 0.577) G_GAN: 1.459 G_GAN_Feat: 2.912 G_VGG: 1.325 D_real: 0.059 D_fake: 0.173 \n",
            "(epoch: 21, iters: 1600, time: 0.576) G_GAN: 1.729 G_GAN_Feat: 3.363 G_VGG: 2.094 D_real: 0.030 D_fake: 0.053 \n",
            "(epoch: 21, iters: 1700, time: 0.577) G_GAN: 1.670 G_GAN_Feat: 3.127 G_VGG: 1.872 D_real: 0.091 D_fake: 0.064 \n",
            "(epoch: 21, iters: 1800, time: 0.575) G_GAN: 1.442 G_GAN_Feat: 2.785 G_VGG: 1.728 D_real: 0.192 D_fake: 0.074 \n",
            "(epoch: 21, iters: 1900, time: 0.576) G_GAN: 1.436 G_GAN_Feat: 3.030 G_VGG: 1.912 D_real: 0.051 D_fake: 0.092 \n",
            "(epoch: 21, iters: 2000, time: 0.576) G_GAN: 1.915 G_GAN_Feat: 3.104 G_VGG: 2.046 D_real: 0.138 D_fake: 0.064 \n",
            "saving the latest model (epoch 21, total_steps 86000)\n",
            "(epoch: 21, iters: 2100, time: 0.575) G_GAN: 1.896 G_GAN_Feat: 3.196 G_VGG: 1.937 D_real: 0.610 D_fake: 0.027 \n",
            "(epoch: 21, iters: 2200, time: 0.575) G_GAN: 1.543 G_GAN_Feat: 2.977 G_VGG: 1.942 D_real: 0.033 D_fake: 0.096 \n",
            "(epoch: 21, iters: 2300, time: 0.575) G_GAN: 1.032 G_GAN_Feat: 2.937 G_VGG: 1.457 D_real: 0.075 D_fake: 0.316 \n",
            "(epoch: 21, iters: 2400, time: 0.577) G_GAN: 2.022 G_GAN_Feat: 3.247 G_VGG: 2.024 D_real: 0.084 D_fake: 0.041 \n",
            "(epoch: 21, iters: 2500, time: 0.575) G_GAN: 1.637 G_GAN_Feat: 2.919 G_VGG: 1.832 D_real: 0.088 D_fake: 0.236 \n",
            "(epoch: 21, iters: 2600, time: 0.577) G_GAN: 1.714 G_GAN_Feat: 3.145 G_VGG: 1.985 D_real: 0.173 D_fake: 0.131 \n",
            "(epoch: 21, iters: 2700, time: 0.574) G_GAN: 1.154 G_GAN_Feat: 2.622 G_VGG: 1.235 D_real: 0.187 D_fake: 0.239 \n",
            "(epoch: 21, iters: 2800, time: 0.577) G_GAN: 1.695 G_GAN_Feat: 3.270 G_VGG: 1.685 D_real: 0.061 D_fake: 0.251 \n",
            "(epoch: 21, iters: 2900, time: 0.575) G_GAN: 1.217 G_GAN_Feat: 2.592 G_VGG: 1.511 D_real: 0.158 D_fake: 0.272 \n",
            "(epoch: 21, iters: 3000, time: 0.576) G_GAN: 1.439 G_GAN_Feat: 2.874 G_VGG: 1.442 D_real: 0.039 D_fake: 0.219 \n",
            "saving the latest model (epoch 21, total_steps 87000)\n",
            "(epoch: 21, iters: 3100, time: 0.576) G_GAN: 1.756 G_GAN_Feat: 3.683 G_VGG: 1.956 D_real: 0.255 D_fake: 0.050 \n",
            "(epoch: 21, iters: 3200, time: 0.575) G_GAN: 1.804 G_GAN_Feat: 2.718 G_VGG: 1.650 D_real: 0.013 D_fake: 0.044 \n",
            "(epoch: 21, iters: 3300, time: 0.575) G_GAN: 1.577 G_GAN_Feat: 3.338 G_VGG: 1.929 D_real: 0.177 D_fake: 0.111 \n",
            "(epoch: 21, iters: 3400, time: 0.576) G_GAN: 2.101 G_GAN_Feat: 2.912 G_VGG: 1.466 D_real: 0.189 D_fake: 0.024 \n",
            "(epoch: 21, iters: 3500, time: 0.575) G_GAN: 1.466 G_GAN_Feat: 3.309 G_VGG: 2.182 D_real: 0.061 D_fake: 0.121 \n",
            "(epoch: 21, iters: 3600, time: 0.574) G_GAN: 1.359 G_GAN_Feat: 2.911 G_VGG: 1.965 D_real: 0.154 D_fake: 0.230 \n",
            "(epoch: 21, iters: 3700, time: 0.577) G_GAN: 1.836 G_GAN_Feat: 3.109 G_VGG: 1.455 D_real: 0.115 D_fake: 0.045 \n",
            "(epoch: 21, iters: 3800, time: 0.576) G_GAN: 1.229 G_GAN_Feat: 3.106 G_VGG: 1.805 D_real: 0.357 D_fake: 0.310 \n",
            "(epoch: 21, iters: 3900, time: 0.574) G_GAN: 1.663 G_GAN_Feat: 3.538 G_VGG: 2.105 D_real: 0.066 D_fake: 0.099 \n",
            "(epoch: 21, iters: 4000, time: 0.577) G_GAN: 1.470 G_GAN_Feat: 2.871 G_VGG: 2.254 D_real: 0.039 D_fake: 0.178 \n",
            "saving the latest model (epoch 21, total_steps 88000)\n",
            "(epoch: 21, iters: 4100, time: 0.574) G_GAN: 1.622 G_GAN_Feat: 2.815 G_VGG: 1.479 D_real: 0.566 D_fake: 0.211 \n",
            "(epoch: 21, iters: 4200, time: 0.573) G_GAN: 1.920 G_GAN_Feat: 3.212 G_VGG: 1.965 D_real: 0.050 D_fake: 0.034 \n",
            "End of epoch 21 / 200 \t Time Taken: 2443 sec\n",
            "(epoch: 22, iters: 100, time: 0.576) G_GAN: 1.113 G_GAN_Feat: 2.504 G_VGG: 1.804 D_real: 0.152 D_fake: 0.336 \n",
            "(epoch: 22, iters: 200, time: 0.577) G_GAN: 1.694 G_GAN_Feat: 3.101 G_VGG: 2.150 D_real: 0.034 D_fake: 0.203 \n",
            "(epoch: 22, iters: 300, time: 0.576) G_GAN: 2.200 G_GAN_Feat: 3.297 G_VGG: 1.808 D_real: 0.590 D_fake: 0.084 \n",
            "(epoch: 22, iters: 400, time: 0.575) G_GAN: 1.399 G_GAN_Feat: 2.741 G_VGG: 1.859 D_real: 0.398 D_fake: 0.245 \n",
            "(epoch: 22, iters: 500, time: 0.575) G_GAN: 1.466 G_GAN_Feat: 2.596 G_VGG: 1.557 D_real: 0.362 D_fake: 0.201 \n",
            "(epoch: 22, iters: 600, time: 0.576) G_GAN: 1.666 G_GAN_Feat: 2.667 G_VGG: 1.184 D_real: 0.049 D_fake: 0.045 \n",
            "(epoch: 22, iters: 700, time: 0.576) G_GAN: 1.789 G_GAN_Feat: 3.921 G_VGG: 2.465 D_real: 0.031 D_fake: 0.056 \n",
            "(epoch: 22, iters: 800, time: 0.576) G_GAN: 1.889 G_GAN_Feat: 3.138 G_VGG: 1.972 D_real: 0.061 D_fake: 0.030 \n",
            "saving the latest model (epoch 22, total_steps 89000)\n",
            "(epoch: 22, iters: 900, time: 0.574) G_GAN: 1.692 G_GAN_Feat: 2.756 G_VGG: 2.085 D_real: 0.257 D_fake: 0.068 \n",
            "(epoch: 22, iters: 1000, time: 0.574) G_GAN: 2.168 G_GAN_Feat: 2.924 G_VGG: 1.942 D_real: 0.148 D_fake: 0.023 \n",
            "(epoch: 22, iters: 1100, time: 0.575) G_GAN: 1.574 G_GAN_Feat: 3.162 G_VGG: 2.047 D_real: 0.019 D_fake: 0.065 \n",
            "(epoch: 22, iters: 1200, time: 0.576) G_GAN: 2.018 G_GAN_Feat: 2.883 G_VGG: 1.504 D_real: 0.074 D_fake: 0.033 \n",
            "(epoch: 22, iters: 1300, time: 0.576) G_GAN: 1.550 G_GAN_Feat: 2.932 G_VGG: 1.715 D_real: 0.150 D_fake: 0.136 \n",
            "(epoch: 22, iters: 1400, time: 0.577) G_GAN: 1.632 G_GAN_Feat: 2.980 G_VGG: 1.554 D_real: 0.015 D_fake: 0.040 \n",
            "(epoch: 22, iters: 1500, time: 0.577) G_GAN: 1.565 G_GAN_Feat: 3.267 G_VGG: 1.752 D_real: 0.173 D_fake: 0.138 \n",
            "(epoch: 22, iters: 1600, time: 0.576) G_GAN: 1.108 G_GAN_Feat: 3.076 G_VGG: 2.083 D_real: 0.095 D_fake: 0.273 \n",
            "(epoch: 22, iters: 1700, time: 0.575) G_GAN: 1.637 G_GAN_Feat: 2.965 G_VGG: 1.698 D_real: 0.137 D_fake: 0.065 \n",
            "(epoch: 22, iters: 1800, time: 0.574) G_GAN: 1.389 G_GAN_Feat: 2.983 G_VGG: 1.661 D_real: 0.050 D_fake: 0.157 \n",
            "saving the latest model (epoch 22, total_steps 90000)\n",
            "(epoch: 22, iters: 1900, time: 0.573) G_GAN: 1.348 G_GAN_Feat: 3.456 G_VGG: 1.891 D_real: 0.059 D_fake: 0.106 \n",
            "(epoch: 22, iters: 2000, time: 0.574) G_GAN: 1.913 G_GAN_Feat: 3.540 G_VGG: 1.807 D_real: 0.032 D_fake: 0.020 \n",
            "(epoch: 22, iters: 2100, time: 0.574) G_GAN: 1.888 G_GAN_Feat: 3.402 G_VGG: 1.864 D_real: 0.052 D_fake: 0.025 \n",
            "(epoch: 22, iters: 2200, time: 0.575) G_GAN: 1.886 G_GAN_Feat: 3.548 G_VGG: 2.068 D_real: 0.037 D_fake: 0.061 \n",
            "(epoch: 22, iters: 2300, time: 0.577) G_GAN: 1.452 G_GAN_Feat: 2.813 G_VGG: 1.587 D_real: 0.344 D_fake: 0.080 \n",
            "(epoch: 22, iters: 2400, time: 0.577) G_GAN: 1.758 G_GAN_Feat: 3.112 G_VGG: 1.599 D_real: 0.041 D_fake: 0.035 \n",
            "(epoch: 22, iters: 2500, time: 0.575) G_GAN: 1.876 G_GAN_Feat: 3.162 G_VGG: 1.869 D_real: 0.108 D_fake: 0.066 \n",
            "(epoch: 22, iters: 2600, time: 0.575) G_GAN: 1.693 G_GAN_Feat: 3.029 G_VGG: 1.675 D_real: 0.335 D_fake: 0.051 \n",
            "(epoch: 22, iters: 2700, time: 0.575) G_GAN: 2.036 G_GAN_Feat: 3.474 G_VGG: 1.835 D_real: 0.264 D_fake: 0.017 \n",
            "(epoch: 22, iters: 2800, time: 0.575) G_GAN: 1.865 G_GAN_Feat: 3.458 G_VGG: 1.892 D_real: 0.094 D_fake: 0.035 \n",
            "saving the latest model (epoch 22, total_steps 91000)\n",
            "(epoch: 22, iters: 2900, time: 0.572) G_GAN: 1.724 G_GAN_Feat: 3.014 G_VGG: 2.042 D_real: 0.015 D_fake: 0.085 \n",
            "(epoch: 22, iters: 3000, time: 0.575) G_GAN: 1.862 G_GAN_Feat: 2.839 G_VGG: 1.570 D_real: 0.013 D_fake: 0.044 \n",
            "(epoch: 22, iters: 3100, time: 0.576) G_GAN: 1.899 G_GAN_Feat: 3.423 G_VGG: 1.979 D_real: 0.077 D_fake: 0.046 \n",
            "(epoch: 22, iters: 3200, time: 0.577) G_GAN: 1.049 G_GAN_Feat: 3.178 G_VGG: 2.031 D_real: 0.046 D_fake: 0.449 \n",
            "(epoch: 22, iters: 3300, time: 0.576) G_GAN: 2.401 G_GAN_Feat: 3.017 G_VGG: 1.386 D_real: 0.219 D_fake: 0.063 \n",
            "(epoch: 22, iters: 3400, time: 0.574) G_GAN: 2.194 G_GAN_Feat: 3.464 G_VGG: 1.620 D_real: 0.016 D_fake: 0.018 \n",
            "(epoch: 22, iters: 3500, time: 0.574) G_GAN: 1.497 G_GAN_Feat: 2.826 G_VGG: 1.823 D_real: 0.068 D_fake: 0.198 \n",
            "(epoch: 22, iters: 3600, time: 0.575) G_GAN: 1.395 G_GAN_Feat: 2.655 G_VGG: 1.404 D_real: 0.050 D_fake: 0.248 \n",
            "(epoch: 22, iters: 3700, time: 0.575) G_GAN: 0.971 G_GAN_Feat: 2.776 G_VGG: 1.508 D_real: 0.078 D_fake: 0.523 \n",
            "(epoch: 22, iters: 3800, time: 0.576) G_GAN: 1.160 G_GAN_Feat: 2.915 G_VGG: 2.144 D_real: 0.091 D_fake: 0.178 \n",
            "saving the latest model (epoch 22, total_steps 92000)\n",
            "(epoch: 22, iters: 3900, time: 0.576) G_GAN: 2.166 G_GAN_Feat: 3.196 G_VGG: 1.902 D_real: 0.194 D_fake: 0.019 \n",
            "(epoch: 22, iters: 4000, time: 0.576) G_GAN: 2.294 G_GAN_Feat: 4.104 G_VGG: 2.154 D_real: 0.033 D_fake: 0.028 \n",
            "(epoch: 22, iters: 4100, time: 0.574) G_GAN: 1.817 G_GAN_Feat: 3.146 G_VGG: 1.688 D_real: 0.087 D_fake: 0.090 \n",
            "(epoch: 22, iters: 4200, time: 0.575) G_GAN: 1.254 G_GAN_Feat: 2.932 G_VGG: 1.866 D_real: 0.034 D_fake: 0.226 \n",
            "End of epoch 22 / 200 \t Time Taken: 2437 sec\n",
            "(epoch: 23, iters: 100, time: 0.576) G_GAN: 2.134 G_GAN_Feat: 3.062 G_VGG: 1.821 D_real: 0.247 D_fake: 0.019 \n",
            "(epoch: 23, iters: 200, time: 0.574) G_GAN: 2.158 G_GAN_Feat: 3.795 G_VGG: 2.144 D_real: 0.021 D_fake: 0.025 \n",
            "(epoch: 23, iters: 300, time: 0.577) G_GAN: 1.857 G_GAN_Feat: 2.883 G_VGG: 1.625 D_real: 0.472 D_fake: 0.034 \n",
            "(epoch: 23, iters: 400, time: 0.576) G_GAN: 2.001 G_GAN_Feat: 3.675 G_VGG: 1.811 D_real: 0.023 D_fake: 0.010 \n",
            "(epoch: 23, iters: 500, time: 0.574) G_GAN: 1.561 G_GAN_Feat: 2.797 G_VGG: 1.540 D_real: 0.074 D_fake: 0.176 \n",
            "(epoch: 23, iters: 600, time: 0.576) G_GAN: 1.758 G_GAN_Feat: 3.267 G_VGG: 2.053 D_real: 0.009 D_fake: 0.074 \n",
            "saving the latest model (epoch 23, total_steps 93000)\n",
            "(epoch: 23, iters: 700, time: 0.578) G_GAN: 1.509 G_GAN_Feat: 2.721 G_VGG: 1.543 D_real: 0.120 D_fake: 0.087 \n",
            "(epoch: 23, iters: 800, time: 0.574) G_GAN: 1.688 G_GAN_Feat: 2.810 G_VGG: 1.713 D_real: 0.085 D_fake: 0.064 \n",
            "(epoch: 23, iters: 900, time: 0.574) G_GAN: 1.939 G_GAN_Feat: 3.376 G_VGG: 1.832 D_real: 0.130 D_fake: 0.014 \n",
            "(epoch: 23, iters: 1000, time: 0.576) G_GAN: 1.627 G_GAN_Feat: 3.318 G_VGG: 1.944 D_real: 0.022 D_fake: 0.072 \n",
            "(epoch: 23, iters: 1100, time: 0.574) G_GAN: 1.764 G_GAN_Feat: 2.767 G_VGG: 2.003 D_real: 0.045 D_fake: 0.084 \n",
            "(epoch: 23, iters: 1200, time: 0.576) G_GAN: 1.634 G_GAN_Feat: 2.816 G_VGG: 1.693 D_real: 0.050 D_fake: 0.110 \n",
            "(epoch: 23, iters: 1300, time: 0.576) G_GAN: 1.834 G_GAN_Feat: 2.948 G_VGG: 1.711 D_real: 0.033 D_fake: 0.033 \n",
            "(epoch: 23, iters: 1400, time: 0.576) G_GAN: 1.427 G_GAN_Feat: 3.637 G_VGG: 2.105 D_real: 0.072 D_fake: 0.183 \n",
            "(epoch: 23, iters: 1500, time: 0.574) G_GAN: 1.500 G_GAN_Feat: 2.600 G_VGG: 1.726 D_real: 0.324 D_fake: 0.267 \n",
            "(epoch: 23, iters: 1600, time: 0.575) G_GAN: 1.568 G_GAN_Feat: 2.606 G_VGG: 1.680 D_real: 0.385 D_fake: 0.104 \n",
            "saving the latest model (epoch 23, total_steps 94000)\n",
            "(epoch: 23, iters: 1700, time: 0.578) G_GAN: 1.551 G_GAN_Feat: 2.943 G_VGG: 1.935 D_real: 0.230 D_fake: 0.077 \n",
            "(epoch: 23, iters: 1800, time: 0.572) G_GAN: 1.684 G_GAN_Feat: 2.790 G_VGG: 1.584 D_real: 0.040 D_fake: 0.093 \n",
            "(epoch: 23, iters: 1900, time: 0.576) G_GAN: 2.066 G_GAN_Feat: 2.778 G_VGG: 1.512 D_real: 0.083 D_fake: 0.030 \n",
            "(epoch: 23, iters: 2000, time: 0.576) G_GAN: 1.067 G_GAN_Feat: 3.189 G_VGG: 1.928 D_real: 0.075 D_fake: 0.436 \n",
            "(epoch: 23, iters: 2100, time: 0.577) G_GAN: 2.033 G_GAN_Feat: 3.098 G_VGG: 1.424 D_real: 0.308 D_fake: 0.097 \n",
            "(epoch: 23, iters: 2200, time: 0.574) G_GAN: 1.509 G_GAN_Feat: 3.381 G_VGG: 1.675 D_real: 0.030 D_fake: 0.149 \n",
            "(epoch: 23, iters: 2300, time: 0.577) G_GAN: 1.722 G_GAN_Feat: 3.376 G_VGG: 2.337 D_real: 0.183 D_fake: 0.085 \n",
            "(epoch: 23, iters: 2400, time: 0.575) G_GAN: 1.607 G_GAN_Feat: 3.369 G_VGG: 1.770 D_real: 0.032 D_fake: 0.082 \n",
            "(epoch: 23, iters: 2500, time: 0.575) G_GAN: 1.781 G_GAN_Feat: 2.963 G_VGG: 1.581 D_real: 0.072 D_fake: 0.027 \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}